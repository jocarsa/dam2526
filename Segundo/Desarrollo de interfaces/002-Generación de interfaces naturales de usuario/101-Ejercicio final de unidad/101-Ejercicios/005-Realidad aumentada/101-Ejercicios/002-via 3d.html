<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>FaceMesh 2D Canvas Texture (no Three.js)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    html, body { margin:0; height:100%; background:#000; overflow:hidden; }
    #wrap { position:relative; width:100vw; height:100vh; }
    video, canvas {
      position:absolute; inset:0; margin:auto;
      max-width:min(100vw, 100vh*4/3);
      max-height:min(100vh, 100vw*3/4);
      transform: scaleX(-1); /* selfie view */
    }
    #vid { opacity:.35; }
    #note {
      position:fixed; left:12px; bottom:12px; color:#fff; font:12px/1.2 system-ui, sans-serif; opacity:.85;
      background:rgba(0,0,0,.35); padding:.4rem .6rem; border-radius:.4rem;
    }
  </style>

  <!-- MediaPipe FaceMesh runtime -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

  <!-- Triangulation indices (defines global: TRIANGULATION) -->
  <script src="https://cdn.jsdelivr.net/npm/face-landmarks-triangulation@0.0.7/dist/triangulation.js"></script>

  <!-- TFJS (minimal) + legacy Facemesh UMD just to read UVs at runtime -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@3.21.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@3.21.0/dist/tf-converter.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh@0.0.4/dist/facemesh.js"></script>
</head>
<body>
  <div id="wrap">
    <video id="vid" autoplay muted playsinline></video>
    <canvas id="out"></canvas>
  </div>
  <div id="note">Put a transparent <code>makeup.png</code> next to this file (painted in the canonical UV layout).</div>

  <script>
  // ---------- DOM
  const video  = document.getElementById('vid');
  const canvas = document.getElementById('out');
  const ctx    = canvas.getContext('2d');

  // ---------- Load your texture (must match canonical UV layout)
  const makeupImg = new Image();
  makeupImg.src = 'makeup.png';

  // ---------- Globals
  const VERTS = 468;
  let UVS = null;        // will come from facemesh.getUVCoords()
  let srcTris = null;    // source triangles in texture pixel coords

  // Ask the legacy TFJS Facemesh UMD for canonical UVs
  // (window.facemesh is defined by the UMD bundle we included above)
  (async () => {
    try {
      UVS = await window.facemesh.getUVCoords(); // [[u,v], ...] length 468
      // If texture is already loaded, precompute triangles
      if (makeupImg.complete) buildSrcTriangles();
      else makeupImg.onload = buildSrcTriangles;
    } catch (e) {
      console.error('Failed to get UVs from TFJS facemesh:', e);
    }
  })();

  // Precompute source triangles once UVS + texture ready
  function buildSrcTriangles() {
    if (!UVS) return;
    const w = makeupImg.naturalWidth  || makeupImg.width;
    const h = makeupImg.naturalHeight || makeupImg.height;
    if (!w || !h) return;

    if (!window.TRIANGULATION) {
      console.error('TRIANGULATION not loaded');
      return;
    }
    srcTris = [];
    for (let i = 0; i < TRIANGULATION.length; i += 3) {
      const a = TRIANGULATION[i], b = TRIANGULATION[i+1], c = TRIANGULATION[i+2];
      const [ua, va] = UVS[a], [ub, vb] = UVS[b], [uc, vc] = UVS[c];
      srcTris.push([
        ua*w, (1-va)*h,
        ub*w, (1-vb)*h,
        uc*w, (1-vc)*h
      ]);
    }
  }

  // ---------- Math: affine mapping from source tri -> dest tri (Canvas2D)
  function triToAffine(sx0, sy0, sx1, sy1, sx2, sy2, dx0, dy0, dx1, dy1, dx2, dy2) {
    const denom = sx0*(sy1 - sy2) + sx1*(sy2 - sy0) + sx2*(sy0 - sy1);
    if (Math.abs(denom) < 1e-8) return [1,0,0,1,0,0]; // degenerate
    const m00 = (dx0*(sy1 - sy2) + dx1*(sy2 - sy0) + dx2*(sy0 - sy1)) / denom;
    const m10 = (dy0*(sy1 - sy2) + dy1*(sy2 - sy0) + dy2*(sy0 - sy1)) / denom;
    const m01 = (dx0*(sx2 - sx1) + dx1*(sx0 - sx2) + dx2*(sx1 - sx0)) / denom;
    const m11 = (dy0*(sx2 - sx1) + dy1*(sx0 - sx2) + dy2*(sx1 - sx0)) / denom;
    const m02 = (dx0*(sx1*sy2 - sx2*sy1) + dx1*(sx2*sy0 - sx0*sy2) + dx2*(sx0*sy1 - sx1*sy0)) / denom;
    const m12 = (dy0*(sx1*sy2 - sx2*sy1) + dy1*(sx2*sy0 - sx0*sy2) + dy2*(sx0*sy1 - sx1*sy0)) / denom;
    return [m00, m10, m01, m11, m02, m12]; // a,b,c,d,e,f for ctx.setTransform
  }

  function drawTexturedTriangle(img, s, d) {
    // s: [sx0,sy0, sx1,sy1, sx2,sy2]
    // d: [dx0,dy0, dx1,dy1, dx2,dy2]
    ctx.save();
    ctx.beginPath();
    ctx.moveTo(d[0], d[1]);
    ctx.lineTo(d[2], d[3]);
    ctx.lineTo(d[4], d[5]);
    ctx.closePath();
    ctx.clip();
    const m = triToAffine(
      s[0],s[1], s[2],s[3], s[4],s[5],
      d[0],d[1], d[2],d[3], d[4],d[5]
    );
    ctx.setTransform(m[0], m[1], m[2], m[3], m[4], m[5]);
    ctx.drawImage(img, 0, 0);
    ctx.restore();
  }

  // Map FaceMesh normalized coords (selfie) to canvas pixels (cover fit + mirror)
  function videoToCanvas(xNorm, yNorm) {
    const vW = video.videoWidth, vH = video.videoHeight;
    const cW = canvas.width,     cH = canvas.height;

    const videoAspect = vW / vH, canvasAspect = cW / cH;
    let scale, offX=0, offY=0;
    if (videoAspect > canvasAspect) {
      scale = cH / vH; offX = (vW*scale - cW) / 2;
    } else {
      scale = cW / vW; offY = (vH*scale - cH) / 2;
    }
    const xVid = (1 - xNorm) * vW;  // mirror
    const yVid = yNorm * vH;

    return [xVid * scale - offX, yVid * scale - offY];
  }

  // ---------- MediaPipe FaceMesh
  const faceMesh = new FaceMesh({
    locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`
  });
  faceMesh.setOptions({
    maxNumFaces: 1,
    refineLandmarks: true,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5
  });

  faceMesh.onResults(res => {
    if (!res.multiFaceLandmarks || !makeupImg.complete || !srcTris) return;

    // 1) Draw camera with same cover fit + mirror
    ctx.setTransform(1,0,0,1,0,0);
    const vW = video.videoWidth, vH = video.videoHeight;
    const cW = canvas.width, cH = canvas.height;
    const videoAspect = vW / vH, canvasAspect = cW / cH;
    let dx=0, dy=0, dw=cW, dh=cH;
    if (videoAspect > canvasAspect) {
      dh = cH; dw = vW * (cH / vH); dx = (cW - dw) / 2; dy = 0;
    } else {
      dw = cW; dh = vH * (cW / vW); dx = 0; dy = (cH - dh) / 2;
    }
    ctx.clearRect(0,0,cW,cH);
    ctx.save();
    ctx.translate(cW, 0); ctx.scale(-1, 1);
    ctx.drawImage(video, dx, dy, dw, dh);
    ctx.restore();

    // 2) Cache destination landmark pixels
    const lms = res.multiFaceLandmarks[0];
    const destPts = new Array(VERTS * 2);
    for (let i = 0; i < VERTS; i++) {
      const [x, y] = videoToCanvas(lms[i].x, lms[i].y);
      destPts[i*2]   = x;
      destPts[i*2+1] = y;
    }

    // 3) Warp triangles from texture -> face
    // For grayscale masks you can try: ctx.globalCompositeOperation = 'multiply' or 'soft-light'
    ctx.globalCompositeOperation = 'source-over';
    for (let t = 0, ti = 0; t < srcTris.length; t++, ti += 3) {
      const s = srcTris[t];
      const a = TRIANGULATION[ti], b = TRIANGULATION[ti+1], c = TRIANGULATION[ti+2];
      const d = [
        destPts[a*2], destPts[a*2+1],
        destPts[b*2], destPts[b*2+1],
        destPts[c*2], destPts[c*2+1]
      ];
      drawTexturedTriangle(makeupImg, s, d);
    }
  });

  // ---------- Start camera
  const cam = new Camera(video, {
    onFrame: async () => { await faceMesh.send({ image: video }); },
    width: 640, height: 480
  });
  cam.start();

  // ---------- Keep canvas internal size in sync with its CSS size
  (function syncCanvasSize(){
    const r = canvas.getBoundingClientRect();
    const w = Math.round(r.width), h = Math.round(r.height);
    if (canvas.width !== w || canvas.height !== h) {
      canvas.width = w; canvas.height = h;
    }
    requestAnimationFrame(syncCanvasSize);
  })();
  </script>
</body>
</html>

