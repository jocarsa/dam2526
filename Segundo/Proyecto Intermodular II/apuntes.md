# Proyecto Intermodular II

**Author:** Jose Vicente Carratala Sanchis

## Table of contents

- [Introduccion](#introduccion)
- [An√°lisis](#analisis)
  - [Recopilaci√≥n de informaci√≥n](#recopilacion-de-informacion)
  - [Identificaci√≥n y priorizaci√≥n de necesidades.](#identificacion-y-priorizacion-de-necesidades)
  - [Identificaci√≥n de los aspectos que facilitan o dificultan el desarrollo de la posible intervenci√≥n](#identificacion-de-los-aspectos-que-facilitan-o-dificultan-el-desarrollo-de-la-posible-intervencion)
- [Dise√±o](#diseno)
  - [Definici√≥n o adaptaci√≥n de la intervencioÃÅn](#definicion-o-adaptacion-de-la-intervencion)
  - [Priorizaci√≥n y secuenciaci√≥n de las acciones.](#priorizacion-y-secuenciacion-de-las-acciones)
  - [La planificaci√≥n de la intervenci√≥n](#la-planificacion-de-la-intervencion)
  - [Determinaci√≥n de recursos.](#determinacion-de-recursos)
  - [Planificaci√≥n de la evaluacioÃÅn](#planificacion-de-la-evaluacion)
  - [Dise√±o de documentaci√≥n](#diseno-de-documentacion)
  - [Plan de atenci√≥n al cliente](#plan-de-atencion-al-cliente)
- [Organizacion](#organizacion)
  - [Detecci√≥n de demandas y necesidades](#deteccion-de-demandas-y-necesidades)
  - [Programaci√≥n](#programacion)
  - [Gesti√≥n](#gestion)
  - [Coordinaci√≥n y supervisi√≥n de la intervenci√≥n](#coordinacion-y-supervision-de-la-intervencion)
  - [Elaboraci√≥n de informes](#elaboracion-de-informes)
  - [Seguimiento y control](#seguimiento-y-control)
- [Actividades profesionales](#actividades-profesionales)
  - [√Åreas de sistemas y departamentos de inform√°tica en cualquier sector de actividad](#areas-de-sistemas-y-departamentos-de-informatica-en-cualquier-sector-de-actividad)
  - [Sector de servicios tecnol√≥gicos y comunicaciones](#sector-de-servicios-tecnologicos-y-comunicaciones)
  - [√Årea comercial con gesti√≥n de transacciones por Internet](#area-comercial-con-gestion-de-transacciones-por-internet)
- [Lineas de actuaci√≥n](#lineas-de-actuacion)
  - [La ejecuci√≥n de trabajos en equipo](#la-ejecucion-de-trabajos-en-equipo)
  - [La autoevaluaci√≥n del trabajo realizado](#la-autoevaluacion-del-trabajo-realizado)
  - [La autonom√≠a y la iniciativa](#la-autonomia-y-la-iniciativa)
  - [El uso de las TIC](#el-uso-de-las-tic)
- [Bibliograf√≠a](#bibliografia)
- [Proyectos](#proyectos)
  - [empresarialIA](#empresarialia)
  - [agente virtual](#agente-virtual)
  - [Ejercicio de final de unidad](#ejercicio-de-final-de-unidad)
  - [Examen final](#examen-final)
- [Proyectos segundo trimestre](#proyectos-segundo-trimestre)
  - [Extractor transcripciones](#extractor-transcripciones)
  - [generador web IA](#generador-web-ia)
  - [Entrenamiento IA](#entrenamiento-ia)
  - [entrenar chatbot a partir de whatsapp](#entrenar-chatbot-a-partir-de-whatsapp)
  - [entrenar chatbot a partir de pdf](#entrenar-chatbot-a-partir-de-pdf)
  - [scrapeador web y entrenamiento](#scrapeador-web-y-entrenamiento)
  - [interfaz natural IA](#interfaz-natural-ia)
- [Actividad libre de final de evaluaci√≥n - La milla extra](#actividad-libre-de-final-de-evaluacion-la-milla-extra)
  - [La Milla Extra - Primera evaluaci√≥n](#la-milla-extra-primera-evaluacion)

---

<a id="introduccion"></a>
# Introduccion


<a id="analisis"></a>
# An√°lisis

<a id="recopilacion-de-informacion"></a>
## Recopilaci√≥n de informaci√≥n

### B√∫squeda de empresas

```markdown
Empresas del sector por sus caracteriÃÅsticas organizativas y el tipo de producto o servicio que ofrecen

Empresa objetivo: Una empresa de inteligencia artificial que de servicios con sentido en la zona geogr√°fica de Valencia

An√°lisis de los grandes jugadores:

- OpenAI - ChatGPT - USA
- Microsoft - Copilot - USA
- Mistral - Francia-USA
- Grok - X.com - USA
- Deepseek - IA china
- Claude - Programacion
- Gemini - Google
- Meta - Llama
- Aleph Alpha (Alemania)
- Qwen - Alibaba (China)
- ERNIE - Baidu (China)

Stabbility - Stable Diffusion
Mijourney
Firefly dobe
Elevenlabs Voz
Nano Banana de Google

Perplexity

Son grandes jugadores, marcadores de tendencias, punta de lanza
Inalcanzables en un primer momento


Espa√±a:
Modelo ALIA que es un modelo promocionado por el gobierno para tener un modelo de IA adpatado a las lenguas territoriales espa√±olas y a las expresiones de este territorio

Demanda:
Solo 20% de empresas usan la IA en procesos
Lo hacen en modo experimental

Sectores: Finanzas, teleco, infor, comunicacion

Ejemplo Maisa:

Fundaci√≥n ELLIS Alicante

Ahora Comunidad Valenciana

Instituciones fuertes: ELLIS Alicante es una pieza clave. Tener un centro ELLIS aporta prestigio, atracci√≥n de talento, colaboraci√≥n con instituciones europeas. 
Wikipedia

Hubs & cooperaci√≥n p√∫blico-privada: NTT DATA ha abierto un hub de IA especializado en la CV, ligado a universidades y autoridades locales, para acelerar adopci√≥n en empresas locales. 
es.nttdata.com

Iniciativas ling√º√≠sticas y de soberan√≠a tecnol√≥gica: el proyecto ALIA apunta a modelos p√∫blicos y abiertos en castellano y valenciano, lo cual puede ayudar a diferenciar frente a soluciones internacionales que no siempre consideran esas lenguas ni contextos locales. 
Diario ABC

Financiaci√≥n local para startups: como el caso de Maisa, que demuestra que en Valencia hay capacidad de levantar rondas importantes si la propuesta es buena y diferencial. 
Cinco D√≠as

Marco estrat√©gico regional: la Generalitat ha elaborando estrategia de IA con visi√≥n para promover inversi√≥n, formaci√≥n, infraestructuras, probablemente regulaciones, colaboraci√≥n universidad-empresa. 
Presidencia de la Generalitat Valenciana

Oportunidades

Con lo anterior, estas son las zonas donde hay m√°s espacio para entrar, competir o colaborar:

Modelos de IA adaptados al espa√±ol y valenciano/localizaci√≥n cultural / ling√º√≠stica

Justicia, administraci√≥n p√∫blica, servicios sociales locales, sanidad regional ‚Äî donde puede haber contratos p√∫blicos o necesidad de cumplimiento reglamentario espec√≠fico

IA explicable, trazabilidad, seguridad ‚Äî esos valores tienen tir√≥n en entornos regulados, como finanzas, farmac√©utico, energ√≠a

Hubs de datos, servicios de infraestructura como servicio de inferencia, especializaci√≥n en hardware/software que permita desplegar modelos localmente (on-prem / edge) para cumplir con privacidad y regulaci√≥n europea

Colaboraciones con universidades (ELIS Alicante, UPV, etc.), centros de investigaci√≥n para I+D, acceder a fondos regionales / europeos

Servicios B2B para pymes: muchas pymes quieren usar IA pero no saben c√≥mo empezar, tienen pocos recursos; hay espacio en consultor√≠a, implementaci√≥n, integraci√≥n, capacitaci√≥n

# Qui√©n es Maisa?


Qui√©nes son

Nombre completo: Maisa (a veces ‚ÄúMaisa AI‚Äù).

Fundaci√≥n: principios de 2024. 
Emprendedores
+1

Fundadores: David Villal√≥n (CEO) y Manuel Romero (CSO). 
EL ESPA√ëOL
+2
EU-Startups
+2

David Villal√≥n ven√≠a de roles relacionados con IA aplicada / producto (fue Chief AI Officer en Clibrain, director de producto en Voicemod). 
EL ESPA√ëOL

Manuel Romero tiene un perfil fuerte en contribuciones open source: numerosos modelos en Hugging Face, bastante reputaci√≥n t√©cnica. 
EL ESPA√ëOL
+1

Ubicaci√≥n: sede principal en Valencia, Espa√±a, con presencia tambi√©n en San Francisco, EE.UU. 
Emprendedores
+2
EU-Startups
+2

Qu√© hacen / Propuesta de valor

Producto central: agentes de IA (‚Äúdigital workers‚Äù, agentes IA) con trazabilidad y auditabilidad, pensados para operaciones empresariales cr√≠ticas, reguladas, exigentes. 
Cinco D√≠as
+2
EU-Startups
+2

Tecnolog√≠a propia destacada:

KPU (Knowledge Processing Unit): motor propio de razonamiento / procesamiento del conocimiento. Permite convertir modelos de lenguaje en agentes aut√≥nomos que ejecutan tareas complejas con menos ‚Äúalucinaciones‚Äù. 
EU-Startups
+2
TechCrunch
+2

Chain-of-Work (‚Äúcadena de trabajo‚Äù): cada acci√≥n ejecutada por los agentes queda registrada, con la l√≥gica, datos, herramientas usadas: proporciona transparencia, auditabilidad. 
Cinco D√≠as
+1

HALP (‚ÄúHuman-Augmented LLM Processing‚Äù): m√©todo que incorpora intervenci√≥n humana en el entrenamiento / supervisi√≥n de los agentes para mejorar fiabilidad. 
Emprendedores
+1

Otra parte del producto: Maisa Studio ‚Äî plataforma no-code / low-code que permite a usuarios no t√©cnicos (ej. expertos de negocio) definir agentes con lenguaje natural, desplegarlos, integrarlos, sin necesidad de escribir c√≥digo intensivamente. 
Cinco D√≠as
+2
EU-Startups
+2

Flexibilidad de despliegue: puede funcionar en nube segura de Maisa o mediante despliegue privado / on-premise, para cumplir requisitos de regulaci√≥n/compliance. 
EU-Startups
+2
TechCrunch
+2

Sectores de aplicaci√≥n temprana: finanzas, fabricaci√≥n (automotriz), energ√≠a, cadenas de suministro, reconciliaci√≥n de transacciones, evaluaci√≥n de riesgos, monitoreo, etc. 
TechCrunch
+2
Cinco D√≠as
+2

Hitos / financiaci√≥n

Pre-seed: finales de 2024, levantaron ~ 5 millones de d√≥lares con participaci√≥n de NFX y Village Global. 
The SaaS News
+1

Ronda semilla reciente (agosto-septiembre 2025): 25 millones de d√≥lares (‚âà 21,4-22 millones de euros seg√∫n cambio) la mayor ronda semilla registrada hasta ahora en Espa√±a para una IA nativa. Liderada por Creandum, con participaci√≥n de Forgepoint Capital International (v√≠a alianza europea con Banco Santander), NFX, Village Global. 
Valencia Plaza
+3
Cinco D√≠as
+3
TechCrunch
+3

Uso de los fondos: reforzar equipo en I+D (IA), ingenier√≠a, ventas, atenci√≥n al cliente; expansi√≥n internacional (Europa y Norteam√©rica). 
Cinco D√≠as
+1

Crecimiento de plantilla: estimado que van de ~35 personas actualmente hacia unas 65 personas para Q1 2026. 
TechCrunch
+1

Reconocimiento: mencionada junto a grandes como Google o Amazon en informes de Gartner como uno de los proveedores destacados de agentes de IA (‚ÄúAI agent vendors‚Äù). 
Cinco D√≠as
+1

Tama√±o / estructura interna conocida

Equipo t√©cnico bastante fuerte debido al perfil de los fundadores. Se espera que gran parte del hiring vaya a R&D / ingenier√≠a, dado su enfoque tecnol√≥gico de diferenciaci√≥n. 
EU-Startups
+1

Actualmente (~agosto-septiembre 2025) alrededor de 35 personas en plantilla, camino a ~65 para inicios de 2026. 
TechCrunch
+1

√Åreas principales internas: investigaci√≥n y desarrollo en IA, ingenier√≠a, ventas, atenci√≥n al cliente. Tambi√©n operaci√≥n/comercial para su despliegue en clientes exigentes. 
EU-Startups
+1

Ventajas competitivas y retos
Ventajas

Confianza / auditabilidad: muchas empresas dudan de adoptar IA porque no pueden explicar los resultados, revisar errores o cumplir normativas. Maisa se posiciona justo ah√≠.

No-code / citizen developer: permite a no t√©cnicos implementar agentes, lo que reduce barreras de entrada, dependencia de desarrolladores.

Modelo agn√≥stico: al no depender de un √∫nico modelo de lenguaje, puede adaptar a distintos backends y exigencias del cliente.

Mercado regulado: sectores como banca, energ√≠a, automotriz tienen altos requisitos de compliance, trazabilidad, fiabilidad; si conquistas ah√≠, las barreras de entrada para competidores pueden ser mayores.

Financiaci√≥n fuerte + reconocimiento internacional: les da m√∫sculo para expandir r√°pido, captar talento, mejorar su producto.

Retos

Competencia fuerte: muchas empresas (grandes y emergentes) buscan lo mismo: agentes auditable, confiables, IA ‚Äúexplicable‚Äù. El est√°ndar de ‚Äúsin alucinaciones‚Äù es dif√≠cil de cumplir consistentemente.

Escalabilidad vs personalizaci√≥n: atender clientes regulados suele requerir adaptaciones espec√≠ficas, certificaciones, integraciones complicadas, lo que puede ralentizar crecimiento.

Talento: captar personal de alto nivel en IA, ingenier√≠a, seguridad, cumplimiento legal. Comunidad t√©cnica y regulatoria en Espa√±a a√∫n en desarrollo.

Infraestructura / costes operativos: despliegues privados, auditor√≠as, seguridad, cumplimiento de leyes de datos, etc., tienen costes elevados.

Validaci√≥n continua y reputaci√≥n: deben demostrar en la pr√°ctica que su Chain-of-Work, su KPU y HALP funcionan y reducen riesgos; los clientes ser√°n exigentes y cualquier bug o error puede costar mucho en reputaci√≥n.

Perspectivas y lo que podr√≠a venir

Es muy probable que sus pr√≥ximos 12-18 meses vean:

Incremento notable en la plantilla y fortalecimiento de equipo t√©cnico (IA, ML, seguridad, devops) y comercial (ventas corporativas).

Contratos piloto / adopciones productivas en m√°s empresas grandes, especialmente internacionales, que requieren servicios de confianza.

Profundizaci√≥n del producto: mejoras en KPU, m√°s conectores, mejores integraciones con sistemas empresariales ya existentes, quiz√°s certificaciones de seguridad/compliance (ISO, GDPR, AI Act UE).

Expansi√≥n geogr√°fica: presencia en Norteam√©rica, quiz√° oficinas de representaci√≥n, clientes en EE.UU. y Europa fuera de Espa√±a.

Potenciales rondas adicionales si cumplen hitos, en Serie A o extensi√≥n de ronda semilla si necesitan recursos para competir con grandes players.


Analizar perfil personal:
https://www.linkedin.com/in/manuel-romero-cs/ - MAISA
```

### Empresas tipo indicando la estructura organizativa y las funciones de cada departamento

```markdown
# Posibles clientes: PYME Valenciana

1. Panorama general del tejido empresarial en la Comunitat Valenciana

Dominio de la pyme y microempresa

M√°s del 95 % de las empresas de la Comunitat tienen menos de 10 trabajadores (microempresas).

Sectores tradicionales: industria (cer√°mica en Castell√≥n, textil en Alcoy, calzado en Elche, agroalimentario en toda la regi√≥n), comercio, servicios y turismo.

Estas empresas son √°giles, pero con recursos limitados para innovaci√≥n tecnol√≥gica: externalizan mucho y buscan soluciones SaaS sencillas y de coste contenido.

Pymes medianas (50‚Äì250 trabajadores)

Suelen ser empresas familiares consolidadas en industria/manufactura o distribuci√≥n.

Algunas ya tienen departamentos TIC peque√±os o externalizados, lo que las convierte en candidatas naturales a adoptar servicios de IA ‚Äúllave en mano‚Äù.

Grandes empresas

Pocas en n√∫mero, pero con impacto: Ford Almussafes, Mercadona, Bale√†ria, Stadler, Power Electronics, etc.

Aqu√≠ ya hay departamentos internos de I+D+i, estructuras jer√°rquicas m√°s complejas, y capacidad para proyectos a medida, incluso colaboraciones con universidades (UPV, UA, UV).

Son las que suelen absorber proyectos piloto de gran escala (gemelos digitales, log√≠stica, mantenimiento predictivo, etc.).

2. Estructura organizativa t√≠pica

Microempresas y peque√±as:

Propietario‚Äìgerente + 1‚Äì2 t√©cnicos/administrativos.

La toma de decisiones es muy centralizada, poco formalizada.

La digitalizaci√≥n depende casi siempre de subvenciones o asesoramiento externo.

Pymes medianas:

Direcci√≥n general (propietario o familia) + √°reas separadas: producci√≥n, comercial, administraci√≥n/finanzas.

Puede haber un responsable TIC o sistemas, pero la IA no est√° integrada todav√≠a; lo ven como un a√±adido en productividad/marketing.

Grandes empresas:

Direcci√≥n general + comit√© ejecutivo con √°reas funcionales (finanzas, operaciones, RRHH, innovaci√≥n).

Departamentos propios de IT/innovaci√≥n con perfiles de ingenieros.

Estructura jer√°rquica m√°s clara y capacidad de trabajar en proyectos de IA con partners locales o internacionales.

3. Particularidad de las empresas de IA valencianas

Muchas startups nacen como spin-offs universitarios (UPV, UV, UA, ELLIS Alicante).

Suelen arrancar como microempresas con equipos muy especializados (3‚Äì10 personas): data scientists, ingenieros de software, un perfil de negocio.

Dependencia inicial de rondas de financiaci√≥n y aceleradoras (Lanzadera en Marina de Empresas, por ejemplo).

Si crecen, adoptan estructura de scale-up: CEO, CTO, responsable de operaciones, equipo t√©cnico y marketing internacional.

Ejemplo: Maisa (Valencia), que con 25 M$ de inversi√≥n ya apunta a estructura de scale-up con m√°s de 50 empleados a corto plazo.

4. Implicaciones para una empresa de IA en Valencia

Mercado principal: pymes locales (que son muchas y con poca IA). Necesitan soluciones empaquetadas y asequibles.

Oportunidades grandes: alianzas con grandes compa√±√≠as valencianas (Mercadona, Bale√†ria, etc.) para proyectos piloto de impacto.

Modelo de crecimiento: empezar peque√±o (consultor√≠a/servicios SaaS para pymes) y escalar hacia corporativos o incluso ofrecer plataforma exportable al resto de Espa√±a/Europa.

# Estructura de las empresas de IA

üìä Tama√±o promedio

Micro y peque√±as empresas (3‚Äì20 personas):

Son la gran mayor√≠a.

Nacen como startups o spin-offs universitarias (UPV, UV, UA, UMH, o ligadas a ELLIS Alicante).

En fase inicial se sostienen con incubadoras (Lanzadera, Demium, Innsomnia), ayudas regionales (IVACE, RETECH, CDTI) o proyectos piloto con grandes clientes.

Scale-ups (20‚Äì100 personas):

Mucho menos frecuentes, pero existen casos como Maisa (Valencia), que tras rondas de inversi√≥n ha empezado a crecer r√°pidamente hacia estructura de scale-up.

Entran en rondas semilla o Serie A y comienzan a profesionalizar gesti√≥n.

Grandes (>100 personas):

Pr√°cticamente inexistentes en Valencia como nativas de IA.

Los que llegan a ese tama√±o suelen ser filiales o hubs de multinacionales (ej. NTT DATA hub de IA en la CV).

üèóÔ∏è Estructura empresarial t√≠pica
1. Etapa semilla (3‚Äì10 personas)

CEO/fundador: perfil mixto (negocio + tecnolog√≠a).

CTO: responsable t√©cnico de producto/modelos.

Data scientists / ML engineers (1‚Äì3).

Backend/frontend developer (1‚Äì2).

Business developer/marketing (opcional o compartido entre fundadores).

‚û°Ô∏è Estructura muy horizontal, todos hacen de todo.

2. Etapa early growth (10‚Äì30 personas)

CEO + COO/CTO.

Equipo t√©cnico dividido en:

Data/IA: entrenamiento de modelos, integraciones.

Software/Producto: APIs, front, UX.

Equipo negocio: ventas, alianzas, marketing digital.

Soporte/operaciones: proyectos, administraci√≥n.

‚û°Ô∏è Comienzan a separar producto de servicios, y a crear √°reas diferenciadas.

3. Scale-up (30‚Äì100 personas)

C-level definido: CEO, CTO, COO, CPO (producto), CFO.

Departamentos:

I+D en IA (a menudo con colaboraciones universidad-empresa).

Desarrollo de software (producto SaaS o soluciones).

Comercial y Customer Success (clientes B2B).

Marketing y comunicaci√≥n.

Administraci√≥n/finanzas.

Estructura m√°s jer√°rquica, con managers intermedios.

‚û°Ô∏è Es la etapa que muy pocas alcanzan todav√≠a en la CV, pero hacia donde apuntan startups con rondas grandes (ej. Maisa).
```

<a id="identificacion-y-priorizacion-de-necesidades"></a>
## Identificaci√≥n y priorizaci√≥n de necesidades.

### Necesidades maÃÅs demandadas a las empresas

```markdown
Que necesita la empresa valenciana

Para que una pyme valenciana desarrolle, lance y mantenga con √©xito productos de IA, debe contar con varios componentes clave. Algunos internos, otros externos (ecosistema, regulaciones, financiaci√≥n). Aqu√≠ van los m√°s importantes:

Elemento	Detalles / lo que implica
Talento t√©cnico	Tener en su plantilla o asociado personal con conocimientos en ML/IA, ingenier√≠a de datos, DevOps, seguridad de datos, UX para IA. Sin esto, dependes de terceros o te falta calidad.
Infraestructura tecnol√≥gica	Computaci√≥n para entrenar modelos (GPU/TPU o acceso a nube), almacenamiento de datos, capacidades de inferencia, pipelines de datos limpios, herramientas de monitorizaci√≥n.
Datos de calidad	Datos suficientes, bien etiquetados, bien estructurados, relevantes para el problema que quieras resolver. En muchos casos falta esa base de datos interna o acceso a datos externos legales y limpios.
Acceso a financiamiento	Capital para I+D, para prototipar, para pruebas piloto, para escalar. Subvenciones regionales, nacionales, fondos europeos; tambi√©n inversi√≥n privada.
Comprensi√≥n del mercado	Saber qu√© necesidad concreta resuelves, qui√©n ser√° cliente, precio que aceptan, barreras a la compra, regulaci√≥n sectorial. Productos de IA ‚Äúde laboratorio‚Äù que no se adaptan al cliente no escalan.
Regulaci√≥n, privacidad, cumplimiento legal	GDPR, leyes locales, normativas sectoriales (salud, finanzas, energ√≠a, etc.). Necesitan asegurar que su producto respeta derechos, tiene transparencia, auditabilidad.
Interoperabilidad / integraci√≥n	El producto de IA debe integrarse con sistemas ya existentes en la empresa cliente (ERP, CRM, sistemas de producci√≥n, log√≠stica, etc.). Interfaces limpias, APIs, conectores son clave.
Cultura interna de adopci√≥n	Resistencias al cambio, formaci√≥n del personal, mindset orientado a iterar, aprender de errores. Empezar con pruebas piloto, no pretender darlo todo desde el primer d√≠a.
Soporte, mantenimiento y confiabilidad	Sistemas de monitorizaci√≥n, detecci√≥n/correcci√≥n de errores, feedback con usuarios. Si tu IA ‚Äúfalla‚Äù mucho, se rompe confianza.
Estrategia de escalabilidad y comercializaci√≥n	Plan para crecer fuera del cliente piloto, tanto en producto (modularidad, robustez) como en mercado (¬øsolo CV? ¬øEspa√±a? ¬øEuropa?). Tambi√©n estrategia de precios, canales de venta, soporte postventa.

# An√°lisis de necesidades

Diagn√≥stico claro de necesidades

No saben por d√≥nde empezar. Te pedir√°n que analices su negocio y les digas:

qu√© procesos se pueden automatizar,

cu√°nto costar√°,

qu√© beneficios tendr√°n (ahorro, rapidez, competitividad).

Necesitan consultor√≠a simple y pedag√≥gica.

Soluciones empaquetadas y f√°ciles de usar

No quieren proyectos de I+D eternos. Quieren SaaS / servicios listos para usar.

Ejemplos:

Chatbots de atenci√≥n al cliente en espa√±ol/valenciano.

Clasificaci√≥n autom√°tica de facturas y documentos.

An√°lisis de ventas y predicci√≥n de demanda.

Control de calidad con visi√≥n artificial (fabricaci√≥n, cer√°mica, calzado).

Coste predecible y ajustado

La mayor√≠a son microempresas (<10 empleados). Necesitan precios asequibles: cuota mensual o por uso, sin grandes inversiones iniciales.

Modelos tipo suscripci√≥n (ej. 50‚Äì200 ‚Ç¨/mes por servicio b√°sico) funcionan mejor que proyectos cerrados de 30 000 ‚Ç¨.

Integraci√≥n con lo que ya tienen

Trabajan con ERP b√°sicos, hojas Excel, software de facturaci√≥n, CRMs sencillos.

Necesitan que tu IA se conecte f√°cilmente (APIs, m√≥dulos, conectores listos).

Si les dices que deben cambiar todo su software, no entrar√°n.

Cumplimiento legal y seguridad de datos

Les preocupa mucho GDPR (protecci√≥n de datos de clientes).

Necesitan que les garantices:

datos dentro de la UE,

privacidad,

trazabilidad de las decisiones de la IA.

Formaci√≥n y acompa√±amiento

No basta con instalar un sistema. Necesitan que ense√±es a sus equipos c√≥mo usarlo.

Formaci√≥n breve, pr√°ctica, y soporte continuado (‚Äúllama si tienes un problema‚Äù).

Resultados r√°pidos y visibles

Quieren ROI en semanas/meses, no en a√±os.

Por ejemplo: ‚ÄúCon este sistema, tus clientes obtienen respuesta autom√°tica en 24 h y reduces 40 % de llamadas‚Äù.

Soporte humano cercano

Valoran que la empresa sea de aqu√≠ (Valencia) para tener a qui√©n llamar y poder reunirse en persona.

Es una ventaja competitiva frente a grandes multinacionales impersonales.

üéØ Oportunidades claras para ti

Sector industrial valenciano: cer√°mica, textil, calzado, agroalimentario ‚Üí visi√≥n artificial, predicci√≥n de demanda, mantenimiento predictivo.

Comercio y turismo: chatbots multiling√ºes, asistentes de reservas, an√°lisis de rese√±as.

Administraci√≥n y despachos: automatizaci√≥n de facturas, contratos, clasificaci√≥n documental.

Servicios profesionales (abogados, asesor√≠as): IA para revisar documentos, generar borradores, responder clientes.

Pymes exportadoras: traducci√≥n autom√°tica especializada, an√°lisis de mercados.
```

### Oportunidades de negocio previsibles en el sector

```
IA

Big Data

Robotica

Comp Cuantica

Meta ha presentado sus gafas

Ideas generadas:

Un **buen producto para empezar como MVP en Valencia** ser√≠a:

## üì¶ MVP: Clasificador autom√°tico de facturas y tickets (document AI sencillo)

### ‚úÖ Por qu√© este producto

* **Problema claro y universal**: todas las pymes manejan facturas, tickets y documentos contables.
* **Dolor inmediato**: pierden tiempo introduciendo datos a mano en ERP, facturaci√≥n o Excel.
* **Beneficio directo**: ahorro de horas administrativas, reducci√≥n de errores, cumplimiento con Hacienda.
* **Mercado amplio**: desde microempresas hasta asesor√≠as que gestionan decenas de clientes.
* **Tecnolog√≠a asequible**: no requiere entrenar un gran modelo desde cero, se puede montar con OCR + modelos de clasificaci√≥n/extracci√≥n + integraci√≥n sencilla.

### üöÄ C√≥mo plantearlo en versi√≥n m√≠nima viable

1. **Subida de documento (PDF, foto de ticket)** ‚Üí v√≠a web o email.
2. **OCR e IA** ‚Üí extrae: proveedor, fecha, base imponible, IVA, total.
3. **Exportaci√≥n** ‚Üí a CSV/Excel o integraci√≥n con software de facturaci√≥n b√°sico.
4. **Interfaz simple** ‚Üí el usuario valida y corrige si hace falta.

### üí∞ Modelo de negocio

* Suscripci√≥n mensual: **50‚Äì150 ‚Ç¨/mes** seg√∫n volumen de documentos.
* Opci√≥n de **pago por uso** para micropymes (ej. 0,10 ‚Ç¨ por factura procesada).
* Hosting en servidores europeos ‚Üí se cumple GDPR, algo muy sensible para asesor√≠as y pymes.

### üå± Escalabilidad futura

* M√≥dulos adicionales: gesti√≥n de contratos, clasificaci√≥n de correos, conciliaci√≥n bancaria.
* Integraciones con ERP/CRM locales (ej. Aitana, Sage, Holded, Odoo).
* Verticalizaci√≥n: adaptaciones para comercio, industria, asesor√≠as.

---

üëâ Con este MVP arrancas con **algo sencillo, necesario, r√°pido de probar y mostrar ROI inmediato**.
Despu√©s puedes abrir otros verticales (chatbots, visi√≥n artificial industrial, predicci√≥n de demanda).

¬øQuieres que te dise√±e un **prototipo funcional paso a paso** (stack t√©cnico, arquitectura y demo m√≠nima en Python/Flask o Node) para que lo uses con empresas piloto en Valencia?

-----------------------------------

Otro proyecto:

Perfecto üöÄ, aqu√≠ va otra idea distinta de MVP para pymes en Valencia:

---

## ü§ñ MVP: Chatbot multiling√ºe de atenci√≥n al cliente (espa√±ol/valenciano/ingl√©s)

### ‚úÖ Por qu√© este producto

* **Dolor inmediato**: muchas pymes (tiendas, turismo, restauraci√≥n, academias) reciben decenas de consultas repetitivas (horarios, reservas, precios, disponibilidad).
* **Beneficio directo**: ahorran tiempo del personal, dan respuesta inmediata 24/7 y mejoran la experiencia del cliente.
* **Diferenciaci√≥n local**: soporte en **valenciano** y adaptaci√≥n cultural ‚Üí ventaja frente a chatbots gen√©ricos de grandes plataformas.

### üöÄ C√≥mo plantearlo en versi√≥n m√≠nima viable

1. **Widget web + WhatsApp/Telegram** ‚Üí integraci√≥n r√°pida en la web de la pyme o en su canal de mensajer√≠a.
2. **FAQ b√°sicas cargadas en el sistema** ‚Üí horarios, direcciones, precios, productos, reservas.
3. **IA ligera** para responder variaciones ‚Üí con un modelo preentrenado (no hace falta entrenar desde cero).
4. **Transferencia a humano** ‚Üí si la pregunta no se entiende, el chatbot env√≠a la consulta al email/WhatsApp del negocio.

### üí∞ Modelo de negocio

* Suscripci√≥n mensual: **30‚Äì100 ‚Ç¨/mes** por pyme seg√∫n n√∫mero de interacciones.
* Valor a√±adido: personalizaci√≥n con logo, colores, tono de comunicaci√≥n en valenciano/castellano.
* Coste muy bajo para arrancar: un solo servidor puede dar servicio a muchas pymes.

### üå± Escalabilidad futura

* Conectar con reservas online (hosteler√≠a, academias, turismo).
* Integrar con CRM/ERP (ej. registrar leads, abrir tickets).
* A√±adir anal√≠ticas: qu√© preguntan m√°s los clientes, picos de demanda, idiomas m√°s usados.

---

üëâ Este MVP es ideal porque combina **impacto r√°pido, bajo coste de implantaci√≥n y atractivo para comercio/turismo valenciano**, que son sectores muy potentes en la regi√≥n.

¬øQuieres que te prepare una **comparativa r√°pida** entre este chatbot y el de facturas (coste de desarrollo, rapidez en conseguir clientes, potencial de escalado) para decidir cu√°l atacar primero?


sobre la diferenciaci√≥n:

Muy buena observaci√≥n üëå. Lo de ‚Äúmultiling√ºe valenciano‚Äù es un **plus simp√°tico**, pero no es un **driver de compra fuerte** para la pyme. Si quieres que este chatbot tenga **peso real en el mercado local**, la diferenciaci√≥n debe atacar **dolores que de verdad importan a las empresas**: tiempo, dinero, clientes.

---

## üîë Diferenciaciones m√°s potentes para el chatbot

1. **Integraci√≥n directa con WhatsApp Business**

   * En Valencia (y en toda Espa√±a), WhatsApp es el **canal de comunicaci√≥n N¬∫1** de las pymes con clientes.
   * Si el chatbot funciona **dentro de WhatsApp**, sin obligar a instalar nada m√°s ‚Üí √©xito seguro.
   * Ejemplo: restaurantes, talleres, academias ‚Üí reservas, citas y consultas llegan ya por WhatsApp.

2. **Conexi√≥n con herramientas que ya usan**

   * Exportar leads a Excel, enviar facturas, registrar citas en Google Calendar o directamente en el ERP.
   * Si ahorras trabajo administrativo **adem√°s de contestar**, pasas de ‚Äúgadget‚Äù a ‚Äúherramienta necesaria‚Äù.

3. **Chatbot con ventas integradas**

   * Que no solo responda, sino que **concrete una acci√≥n**:

     * Enviar link de pago.
     * Confirmar reserva.
     * Generar presupuesto b√°sico.
   * Eso s√≠ impacta en ingresos, y por tanto justifica el gasto.

4. **Anal√≠tica de clientes**

   * Panel donde la pyme vea qu√© preguntan m√°s los clientes, a qu√© horas, en qu√© idioma.
   * Informaci√≥n pr√°ctica para mejorar negocio (ej: ‚Äúla mayor√≠a preguntan por producto X que no tenemos en web‚Äù).

---

## üí° Alternativa de proyecto (si quieres pivotar)

Otro MVP muy viable ser√≠a:

### üìä Dashboard de ventas inteligente para pymes

* Se conecta a su facturaci√≥n (muchas usan programas sencillos o incluso Excel).
* Les da **gr√°ficas simples pero √∫tiles**:

  * Qu√© productos se venden m√°s.
  * Qu√© clientes compran m√°s.
  * Predicci√≥n de demanda (b√°sica, con hist√≥rico de ventas).
* Diferencia: la mayor√≠a de pymes no tienen BI ‚Üí un ‚ÄúPower BI a medida y f√°cil‚Äù para micronegocios puede ser brutal.

---

üëâ En resumen:

* El **chatbot** es muy viable si lo diferencias con **WhatsApp + acciones concretas + anal√≠tica de clientes**.
* El **dashboard de ventas** puede ser incluso m√°s atractivo para empresarios cl√°sicos que **quieren ver n√∫meros claros y tomar decisiones**.

¬øQuieres que te arme una **matriz comparativa r√°pida (chatbot vs dashboard)** con: inversi√≥n inicial, facilidad de venta en Valencia, tiempo para ver ROI y escalabilidad?
```

### Tipo de proyecto requerido para dar respuesta a las demandas previstas

```markdown
Tipo de proyecto es un desarrollo inform√°tico

Servidor de IA con potencia suficiente para arrancar un hilo, y potencia deseable para mantener varios hilos
Para un primer prototipo puedo usar una API externa - pero siendo realistas, necesito LLM instalado en local en el que pueda tener m√°s control

Fondo de modelo instalado - hoy (26 sept- ollama) - estoy analizando vllm que son modelos m√°s escalables y distribuidos

Python como lenguaje para gestionar la IA

Por una parte voy a tener que crear un front web que se conecte a un back
Por otra parte, voy a tener que conectar con la API de whatsapp/telegram

Este proyecto busca el progreso y la mejora tecnol√≥gica de los negocios de la zona geogr√°fica de Valencia, CV, Espa√±a
```

### CaracteriÃÅsticas especiÃÅficas del proyecto seguÃÅn los requerimientos

```markdown
x servidores con potencia suficiente como para ejecutar un modelo
cuantos servidores depende de cuantos clientes
hay un "problema" de escalabilidad

necesito un equipo de desarrollo (hardware)

no necesito personal inform√°tico

necesitar√≠a personal comercial

necesitar√≠a personal administrativo

quizas una oficina, x ordenadores, red, mobiliario, luz, agua
```

<a id="identificacion-de-los-aspectos-que-facilitan-o-dificultan-el-desarrollo-de-la-posible-intervencion"></a>
## Identificaci√≥n de los aspectos que facilitan o dificultan el desarrollo de la posible intervenci√≥n

### Obligaciones fiscales, laborales y de prevencioÃÅn de riesgos y sus condiciones de aplicacioÃÅn

```markdown
Obligaciones fiscales, laborales y de prevencioÃÅn de riesgos y sus condiciones de aplicacioÃÅn

Registro p√∫blico de algoritmos en la CV (Comunitat Valenciana), para transparencia de algoritmos en servicios p√∫blicos.

Generales:

alta como aut√≥nomo
alta como empresa

IVA m√°s declaraci√≥n 303 trimestral

IRPF autonomo o impuesto de sociedades

retenciones y modelos peri√≥dicos

obligaciones fiscales como libros de ingresos

Obligaciones fiscales espec√≠ficas de un SaaS de IA

Tratamiento de ingresos digitales

Aunque vendas un ‚Äúchatbot IA‚Äù, fiscalmente se considera prestaci√≥n de servicios digitales.

Si facturas a particulares de la UE ‚Üí se aplica el IVA del pa√≠s de destino (r√©gimen OSS, ventanilla √∫nica).

Si facturas a empresas de la UE con VAT v√°lido ‚Üí operaci√≥n intracomunitaria (IVA 0%, pero hay que declararlo en modelo 349).

GDPR y costes asociados (no es un impuesto, pero afecta):

Tendr√°s que documentar el cumplimiento de protecci√≥n de datos.

Si gestionas datos sensibles (clientes de tus clientes), podr√≠as necesitar un Delegado de Protecci√≥n de Datos (DPD).

Esto conlleva gastos de consultor√≠a legal recurrentes, que son fiscalmente deducibles.

Subvenciones y deducciones

Muchas l√≠neas de ayuda (IVACE, fondos europeos, ENISA) obligan a justificar gastos de I+D.

Existe la deducci√≥n fiscal por I+D+i en el Impuesto de Sociedades (puede llegar al 42%).

Importante llevar bien la contabilidad para aprovecharlo.

Licencias de software y royalties

Si usas APIs de terceros (ej. OpenAI, WhatsApp Cloud API), son gastos deducibles.

Pero si revendes servicios de un tercero, ojo: debes repercutir IVA en tu factura aunque el proveedor sea extranjero (mecanismo de inversi√≥n del sujeto pasivo).

Obligaciones con la Seguridad Social

Aunque sea SaaS, si contratas personal t√©cnico o comercial ‚Üí alta en R√©gimen General.

Posibilidad de bonificaciones si contratas perfiles j√≥venes o I+D.

Muy buen punto: una cosa es hablar de obligaciones fiscales; otra muy distinta (y emergente) es lo que la normativa de **transparencia algor√≠tmica / gobernanza de IA** puede exigirte como proveedor o usuario de sistemas de IA. Aqu√≠ va un panorama con lo que hasta hoy existe, lo que est√° en tramitaci√≥n, y las obligaciones (reales o previsibles) que podr√≠as tener que cumplir:

---

## üìö Lo que ya existe / se est√° desplegando

### Registro de algoritmos en la Comunitat Valenciana (‚ÄúGVA ConfIA‚Äù)

* La Generalitat Valenciana ha lanzado un registro p√∫blico de algoritmos llamado **GVA ConfIA** para que los ciudadanos conozcan cu√°ndo se usa un algoritmo, con qu√© finalidad, qu√© datos lo alimentan y qu√© garant√≠as existen frente a sesgos o decisiones injustas. ([DGTIC][1])
* En la actualidad est√° en fase piloto, e inicialmente contempla algoritmos de √°mbitos como salud, educaci√≥n y medioambiental. ([ESMARTCITY][2])
* Este registro es una obligaci√≥n de **publicidad activa de la administraci√≥n auton√≥mica valenciana**, para los algoritmos que esta utilice, bajo los principios de transparencia. ([Universitat de Val√®ncia][3])

Importante: este registro **no implica autom√°ticamente** que todas las empresas privadas deban inscribir sus algoritmos all√≠. Se dirige principalmente a las entidades p√∫blicas que usan sistemas automatizados para toma de decisiones administrativas. ([Open Government Partnership][4])

---

## ‚öñÔ∏è Normativa europea y espa√±ola sobre IA y transparencia algor√≠tmica

Para saber qu√© obligaciones tienes hoy o pr√≥ximamente, conviene mirar el nuevo marco regulador europeo y c√≥mo se traslada a Espa√±a:

1. **Reglamento de Inteligencia Artificial (IA Act / Ley de IA de la UE)**

   * El Reglamento de IA de la UE (aprobado en 2024) establece exigencias de transparencia, explicabilidad, trazabilidad y comunicaci√≥n de informaci√≥n a los usuarios en determinados sistemas de IA. ([Parlamento Europeo][5])
   * Los sistemas considerados de **alto riesgo** estar√°n sujetos a obligaciones m√°s estrictas (auditor√≠as, documentaci√≥n t√©cnica, vigilancia, evaluaci√≥n de riesgos). ([Parlamento Europeo][5])
   * Para sistemas m√°s ‚Äúligeros‚Äù, habr√° obligaciones de transparencia m√≠nimas (por ejemplo, informar al usuario que est√° interactuando con IA, l√≠mites del sistema, recomendaciones de uso). ([Agencia Espa√±ola de Protecci√≥n de Datos][6])

2. **Transparencia-RGPD / obligaciones de informaci√≥n cuando hay tratamiento de datos personales**

   * Bajo el RGPD y su desarrollo en Espa√±a (LOPDGDD), ya tienes obligaciones de transparencia hacia los usuarios cuyos datos procesas: informar qu√© datos, con qu√© finalidad, qui√©n es responsable, derechos, etc. ([Agencia Espa√±ola de Protecci√≥n de Datos][6])
   * Cuando la IA intervenga en decisiones automatizadas que afecten a las personas, hay obligaciones adicionales de informaci√≥n y posibilidad de interponer recursos humanos si la decisi√≥n les perjudica. ([Agencia Espa√±ola de Protecci√≥n de Datos][6])

3. **Ley de Igualdad de Trato / no discriminaci√≥n**

   * La Ley 15/2022 para la igualdad de trato incorpora obligaciones a evitar que las decisiones automatizadas basadas en algoritmos discriminen por raz√≥n de cualquier condici√≥n personal (sexo, origen, edad, discapacidad, etc.). ([Wikipedia][7])
   * Si tu sistema ‚Äúdecide‚Äù algo (o da recomendaciones) que puede tratar diferentemente a grupos protegidos, debes asegurarte de pruebas contra sesgos, auditor√≠as, transparencia en criterios.

4. **Obligaciones futuras / sanciones y cumplimiento exigente**

   * Las autoridades nacionales (como la Agencia Espa√±ola de Supervisi√≥n de la Inteligencia Artificial ‚Äî AESIA) ser√°n responsables de supervisar el cumplimiento de la normativa de IA en Espa√±a. ([Wikipedia][8])
   * Si incumples obligaciones de transparencia, no explicas decisiones automatizadas, no haces auditor√≠as o generas discriminaciones, podr√≠as estar sujeto a sanciones administrativas (el proyecto de IA Act prev√© multas para incumplimientos). ([Computing][9])
   * A partir de 2026 algunas obligaciones de etiquetado de contenido generado por IA entrar√°n en vigor (que los contenidos generados con IA deben indicar que lo son) seg√∫n el Reglamento de IA. ([Cinco D√≠as][10])

---

## üßæ Qu√© obligaciones espec√≠ficas podr√≠as tener para tu proyecto (chatbot / SaaS IA)

Partiendo del proyecto que quieres construir (chatbot IA / SaaS con procesamiento autom√°tico), aqu√≠ las obligaciones concretas que deber√≠as contemplar:

| Obligaci√≥n                                        | Qu√© implica para tu proyecto                                                                                                                                                      |
| ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Transparencia al usuario**                      | Informar claramente que el usuario est√° conversando con un sistema de IA, no con un humano, y cu√°les son las limitaciones del sistema.                                            |
| **Documentaci√≥n t√©cnica / trazabilidad**          | Mantener documentaci√≥n interna que describa c√≥mo se entrena el modelo, con qu√© datos, qu√© versiones del modelo usas, m√©tricas de sesgo, logs que permitan reconstruir decisiones. |
| **Explicabilidad**                                | Si el chatbot toma decisiones (ej. recomendar un producto, rechazar una petici√≥n), debes poder explicar de forma comprensible por qu√© lo hizo.                                    |
| **Evaluaci√≥n de riesgos / auditor√≠as**            | An√°lisis de riesgos en fases de dise√±o: qu√© puede salir mal (sesgo, discriminaci√≥n, errores), mitigaciones, controles. Auditor√≠a peri√≥dica del sistema.                           |
| **Protecci√≥n de datos / conformidad RGPD**        | Si el chatbot maneja datos personales (nombres, emails, historial), cumplir con consentimiento, derechos de acceso, rectificaci√≥n, supresi√≥n, portabilidad.                       |
| **No discriminaci√≥n**                             | Asegurarte de que el algoritmo no discrimine grupos protegidos (sexo, edad, origen). Realizar pruebas de equidad.                                                                 |
| **Etiquetado de contenido IA (a partir de 2026)** | Si el sistema genera texto, audio, imagen o v√≠deo, deber√°s marcarlo como generado por IA cuando la intervenci√≥n sea sustancial.                                                   |
| **Supervisi√≥n humana**                            | Tener mecanismos de supervisi√≥n humana, posibilidad de ‚Äúrecurrir‚Äù la respuesta del chatbot a un operador humano si el usuario lo solicita o si la decisi√≥n es cr√≠tica.            |
| **Conservaci√≥n de logs / registros**              | Conserva registro de interacciones, decisiones del sistema, versiones del modelo, para auditor√≠a futura o responsabilidad.                                                        |
| **Actualizaci√≥n / mantenimiento**                 | Si los datos cambian, debes revisar y reentrenar modelos, corregir sesgos emergentes, y documentar esos cambios.                                                                  |
| **Cooperar con supervisores**                     | Ante requerimientos de autoridades (por ejemplo AESIA, agencias de protecci√≥n de datos), tener capacidad de exhibir la documentaci√≥n y acreditar cumplimiento.                    |

---

## ‚ö†Ô∏è Lo que *no* parece que te exijan hoy pero podr√≠a exigirse

* Que inscribas tu algoritmo en **GVA ConfIA** (registro p√∫blico auton√≥mico) ‚Äî salvo que act√∫es como administraci√≥n p√∫blica valenciana.
* Que publiques en abierto el c√≥digo completo o modelos sensibles, salvo que la normativa lo requiera como parte de obligaciones de alto riesgo.
* Que anticipadamente cumplas todas las obligaciones del AI Act si tu sistema no est√° en la categor√≠a de ‚Äúalto riesgo‚Äù.

---

Si quieres, puedo revisar **precisamente para tu chatbot (versi√≥n MVP)** qu√© nivel de riesgo tendr√≠a bajo el Reglamento IA y qu√© obligaciones concretas legales (transparencia, auditor√≠a, registros) te corresponder√≠an, para que lo ajustes de inicio. ¬øHacemos eso?

[1]: https://dgtic.gva.es/es/-/la-generalitat-presenta-el-nuevo-registro-de-algoritmos-gva-confia-para-una-ia-transparente-y-responsable?redirect=%2Fes%2Factualidad%3Fp_p_id%3Dcom_liferay_asset_publisher_web_portlet_AssetPublisherPortlet_INSTANCE_0YobAjUX6lT2%26p_p_lifecycle%3D0%26p_p_state%3Dnormal%26p_p_mode%3Dview%26_com_liferay_asset_publisher_web_portlet_AssetPublisherPortlet_INSTANCE_0YobAjUX6lT2_delta%3D5%26p_r_p_resetCur%3Dfalse%26_com_liferay_asset_publisher_web_portlet_AssetPublisherPortlet_INSTANCE_0YobAjUX6lT2_cur%3D1&utm_source=chatgpt.com "La Generalitat presenta el nuevo Registro de Algoritmos 'GVA ConfIA' para una IA transparente y responsable - Tecnolog√≠as de la Informaci√≥n y las Comunicaciones"
[2]: https://www.esmartcity.es/2025/05/26/generalitat-valenciana-lanza-registro-algoritmos-uso-responsable-administracion?utm_source=chatgpt.com "La Generalitat Valenciana lanza un registro de algoritmos para su uso responsable en la administraci√≥n"
[3]: https://www.uv.es/cotino/publicaciones/informe_gvav3logos.pdf?utm_source=chatgpt.com "La implantaci√≥n de la transparencia algor√≠tmica"
[4]: https://www.opengovpartnership.org/es/the-open-gov-challenge/valencian-community-spain-create-a-public-algorithmic-registry/?utm_source=chatgpt.com "Comunidad Valenciana, Espa√±a Crear un registro algor√≠tmico p√∫blico"
[5]: https://www.europarl.europa.eu/topics/es/article/20230601STO93804/ley-de-ia-de-la-ue-primera-normativa-sobre-inteligencia-artificial?utm_source=chatgpt.com "Ley de IA de la UE: primera normativa sobre inteligencia ..."
[6]: https://www.aepd.es/prensa-y-comunicacion/blog/inteligencia-artificial-transparencia?utm_source=chatgpt.com "Inteligencia artificial: Transparencia | AEPD"
[7]: https://es.wikipedia.org/wiki/Ley_para_la_igualdad_de_trato_%28Espa%C3%B1a%29?utm_source=chatgpt.com "Ley para la igualdad de trato (Espa√±a)"
[8]: https://es.wikipedia.org/wiki/Agencia_Espa%C3%B1ola_de_Supervisi%C3%B3n_de_la_Inteligencia_Artificial?utm_source=chatgpt.com "Agencia Espa√±ola de Supervisi√≥n de la Inteligencia Artificial"
[9]: https://www.computing.es/administracion/la-ue-ha-activado-la-supervision-y-las-sanciones-sobre-la-inteligencia-artificial/?utm_source=chatgpt.com "Espa√±a pone en marcha control y multas a la inteligencia ..."
[10]: https://cincodias.elpais.com/legal/2025-09-17/las-empresas-deberan-etiquetar-los-contenidos-generados-por-ia-a-partir-de-agosto-de-2026.html?utm_source=chatgpt.com "Las empresas deber√°n etiquetar los contenidos generados por IA a partir de agosto de 2026"
```

### Posibles ayudas o subvenciones para la incorporacioÃÅn de las nuevas tecnologiÃÅas de produccioÃÅn o de servicio propuestas

```markdown
Posibles ayudas o subvenciones para la incorporacioÃÅn de las nuevas tecnologiÃÅas de produccioÃÅn o de servicio propuestas

En la Comunidad Valenciana, el proyecto RETECH (‚ÄúRed de Hubs en Inteligencia Artificial‚Äù) tiene un presupuesto cercano a los 11,86 millones de euros en ayudas para proyectos de IA y espacios de datos.

Estrategia de IA de la Generalitat Valenciana: documentos para orientar pol√≠ticas, apoyo a startups, coordinaci√≥n universidad-empresa

https://presidencia.gva.es/documents/80279719/169117420/Dossier_cas.pdf?utm_source=chatgpt.com

S√≠ ‚Äî adem√°s del proyecto **RETECH** y otros ya conocidos, hay varias l√≠neas de ayudas, subvenciones y convocatorias que podr√≠as aprovechar para tu proyecto de IA en Valencia. Aqu√≠ te las detallo, con lo que hay ahora y lo que es probable que aparezca:

---

## üéØ Ayudas / subvenciones vigentes o recientes relevantes

1. **Ayudas para proyectos de desarrollo experimental en IA / Espacios de datos (Comunidad Valenciana)**

   * L√≠nea de actuaci√≥n 1: proyectos de desarrollo experimental en IA. ([GVA][1])
   * L√≠nea de actuaci√≥n 2: impulso de espacios de datos sectoriales e interoperables. ([GVA][1])
   * En esa convocatoria, los gastos subvencionables incluyen personal, instrumental, consultor√≠a, servicios, suministros, etc. ([Inn√≥valis][2])
   * La convocatoria ya ha cerrado en su plazo (finaliz√≥ en enero de 2025). ([GVA][1])

2. **Tech FabLab CV ‚Äî Innovaci√≥n tecnol√≥gica para pymes**

   * Convocatoria para financiar proyectos de innovaci√≥n tecnol√≥gica dentro de la Comunidad Valenciana. ([techfablabcv.es][3])
   * Las tecnolog√≠as habilitadoras digitales admitidas incluyen IA, an√°lisis de datos, automatizaci√≥n, nube, entre otras. ([techfablabcv.es][3])
   * Dotaci√≥n: 1.800.000 ‚Ç¨ para proyectos con inversi√≥n entre 5.000 ‚Ç¨ y 50.000 ‚Ç¨. ([techfablabcv.es][3])
   * Plazo de presentaci√≥n: del 25 de julio de 2025 hasta el 30 de septiembre de 2025. ([techfablabcv.es][3])

3. **Kit Digital (Espa√±a)**

   * Programa nacional para subvencionar la adopci√≥n de soluciones digitales por pymes y aut√≥nomos. ([Acelera Pyme][4])
   * Las cuant√≠as dependen del tama√±o del negocio: por ejemplo, para empresas entre 0 y 2 trabajadores pueden tener hasta ‚âà 3.000 ‚Ç¨ para soluciones digitales. ([Impulsa Empresa][5])
   * El cat√°logo de soluciones digitales puede incluir servicios relacionados con IA, dependiendo de la convocatoria vigente. ([portal.mineco.gob.es][6])

4. **Convocatorias de Red.es para I+D / IA / tecnolog√≠as duales**

   * En 2025 se ha publicado una convocatoria para proyectos de investigaci√≥n y desarrollo en inteligencia artificial y tecnolog√≠as duales (C003/25-ED). ([Sede electr√≥nica][7])
   * Esta convocatoria busca promover la integraci√≥n de la IA en cadenas de valor productivas. ([Sede electr√≥nica][7])

5. **Ayudas de eventos tecnol√≥gicos en Valencia**

   * El Ayuntamiento de Valencia ha convocado ayudas para eventos tecnol√≥gicos, con importes entre 6.000 y 40.000 ‚Ç¨. ([Valencia][8])
   * No es directamente para el desarrollo de IA, pero puede servir para presentaci√≥n, promoci√≥n, networking del proyecto. ([Valencia][8])

6. **Agencia Valenciana de la Innovaci√≥n (AVI)**

   * Aunque no es una convocatoria espec√≠fica, la AVI es un organismo clave en la Comunidad Valenciana para apoyar proyectos de innovaci√≥n, coordinar ayudas, aportar cofinanciaci√≥n, fomentar colaboraci√≥n entre empresa, universidad e institutos tecnol√≥gicos. ([Wikipedia][9])

---

## ‚ö†Ô∏è Consideraciones clave y estrategias para maximizar las ayudas

* **Timing**: muchas convocatorias cierran plazos o se publican en periodos concretos. Debes estar atento a nuevas convocatorias de innovaci√≥n / IA en la Generalitat, Ministerio y fondos europeos.
* **Consorcios / cooperaci√≥n**: algunos proyectos de IA se valoran mejor si haces cooperaci√≥n entre empresas, universidades o centros tecnol√≥gicos.
* **Montos m√≠nimos y m√°ximos**: por ejemplo, en la ayuda de IA experimental de la CV, los proyectos individuales deben superar 100.000 ‚Ç¨. ([Inn√≥valis][2])
* **Intensidad de la subvenci√≥n**: en algunas ayudas se admite hasta un porcentaje elevado del coste del proyecto (p.ej. hasta 80 % para empresas) pero depende de la l√≠nea. ([Inn√≥valis][2])
* **Gastos elegibles**: personal, consultor√≠a, servicios externos, material, equipos, etc., siempre que est√©n justificados y sean necesarios para el proyecto.
* **Cumplimiento y justificaci√≥n**: tendr√°s que llevar una contabilidad rigurosa, justificar los gastos con facturas, plazos, memoria t√©cnica, resultados.
* **Ayudas vinculadas a I+D+i**: m√°s all√° de subvenciones directas, hay deducciones fiscales por I+D+i que pueden aplicar si el proyecto t√©cnico tiene elementos de innovaci√≥n.

---

¬øTe interesa que yo filtre **las ayudas activas ahora mismo para proyectos de IA en Valencia (2025/2026)**, con plazos y montos, y te env√≠e unos formularios modelo que puedas usar para tu chatbot?

[1]: https://www.gva.es/es/inicio/procedimientos?id_proc=96650&utm_source=chatgpt.com "Ayudas para proyectos de desarrollo experimental en el ..."
[2]: https://innovalis.es/ayudas-proyectos-de-desarrollo-experimental-en-el-area-de-la-ia/?utm_source=chatgpt.com "AYUDAS PROYECTOS DE DESARROLLO ..."
[3]: https://techfablabcv.es/ayudas-financiacion-proyectos-innovacion-comunidad-valenciana/?utm_source=chatgpt.com "Convocatoria de ayudas para la financiaci√≥n de proyectos de ..."
[4]: https://www.acelerapyme.gob.es/kit-digital?utm_source=chatgpt.com "Kit Digital"
[5]: https://www.impulsa-empresa.es/guia-kit-digital/?utm_source=chatgpt.com "Gu√≠a del Kit Digital 2025: todas las claves"
[6]: https://portal.mineco.gob.es/ca-es/comunicacion/Pagines/211125_np_kit.aspx?utm_source=chatgpt.com "El Gobierno lanza el programa Kit Digital para invertir m√°s ..."
[7]: https://sede.red.gob.es/es/procedimientos/convocatoria-de-ayudas-2025-destinadas-proyectos-de-investigacion-y-desarrollo-en?utm_source=chatgpt.com "CONVOCATORIA DE AYUDAS 2025 DESTINADAS A ..."
[8]: https://www.valencia.es/es/-/subvenciones-eventos-tecnol%C3%B3gicos-2025?redirect=%2Fes%2Fcas%2Fpagina-de-categoria%3Fp_p_id%3Dcom_liferay_asset_publisher_web_portlet_AssetPublisherPortlet_INSTANCE_JzpZn9jBadjS%26p_p_lifecycle%3D0%26p_p_state%3Dnormal%26p_p_mode%3Dview%26p_r_p_tag%3Deventos%2Btecnol%25C3%25B3gicos&utm_source=chatgpt.com "Val√®ncia impulsa la innovaci√≥n con 500.000 euros en ..."
[9]: https://es.wikipedia.org/wiki/Agencia_Valenciana_de_la_Innovaci%C3%B3n?utm_source=chatgpt.com "Agencia Valenciana de la Innovaci√≥n"

¬°Claro! Aqu√≠ tienes los enlaces en texto plano para que los puedas visitar despu√©s:

---

### üìå Comunitat Valenciana

* Ayudas para proyectos de desarrollo experimental en IA / espacios de datos (GVA):
  [https://www.gva.es/es/inicio/procedimientos?id_proc=96650](https://www.gva.es/es/inicio/procedimientos?id_proc=96650)

* Noticia sobre convocatoria IA CV (Innovalis):
  [https://innovalis.es/ayudas-proyectos-de-desarrollo-experimental-en-el-area-de-la-ia/](https://innovalis.es/ayudas-proyectos-de-desarrollo-experimental-en-el-area-de-la-ia/)

* Presentaci√≥n GVA ConfIA (registro de algoritmos):
  [https://dgtic.gva.es/es/-/la-generalitat-presenta-el-nuevo-registro-de-algoritmos-gva-confia-para-una-ia-transparente-y-responsable](https://dgtic.gva.es/es/-/la-generalitat-presenta-el-nuevo-registro-de-algoritmos-gva-confia-para-una-ia-transparente-y-responsable)

* Estrategia de IA de la Generalitat (dossier oficial):
  [https://presidencia.gva.es/documents/80279719/169117420/Dossier_cas.pdf](https://presidencia.gva.es/documents/80279719/169117420/Dossier_cas.pdf)

---

### üìå Programas y hubs

* Tech FabLab CV ‚Äì ayudas innovaci√≥n tecnol√≥gica pymes (2025):
  [https://techfablabcv.es/ayudas-financiacion-proyectos-innovacion-comunidad-valenciana/](https://techfablabcv.es/ayudas-financiacion-proyectos-innovacion-comunidad-valenciana/)

---

### üìå Nacionales

* Kit Digital ‚Äì programa nacional de digitalizaci√≥n de pymes:
  [https://www.acelerapyme.gob.es/kit-digital](https://www.acelerapyme.gob.es/kit-digital)

* Gu√≠a del Kit Digital (Impulsa Empresa):
  [https://www.impulsa-empresa.es/guia-kit-digital/](https://www.impulsa-empresa.es/guia-kit-digital/)

* Nota de prensa oficial del Ministerio de Econom√≠a:
  [https://portal.mineco.gob.es/ca-es/comunicacion/Pagines/211125_np_kit.aspx](https://portal.mineco.gob.es/ca-es/comunicacion/Pagines/211125_np_kit.aspx)

---

### üìå Red.es y convocatorias I+D

* Convocatoria Red.es 2025 para proyectos de IA y tecnolog√≠as duales:
  [https://sede.red.gob.es/es/procedimientos/convocatoria-de-ayudas-2025-destinadas-proyectos-de-investigacion-y-desarrollo-en](https://sede.red.gob.es/es/procedimientos/convocatoria-de-ayudas-2025-destinadas-proyectos-de-investigacion-y-desarrollo-en)

---

### üìå Ayuntamiento de Val√®ncia

* Subvenciones para eventos tecnol√≥gicos 2025:
  [https://www.valencia.es/es/-/subvenciones-eventos-tecnol%C3%B3gicos-2025](https://www.valencia.es/es/-/subvenciones-eventos-tecnol%C3%B3gicos-2025)

---

### üìå Organismos

* Agencia Valenciana de la Innovaci√≥n (AVI):
  [https://es.wikipedia.org/wiki/Agencia_Valenciana_de_la_Innovaci%C3%B3n](https://es.wikipedia.org/wiki/Agencia_Valenciana_de_la_Innovaci%C3%B3n)

---

¬øQuieres que te prepare un **mapa cronol√≥gico (2025‚Äì2026)** con fechas de apertura/cierre de estas convocatorias para que sepas **cu√°ndo aplicar a cada una**?
```

### GuioÃÅn de trabajo que se va a seguir para la elaboraci√≥n del proyecto

```markdown
GuioÃÅn de trabajo que se va a seguir para la elaboraci√≥n del proyecto
```

### Peligros de la IA

```markdown
Existe el riesto de la explosi√≥n de una burbuja financiera de la IA
Hay una sobrevaloraci√≥n de las empresas de IA
Existe riesgo de pinchazo financiero

Precedente: La burbuja.com de 1999-2000
Se financiaron proyectos relacionados con internet
Se pensaba que todo lo relacionado con internet iba a ser oro

El famoso "caso Terra"

Al explotar la burbuja luego hay un valle de precaucion, de cuidado
Y luego se alcanca de nuevo el crecimiento
```


<a id="diseno"></a>
# Dise√±o

<a id="definicion-o-adaptacion-de-la-intervencion"></a>
## Definici√≥n o adaptaci√≥n de la intervencioÃÅn

### Introduccion

```markdown
Voy a crear un agente de IA que proporcione servicios personalizados de atenci√≥n al cliente o simiares para empresas de la zona de Valencia y Comunidad Valenciana.

El sistema de IA buscar√°:
1.-El cumplimiento normativo que corresponda
2.-En la medida de lo posible, almacenar y gestionar todos los sistemas inform√°ticos geogr√°ficamente en Valencia
3.-Eso incluye el coraz√≥n del proyecto, que es el motor de IA
4.-La mejora de la competitividad del tejido empresarial valenciano
5.-La implantaci√≥n de sistemas de IA en la sociedad valenciana de forma √©tica, responsable y a la vez efectiva
6.-Que la IA no sea una via de reemplazo del trabajo humano, sino que amplifique el trabajo humano, para que el ser humano pueda llegar donde dese√≥ llegar, y las limitaciones t√©cnicas se lo han impedido hasta el momento

El proyecto tendr√° varias partes
0.-Una infraestructura y un flujo de trabajo que permita entrenar (fine tune) un modelo previo con los datos espec√≠ficos de un cliente
1.-Un servidor en el cual existir√° un modelo de inteligencia artificial entrenado con los datos de un cliente (luego por lo tanto X modelos operativos)
2.-Un servidor web que proporcionar√° un servicio al p√∫blico general en forma de widget web
3.-Una conexi√≥n con la API de Whatsapp para poder ofrecer el servicio
```

<a id="priorizacion-y-secuenciacion-de-las-acciones"></a>
## Priorizaci√≥n y secuenciaci√≥n de las acciones.

### Introduccion

```markdown
1.-an√°lisis de diferentes motores LLM
  1.1.-Descarga de motores de ollama
  1.2.-Tengo que preparar una bateria de tests
  1.3.-Un conjunto de preguntas tipo de una empresa X
  1.4.-Podr√≠a contactar con una empresa - TAME
  1.5.-Lanzamiento de los tests
  1.6.-Recogida de resultados
  1.7.-Evaluaci√≥n y toma de decisiones
2.-Elecci√≥n de uno o varios modelos
  2.1.- Realizaremos entrenamoiento con el modelo establecido
3.-Comprobaci√≥n de entrenamiento
4.-Comprobaci√≥n de la viabilidad t√©cnica
5.-Instalaci√≥n del motor en un servidor
6.-Desarrollo del widget web
7.-conexi√≥n con API Whatsapp
8.-Desarrollo de un MVP - cliente muestra
9.-Proceso de venta de ese MVP
```

<a id="la-planificacion-de-la-intervencion"></a>
## La planificaci√≥n de la intervenci√≥n

### Introduccion

```markdown
Tipos de licencias en software:

GPL - con servicios de valor a√±adido o soporte mediante cobro - Modelo Odoo

Software privativo: Compras una licencia de por vida del software - tu la compras, y puedes usar el software siempre que quieras - las actualizaciones se pagan aparte
Ejemplo el software vale 2000 - la actualizaci√≥n de valor cuesta horquilla entre 200-500
Hist√≥ricamente ha sido objeto de pirateo

SaaS - Tu te quedas el software en tu servidor, al cliente le llega un acceso (hace a√±os era mediante Citrix o similares - actualmente mediante web) - El cliente paga
-Opcionalmente una cuota de puesta en marcha
-Obligatoriamente una cuota mensual por los servicios contratados
Es un modelo muy preferido porque es mas dificil de piratear
Exige tener servidores en marcha para dar el servicio

AIaaS - Software que proporciona a los clientes servicios de IA, pero la IA est√° alojada en tu servidor
```

### en mi caso concreto

```markdown
Yo quiero hacer:

Un SaaS orientado a centros de formaci√≥n
quiero que ese software tenga AIaaS

Un servidor levantado 24 horas al dia, con capacidad para IA:
Que servidor necesito?

Servidor web est√°ndar
2 cpu 2 ram un servidor estandar


para IA:
6 cores/12 threads Xeon
max 5.0GHz
32 GB
512 GB
60 euros al mes

Servidor con el que vamos a contar para este proyecto
i7 16nucleos
32 GB
1TB
Nvidia GTX 5060 16GB RAM

Para este proyecto se va a usar un servidor propio
```

<a id="determinacion-de-recursos"></a>
## Determinaci√≥n de recursos.

### Introduccion

```markdown
Recursos materiales
Recursos econ√≥micos
Recursos humanos

Recursos materiales:
-Recursos necesarios durante el desarrollo (fase de prototipo)
-Recursos necesarios durante la implantaci√≥n/puesta en marcha/mantenimiento

Supuesto recursos humanos: 
Soporte 24 horas al d√≠a en una aplicaci√≥n inform√°tica
24 horas al d√≠a = 3 turnos de 8 horas
Supuesto= 1200‚Ç¨ al mes
Tres personas cobrando 1200 euros al mes = 3600 euros al mes
Multiplicado por 12 meses son 43200 euros al a√±o solo en salarios - netos
Para que una persona cobre 1200 euros al mes, tu como empresa pagas en torno a 1570 al mes 
De 43200 podemos pasar a en torno a 52.000 mas o menos brutos

Supuesto recursos materiales:
Digamos que lo desarrollas en tu casa
En torno a 1000 euros equipo inform√°tico para desarrollo, implantaci√≥no, puesta en marcha y mantenimiento - un equipo para el desarrollador
Otro equipo para el equipo de soporte
Supuesto de alquilar oficina 800 euros al mes

En mi caso supuesto de aplicaci√≥n de IA educativa
Minino servidor 60 euros al mes, 720 euros al a√±o

Recursos econ√≥micos / dificilmente tangibles:
-Campa√±a de marketing 500 euros durante dos meses
-Un comercial contratado dos meses 1000 euros al mes

Total de costes aproximados en primer a√±o: 
60.000 euros el primer a√±o

Qu√© vamos a hacer para compensar esos 60.000?

Como minimo ganar 80.000 euros, para tener un beneficio de 20.000‚Ç¨
1.000 euros al mes
Y dejamos en la empresa 8000 euros para fondo, reinversi√≥n, socorros, etc

La pregunta es:
-O bajamos los gastos
-O aseguramos ingresos

Aseguramos ingresos desarrollando algo que pensemos que luego el mercado va a comprar.

Supuesto:
Aplicacion de facturacion con Verifactu
Holded Standard = 60 al mes * 12 = 720

Supuesto de 8000 euros al a√±o por ticket (por licencia vendida anual) - tengo que hacer 10 clientes de 8000 euros cada uno

En el supuesto de competir en precio con Holded o similares - asumiendo mismo precio - 720 - 80000/720 = 112
Podr√≠amos vender 112 licencias de software en un a√±o?


Opci√≥n 2:
Si la estimaci√≥n econ√≥mica es muy alta, es muy fuerte, rebajemos recursos necesarios al m√≠nimo para poder arrancar el negocio:

1 desarrollador desarrolla, implanta, arranca,
ese mismo desarrollador hace el soporte
Equipo propio, servidor propio

1000 equipo de desarrollo
1000 servidor
1300euros * 12 meses = 
Total de 18.000 euros de presupuesto

Supuesto de que licencias cuestan 720 euros
Tendr√≠as que vender 25 licencias para empezar a alcanzar rentabilidad
```

### En mi caso particular

```markdown
Preguntas que me tengo que hacer:

Cuanto me va a costar el proceso de entrenamiento? Fine tune
Cuantas horas?
Cuantos tokens?

Que servidor m√≠nimo necesito para poner en marcha el sistema?
Voy a tener que probar diferentes motores, en diferentes equipos (maquinas virtuales) para comprobar cual es el minimo necesario
```

### Compra versus alquiler

```markdown
 NVMe
Intel¬ÆXeon¬Æ E-2356G
Rocket Lake
6 cores/12 threads x 3,2 GHz
max. 5,0 GHz Turbo Boost
32 GB
DDR4 ECC
512 GB
(2 x 512 GB NVMe SSD)
Software RAID 1  60 euros de alquiler al mes

Si compramos el mismo equipo, en lugar de alquilarlo:

An√°lisis de costes estimados

Voy a desglosar lo que implican los componentes que pides:

Procesador Intel¬Æ Xeon¬Æ E-2356G (Rocket Lake, 6 n√∫cleos / 12 hilos, 3,2 GHz base / hasta 5,0 GHz turbo) ‚Äî como referencia, este CPU cuesta unos ‚Ç¨350‚Äì400 solo el procesador (sin IVA) en Espa√±a. 
Senetic
+1

Placa base / sistema que soporte DDR4 ECC, NVMe, software RAID, etc. ‚Äî deber√° ser una placa de servidor/tipo workstation con buen soporte para RAID por hardware/firmware o por BIOS/software, con ranuras PCIe para NVMe, etc. Podr√≠a costar entre ‚Ç¨250 y ‚Ç¨500 o m√°s, seg√∫n la marca y funciones (controladoras, puertos, gesti√≥n remota, etc.).

Memoria 32 GB DDR4 ECC ‚Äî puede costar unos ‚Ç¨80‚Äì150 dependiendo de marca, latencias, etc.

2 √ó 512 GB NVMe SSD ‚Äî para SSDs NVMe de calidad empresarial/buen rendimiento, puede rondar ‚Ç¨60‚Äì150 cada una, digamos unos ‚Ç¨120‚Äì300 en total para ambos (dependiendo de marca, velocidad, clase ‚Äì PCIe 3.0 vs 4.0, endurance, etc.).

Chasis / fuente de alimentaci√≥n / ventilaci√≥n / placa de soporte / conectores ‚Äî en servidores, el chasis puede ser bastante costoso, con fuentes redundantes, ventiladores, bah√≠as, gesti√≥n (IPMI, etc.). Esto podr√≠a agregar ‚Ç¨200 a ‚Ç¨600 o m√°s dependiendo de cu√°n ‚Äúservidor profesional‚Äù lo quieras.

Software / licencias / garant√≠a / soporte ‚Äî si incluyes garant√≠a extendida, soporte, licencias, esto puede subir bastante.

Sumando los costes de los componentes y la mano de obra, un precio razonable para construir (o comprar) ese equipo ‚Äúbare metal‚Äù podr√≠a estar en rango:

Estimaci√≥n baja: ~ ‚Ç¨1,000 ‚Äì ‚Ç¨1,500
Estimaci√≥n media / realista: ~ ‚Ç¨1,500 ‚Äì ‚Ç¨2,500
Estimaci√≥n superior / con marca de servidor, garant√≠a premium: ‚Ç¨2,500 ‚Äì ‚Ç¨3,000+

digamos 2000 euros

Suponemos un periodo de amortizaci√≥n de 2 a√±os

con una amortizaci√≥n de 24 meses, te sale a 84 euros al mes:

O lo que es lo mismo, si el alquiler es de 60 euros, cuantos meses hacen falta para recuperar la inversion:

HAcen falta 33 meses = casi 3 a√±os
Pero problema: dentro de 3 a√±os, ese ordenador est√° para cambiar

Eso sin contar la eelectricidad que consume ese servidor
Eso sin contar con el coste del ancho de banda del servidor
```

### Recursos finales

```markdown
Instalamos un servidor dedicado en la empresa
Los calculos se realizar√°n en un servidor interno

Ventajas: Independencia total
Control absoluto
Disponer de un equipo que no dependa del pago de una cuota mensual

Desventajas: 
Tiene una gr√°fica "de videojuegos"
No tiene una gr√°fica "pura IA" H100
Deberemos cuantizar y deberemos optimizar el proceso de entrenamiento
```

<a id="planificacion-de-la-evaluacion"></a>
## Planificaci√≥n de la evaluacioÃÅn

<a id="diseno-de-documentacion"></a>
## Dise√±o de documentaci√≥n

<a id="plan-de-atencion-al-cliente"></a>
## Plan de atenci√≥n al cliente


<a id="organizacion"></a>
# Organizacion

<a id="deteccion-de-demandas-y-necesidades"></a>
## Detecci√≥n de demandas y necesidades

<a id="programacion"></a>
## Programaci√≥n

<a id="gestion"></a>
## Gesti√≥n

<a id="coordinacion-y-supervision-de-la-intervencion"></a>
## Coordinaci√≥n y supervisi√≥n de la intervenci√≥n

<a id="elaboracion-de-informes"></a>
## Elaboraci√≥n de informes

<a id="seguimiento-y-control"></a>
## Seguimiento y control


<a id="actividades-profesionales"></a>
# Actividades profesionales

<a id="areas-de-sistemas-y-departamentos-de-informatica-en-cualquier-sector-de-actividad"></a>
## √Åreas de sistemas y departamentos de inform√°tica en cualquier sector de actividad

<a id="sector-de-servicios-tecnologicos-y-comunicaciones"></a>
## Sector de servicios tecnol√≥gicos y comunicaciones

<a id="area-comercial-con-gestion-de-transacciones-por-internet"></a>
## √Årea comercial con gesti√≥n de transacciones por Internet


<a id="lineas-de-actuacion"></a>
# Lineas de actuaci√≥n

<a id="la-ejecucion-de-trabajos-en-equipo"></a>
## La ejecuci√≥n de trabajos en equipo

<a id="la-autoevaluacion-del-trabajo-realizado"></a>
## La autoevaluaci√≥n del trabajo realizado

<a id="la-autonomia-y-la-iniciativa"></a>
## La autonom√≠a y la iniciativa

<a id="el-uso-de-las-tic"></a>
## El uso de las TIC


<a id="bibliografia"></a>
# Bibliograf√≠a


<a id="proyectos"></a>
# Proyectos

<a id="empresarialia"></a>
## empresarialIA

<a id="agente-virtual"></a>
## agente virtual

<a id="ejercicio-de-final-de-unidad"></a>
## Ejercicio de final de unidad

### ejercicio

```markdown

```

<a id="examen-final"></a>
## Examen final

### crear tablas

```sql
CREATE DATABASE portafolioceac;

USE portafolioceac;


CREATE TABLE Piezas(
  Identificador INT auto_increment PRIMARY KEY,
  titulo VARCHAR(255),
  descripcion VARCHAR(255),
  imagen VARCHAR(255),
  url VARCHAR(255),
  id_categoria INT
);

CREATE TABLE Categorias(
  Identificador INT auto_increment PRIMARY KEY,
  titulo VARCHAR(255),
  descripcion VARCHAR(255)
);
```

### insertar

```sql
INSERT INTO Categorias VALUES(
  NULL,
  'General',
  'Esta es la categoria general'
);

INSERT INTO Piezas VALUES(
  NULL,
  'Primera pieza',
  'Esta es la descripcion de la primera pieza',
  'josevicente.jpg',
  'https://jocarsa.com',
  1
);
```

### fk

```sql
ALTER TABLE Piezas
ADD CONSTRAINT fk_piezas_categorias
FOREIGN KEY (id_categoria) REFERENCES Categorias(identificador)
ON DELETE CASCADE
ON UPDATE CASCADE;
```

### selecciones

```sql
SELECT * FROM Categorias;

SELECT * FROM Piezas;
```

### left join

```sql
SELECT 
* 
FROM Piezas
LEFT JOIN Categorias
ON Piezas.id_categoria = Categorias.Identificador;
```

### ahora creo la vista

```sql
CREATE VIEW piezas_y_categorias AS 
SELECT 
Categorias.titulo AS categoriatitulo,
Categorias.descripcion AS categoriadescripcion,
Piezas.titulo AS piezatitulo,
Piezas.descripcion AS piezadescripcion,
imagen,
url
FROM Piezas
LEFT JOIN Categorias
ON Piezas.id_categoria = Categorias.Identificador;

SELECT * FROM piezas_y_categorias;
```

### usuario

```sql
-- crea usuario nuevo con contrase√±a
-- creamos el nombre de usuario que queramos
CREATE USER 
'portafolioceac'@'localhost' 
IDENTIFIED  BY 'portafolioceac';

-- permite acceso a ese usuario
GRANT USAGE ON *.* TO 'portafolioceac'@'localhost';
--[tuservidor] == localhost
-- La contrase√±a puede requerir Mayus, minus, numeros, caracteres, min len

-- quitale todos los limites que tenga
ALTER USER 'portafolioceac'@'localhost' 
REQUIRE NONE 
WITH MAX_QUERIES_PER_HOUR 0 
MAX_CONNECTIONS_PER_HOUR 0 
MAX_UPDATES_PER_HOUR 0 
MAX_USER_CONNECTIONS 0;

-- dale acceso a la base de datos empresadam
GRANT ALL PRIVILEGES ON portafolioceac.* 
TO 'portafolioceac'@'localhost';

-- recarga la tabla de privilegios
FLUSH PRIVILEGES;
```


<a id="proyectos-segundo-trimestre"></a>
# Proyectos segundo trimestre

<a id="extractor-transcripciones"></a>
## Extractor transcripciones

### videos

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Expande una lista de playlists de YouTube (array de URLs) en un JSON por playlist.

Entrada:
  playlists_in.json  ->  [ "https://www.youtube.com/playlist?list=...", ... ]

Salida:
  Carpeta playlists_expandidas/ con un archivo JSON por playlist:

  {
    "id": "<playlist_id>",
    "title": "<playlist_title>",
    "url": "https://www.youtube.com/playlist?list=<playlist_id>",
    "items": [
      { "id": "...", "title": "...", "url": "...", "type": "video" },
      ...
    ]
  }

Requisitos: yt-dlp  (pip install yt-dlp)
"""

import json
import sys
import os
import re
from pathlib import Path
from urllib.parse import urlparse, parse_qs

try:
    from yt_dlp import YoutubeDL
except ImportError:
    print("ERROR: yt-dlp is required. Install with: pip install yt-dlp", file=sys.stderr)
    sys.exit(1)

INPUT_FILE = "listas.json"        # JSON con array de URLs de playlists
OUTPUT_DIR = Path("playlists_expandidas")

SAFE_CHARS = r"[^A-Za-z0-9√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú()_. -]"


def safe_filename(name: str) -> str:
    """Sanitiza un texto para usarlo como nombre de archivo."""
    name = name.strip().replace("/", "-").replace("\\", "-")
    name = re.sub(SAFE_CHARS, "", name)
    name = re.sub(r"\s+", " ", name)
    return name[:200] if len(name) > 200 else name


def extract_playlist_id(url):
    """Return playlist ID from any YouTube URL (via 'list=' query param), or None."""
    try:
        qs = parse_qs(urlparse(url).query)
        return qs.get("list", [None])[0]
    except Exception:
        return None


def fetch_playlist_title_and_items(playlist_url):
    """
    Return (title, items) for a playlist URL.
    items = [{id, title, url, type:"video"}, ...]
    """
    ydl_opts = {
        "extract_flat": True,
        "skip_download": True,
        "quiet": True,
        "nocheckcertificate": True,
        "noprogress": True,
    }

    with YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(playlist_url, download=False)

    if not info:
        return None, []

    playlist_title = info.get("title")
    entries = []
    if info.get("_type") == "playlist":
        entries = info.get("entries") or []
    else:
        entries = info.get("entries") or []
        if not entries and info.get("id") and info.get("title"):
            entries = [info]
            playlist_title = playlist_title or "Sin t√≠tulo"

    items = []
    for e in entries:
        vid = e.get("id")
        vtitle = e.get("title")
        vurl = f"https://www.youtube.com/watch?v={vid}" if vid else e.get("url")
        if vid and vtitle and vurl:
            items.append({"id": vid, "title": vtitle, "url": vurl, "type": "video"})

    if not playlist_title:
        playlist_title = "Sin t√≠tulo"

    return playlist_title, items


def disambiguate_title(title, existing_titles):
    """Ensure unique title within existing_titles (append ' (2)', ' (3)', ... if needed)."""
    if title not in existing_titles:
        return title
    i = 2
    while True:
        candidate = f"{title} ({i})"
        if candidate not in existing_titles:
            return candidate
        i += 1


def main():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    try:
        with open(INPUT_FILE, "r", encoding="utf-8") as f:
            playlists = json.load(f)
    except Exception as e:
        print(f"ERROR: no se pudo leer '{INPUT_FILE}': {e}", file=sys.stderr)
        sys.exit(1)

    if not isinstance(playlists, list):
        print(f"ERROR: '{INPUT_FILE}' debe ser un array JSON de URLs de playlists.", file=sys.stderr)
        sys.exit(1)

    used_titles = set()
    total_playlists = 0
    total_videos = 0

    seen_playlist_ids = set()

    for url in playlists:
        if not isinstance(url, str):
            print(f"WARNING: elemento no cadena en JSON, se ignora: {url}", file=sys.stderr)
            continue

        plist = extract_playlist_id(url)
        if not plist:
            print(f"WARNING: URL sin 'list=': {url}", file=sys.stderr)
            continue
        if plist in seen_playlist_ids:
            print(f"INFO: playlist ya procesada, se ignora duplicado: {plist}", file=sys.stderr)
            continue
        seen_playlist_ids.add(plist)

        normalized = f"https://www.youtube.com/playlist?list={plist}"
        print(f"Procesando playlist: {normalized}")

        try:
            ptitle, videos = fetch_playlist_title_and_items(normalized)
        except Exception as e:
            print(f"WARNING: could not fetch playlist {plist}: {e}", file=sys.stderr)
            ptitle, videos = f"Playlist {plist}", []

        unique_title = disambiguate_title(ptitle, used_titles)
        used_titles.add(unique_title)

        safe_title = safe_filename(unique_title)
        out_path = OUTPUT_DIR / f"{safe_title}.json"

        playlist_obj = {
            "id": plist,
            "title": unique_title,
            "url": normalized,
            "items": videos,
        }

        with out_path.open("w", encoding="utf-8") as f:
            json.dump(playlist_obj, f, ensure_ascii=False, indent=2)

        total_playlists += 1
        total_videos += len(videos)
        print(f"  -> Guardado {out_path.name} ({len(videos)} v√≠deos)")

    print("\nDone.")
    print(f"Playlists procesadas: {total_playlists}")
    print(f"V√≠deos totales:       {total_videos}")


if __name__ == "__main__":
    main()
```

### transcripciones

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Transcribe espa√±ol desde YouTube con yt-dlp (con fallback robusto).

Nueva l√≥gica:
- Lee TODOS los JSON de la carpeta 'playlists_expandidas/' (un JSON por playlist).
- Cada JSON tiene:
    {
      "id": "<playlist_id>",
      "title": "<playlist_title>",
      "url": "https://www.youtube.com/playlist?list=...",
      "items": [
        { "id", "title", "url", "type": "video" },
        ...
      ]
    }
- Para cada playlist crea una carpeta dentro de 'transcripts/' con el nombre saneado.
- Genera un .txt por cada v√≠deo (y .log en caso de problemas) dentro de esa carpeta.

L√≥gica de subt√≠tulos:
- Prueba player_client=mweb -> tv -> web -> android (android al final por PO token).
- Prioriza subt√≠tulos en espa√±ol (es, es-ES, es-419, es.*).
- Si no hay espa√±ol: toma el primer auto-caption disponible (p.ej. 'en').
- Como rescate final: --all-subs y luego filtra el mejor candidato.
- Convierte .vtt a texto s√≥lido (un p√°rrafo) sin timestamps, etiquetas, ni roll-up.
- Registra en .log los idiomas disponibles si no se consigue nada.

Requisitos:
  pip install -U yt-dlp
  (opcional) cookies.txt v√°lido (exportado del navegador)
"""

import json
import re
import subprocess
import sys
import time
from collections import deque
from pathlib import Path
from typing import Dict, Iterable, List, Tuple, Optional

# =======================
# CONFIG
# =======================
PLAYLISTS_DIR = Path("playlists_expandidas")  # entrada: JSON por playlist
OUT_BASE_DIR = Path("transcripts")            # salida: carpeta por playlist
SLEEP_BETWEEN = 2.0

COOKIES_FILE = Path("cookies.txt")  # cambia si lo tienes en otra ruta
USER_AGENT = "Mozilla/5.0"

# Ponemos android al final para evitar el problema del PO token si no lo usas
PLAYER_CLIENTS = ["mweb", "tv", "web", "android"]

SAFE_CHARS = r"[^A-Za-z0-9√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú()_. -]"

# Candidatos de idioma (prefijos y exactos) para espa√±ol:
SPANISH_LANG_CANDIDATES = [
    "es-ES",
    "es-419",
    "es",
    "es.*",   # es-AR, es-MX, es-CU, es.automatic, etc.
    "spa",    # poco com√∫n, pero a veces aparece
]

# Prioridad para elegir el VTT si hay varios
LANG_PRIORITY = [
    "es-ES",
    "es-419",
    "es",
    # variantes es.* despu√©s
]

# Fallback si no hay espa√±ol: coger el primer idioma disponible de automatic_captions
FALLBACK_TO_ANY_AUTO = True

# Si hay que caer a un auto-captions no espa√±ol, prioriza en este orden
FALLBACK_AUTO_PRIORITY = ["es-ES", "es-419", "es", "pt", "en"]

# =======================
# UTILIDADES GENERALES
# =======================
WS_RE   = re.compile(r"\s+")
_TS_RE  = re.compile(r"-->\s")         # l√≠neas de tiempo
_NUM_RE = re.compile(r"^\d+$")         # numeraci√≥n de cues
_TAG_RE = re.compile(r"<[^>]+>")       # etiquetas HTML/Style

IGNORE_PREFIXES = (
    "WEBVTT", "NOTE", "X-TIMESTAMP-MAP", "Kind:", "Language:",
    "STYLE", "REGION", "##", "align:", "position:", "line:", "size:"
)

NOISE_TOKENS = ("‚ô™", "‚ô´")


def safe_filename(name: str) -> str:
    name = name.strip().replace("/", "-").replace("\\", "-")
    name = re.sub(SAFE_CHARS, "", name)
    name = re.sub(r"\s+", " ", name)
    return name[:200] if len(name) > 200 else name


def load_json(path: Path) -> Dict:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def iter_videos_from_playlists_folder(folder: Path):
    """
    Recorre todos los JSON de 'folder' y genera:
      (playlist_title, playlist_safe, playlist_url, video_title, video_url, video_id)
    """
    if not folder.is_dir():
        raise FileNotFoundError(f"La carpeta de playlists '{folder}' no existe.")

    for json_file in sorted(folder.glob("*.json")):
        try:
            data = load_json(json_file)
        except Exception as e:
            print(f"WARNING: no se pudo leer {json_file.name}: {e}", file=sys.stderr)
            continue

        playlist_title = data.get("title") or json_file.stem
        playlist_id = data.get("id", "")
        playlist_url = data.get("url", "")
        playlist_safe = safe_filename(playlist_title or playlist_id or json_file.stem)

        items = data.get("items") or []
        for it in items:
            if not isinstance(it, dict):
                continue
            if it.get("type") and it.get("type") != "video":
                continue
            title = it.get("title") or f"video_{it.get('id', '')}"
            url = it.get("url") or (f"https://www.youtube.com/watch?v={it.get('id')}" if it.get("id") else "")
            vid = it.get("id") or ""
            if url and vid and title and "[Deleted video]" not in title:
                yield playlist_title, playlist_safe, playlist_url, title, url, vid


def normalize_line(s: str) -> str:
    s = _TAG_RE.sub("", s)
    for tok in NOISE_TOKENS:
        s = s.replace(tok, "")
    s = s.strip()
    s = WS_RE.sub(" ", s)
    return s


def vtt_to_solid_text(vtt_path: Path) -> str:
    if not vtt_path.exists():
        return ""
    lines_clean: List[str] = []
    last_seen = deque(maxlen=3)
    with vtt_path.open("r", encoding="utf-8", errors="ignore") as f:
        for raw in f:
            s = raw.strip()
            if not s:
                continue
            if s.startswith(IGNORE_PREFIXES):
                continue
            if _NUM_RE.fullmatch(s):
                continue
            if _TS_RE.search(s):
                continue
            s = normalize_line(s)
            if not s:
                continue
            if s in last_seen:
                continue
            lines_clean.append(s)
            last_seen.append(s)
    if not lines_clean:
        return ""
    text = " ".join(lines_clean)
    text = re.sub(r"\s+([,.!?;:])", r"\1", text)
    text = re.sub(r"\(\s+", "(", text)
    text = re.sub(r"\s+\)", ")", text)
    text = re.sub(WS_RE, " ", text).strip()
    return text

# =======================
# YT-DLP HELPERS
# =======================
def _build_common_cmd(url: str, base_out: Path) -> List[str]:
    cmd = [
        "yt-dlp",
        "--user-agent", USER_AGENT,
        "--skip-download",
        "--sub-format", "vtt/srv3",
        "--convert-subs", "vtt",
        "--output", str(base_out) + ".%(ext)s",
        url,
    ]
    if COOKIES_FILE.exists():
        cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
    return cmd


def _list_subs(url: str, player_client: str) -> str:
    """Devuelve el texto de --list-subs para logging."""
    cmd = _build_common_cmd(url, Path("dummy"))
    cmd += ["--list-subs", "--extractor-args", f"youtube:player_client={player_client}"]
    try:
        p = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        return p.stdout.decode("utf-8", errors="ignore")
    except subprocess.CalledProcessError as e:
        return e.stdout.decode("utf-8", errors="ignore") if e.stdout else str(e)


def _spanish_glob_candidates(base_out: Path) -> List[Path]:
    # Cualquier archivo que empiece por el nombre base y tenga .es*.vtt
    pattern = base_out.name + ".es*.vtt"
    return sorted(base_out.parent.glob(pattern))


def _lang_from_filename(path: Path, base_stem: str) -> str:
    # "<base>.<lang>.vtt" -> extrae <lang>
    name = path.name
    if name.startswith(base_stem + "."):
        rest = name[len(base_stem) + 1 :]
    else:
        rest = name
    if rest.lower().endswith(".vtt"):
        rest = rest[:-4]
    return rest


def _pick_best_spanish_vtt(base_out: Path) -> Optional[Tuple[Path, str]]:
    """Elige el mejor VTT espa√±ol disponible y devuelve (ruta, lang_code)."""
    candidates = _spanish_glob_candidates(base_out)
    if not candidates:
        return None

    def score(p: Path) -> Tuple[int, str]:
        lang = _lang_from_filename(p, base_out.name)
        for idx, tag in enumerate(LANG_PRIORITY):
            if lang == tag:
                return (idx, lang)
        if lang.startswith("es.") or lang.startswith("es-"):
            return (len(LANG_PRIORITY), lang)
        if lang == "es":
            return (2, lang)
        return (999, lang)

    ranked = sorted(((score(p), p) for p in candidates), key=lambda x: x[0])
    best = ranked[0][1]
    best_lang = _lang_from_filename(best, base_out.name)
    return best, best_lang

# =======================
# DESCARGA PRINCIPAL (con fallback)
# =======================
def download_spanish_with_ytdlp(url: str, base_out: Path) -> Optional[Tuple[Path, str, str]]:
    """
    1) Sonda -J para ver claves reales (subtitles + automatic_captions) y pide EXACTAMENTE las espa√±olas.
    2) Si no hay espa√±ol y FALLBACK_TO_ANY_AUTO=True: baja el primer auto-captions disponible
       (prioriza FALLBACK_AUTO_PRIORITY).
    3) Si aun as√≠ no hay nada usable, intenta rescate con --all-subs y filtra.
    Devuelve (ruta_vtt, lang_code, player_client) o None.
    """
    def probe_json(client: str) -> dict:
        cmd = [
            "yt-dlp",
            "--user-agent", USER_AGENT,
            "-J", url,
            "--extractor-args", f"youtube:player_client={client}",
        ]
        if COOKIES_FILE.exists():
            cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if p.returncode != 0:
            return {}
        try:
            return json.loads(p.stdout.decode("utf-8", "ignore"))
        except Exception:
            return {}

    def pick_spanish_keys(info: dict) -> List[str]:
        keys = set()
        for field in ("subtitles", "automatic_captions"):
            d = info.get(field) or {}
            for lang in d.keys():
                if lang == "es" or lang.startswith("es-") or lang.startswith("es.") or lang == "spa":
                    keys.add(lang)
        ordered = ["es-ES", "es-419", "es", *sorted([k for k in keys if k not in {"es-ES","es-419","es"}])]
        return [k for k in ordered if k in keys]

    def pick_fallback_auto_key(info: dict) -> Optional[str]:
        auto = info.get("automatic_captions") or {}
        if not auto:
            return None
        # 1) prioridad preferida
        for pref in FALLBACK_AUTO_PRIORITY:
            for lang in auto.keys():
                if lang == pref or lang.startswith(pref + ".") or (pref == "es" and (lang.startswith("es-") or lang.startswith("es."))):
                    return lang
        # 2) si no, la primera clave disponible
        for lang in auto.keys():
            return lang
        return None

    def run_download_with_langs(langs: List[str], client: str) -> Optional[Tuple[Path, str, str]]:
        cmd = [
            "yt-dlp",
            "--user-agent", USER_AGENT,
            "--skip-download",
            "--write-subs", "--write-auto-subs",
            "--sub-format", "vtt/srv3",
            "--convert-subs", "vtt",
            "--sub-langs", ",".join(langs),
            "--output", str(base_out) + ".%(ext)s",
            "--extractor-args", f"youtube:player_client={client}",
            url,
        ]
        if COOKIES_FILE.exists():
            cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            pick = _pick_best_spanish_vtt(base_out)
            if pick:
                vtt_path, lang_code = pick
                return (vtt_path, lang_code, client)
            # Si no hay .es*.vtt, quiz√° el idioma fue otro (fallback): busca cualquier .<lang>.vtt solicitado
            for lang in langs:
                maybe = base_out.with_name(base_out.name + f".{lang}.vtt")
                if maybe.exists():
                    return (maybe, lang, client)
        except subprocess.CalledProcessError:
            pass
        return None

    def all_subs_rescue(client: str) -> Optional[Tuple[Path, str, str]]:
        cmd = [
            "yt-dlp",
            "--user-agent", USER_AGENT,
            "--skip-download",
            "--all-subs",
            "--sub-format", "vtt/srv3",
            "--convert-subs", "vtt",
            "--output", str(base_out) + ".%(ext)s",
            "--extractor-args", f"youtube:player_client={client}",
            url,
        ]
        if COOKIES_FILE.exists():
            cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            # 1¬∫ intenta espa√±ol
            pick = _pick_best_spanish_vtt(base_out)
            if pick:
                vtt_path, lang_code = pick
                return (vtt_path, lang_code, client)
            # 2¬∫ si no hay espa√±ol, cualquier vtt priorizado
            vtts = sorted(base_out.parent.glob(base_out.name + ".*.vtt"))
            langs_to_paths = {p.name[len(base_out.name)+1:-4]: p for p in vtts}
            for pref in FALLBACK_AUTO_PRIORITY:
                for lang, path in langs_to_paths.items():
                    if lang == pref or lang.startswith(pref + ".") or (pref == "es" and (lang.startswith("es-") or lang.startswith("es."))):
                        return (path, lang, client)
            if vtts:
                p = vtts[0]
                lang = p.name[len(base_out.name)+1:-4]
                return (p, lang, client)
        except subprocess.CalledProcessError:
            pass
        return None

    # === Flujo principal por clientes ===
    for client in PLAYER_CLIENTS:
        info = probe_json(client)
        # 1) Idiomas espa√±oles exactos
        es_langs = pick_spanish_keys(info)
        if es_langs:
            got = run_download_with_langs(es_langs, client)
            if got:
                return got

        # 2) Fallback: primer auto disponible (p.ej. 'en'), si est√° activado
        if FALLBACK_TO_ANY_AUTO:
            fb = pick_fallback_auto_key(info)
            if fb:
                got = run_download_with_langs([fb], client)
                if got:
                    return got

        # 3) Rescate --all-subs y luego elegir
        got = all_subs_rescue(client)
        if got:
            return got

    # Segundo intento de rescate global si nada funcion√≥
    for client in PLAYER_CLIENTS:
        got = all_subs_rescue(client)
        if got:
            return got

    return None

# =======================
# MAIN
# =======================
def main():
    if not PLAYLISTS_DIR.is_dir():
        print(f"ERROR: la carpeta '{PLAYLISTS_DIR}' no existe. Ejecuta antes el script de expansi√≥n.", file=sys.stderr)
        sys.exit(1)

    OUT_BASE_DIR.mkdir(parents=True, exist_ok=True)

    total = 0
    ok = 0
    skipped = 0
    failed = 0

    for playlist_title, playlist_safe, playlist_url, title, url, vid in iter_videos_from_playlists_folder(PLAYLISTS_DIR):
        total += 1

        playlist_dir = OUT_BASE_DIR / playlist_safe
        playlist_dir.mkdir(parents=True, exist_ok=True)

        base_name = safe_filename(title)
        base_out = playlist_dir / base_name     # sin extensi√≥n
        txt_path = playlist_dir / f"{base_name}.txt"
        log_path = playlist_dir / f"{base_name}.log"

        if txt_path.exists():
            print(f"[SKIP] ({playlist_title}) Ya existe: {txt_path.name}")
            continue

        print(f"[{total:03d}] [{playlist_title}] {title} -> {url}")

        # 1) Descargar subt√≠tulos (espa√±ol o fallback)
        pick = download_spanish_with_ytdlp(url, base_out)

        if not pick:
            skipped += 1
            # Para diagn√≥stico: listar subs disponibles por cada client
            lists = []
            for client in PLAYER_CLIENTS:
                ls = _list_subs(url, client)
                lists.append(f"\n--- --list-subs (client={client}) ---\n{ls}")
            log_path.write_text(
                f"No se pudieron obtener subt√≠tulos.\n"
                f"PLAYLIST:{playlist_title}\nPLAYLIST_URL:{playlist_url}\n"
                f"TITLE:{title}\nURL:{url}\nID:{vid}\n"
                f"Intentos: espa√±ol -> fallback_any_auto -> all-subs\n"
                f"{''.join(lists)}\n",
                encoding="utf-8"
            )
            print("  ¬∑ No hay subt√≠tulos utilizables (registrado .log)")
            time.sleep(SLEEP_BETWEEN)
            continue

        vtt_path, lang_code, client_used = pick

        # 2) Convertir VTT -> TEXTO S√ìLIDO
        try:
            solid = vtt_to_solid_text(vtt_path)
            if not solid:
                failed += 1
                log_path.write_text(
                    f"VTT vac√≠o o no legible tras limpieza.\n"
                    f"PLAYLIST:{playlist_title}\nPLAYLIST_URL:{playlist_url}\n"
                    f"TITLE:{title}\nURL:{url}\nID:{vid}\n"
                    f"VTT: {vtt_path.name} ({lang_code}) client={client_used}\n",
                    encoding="utf-8"
                )
                print("  ‚úó VTT vac√≠o/ilegible (registrado .log)")
            else:
                fallback_mark = ""
                if lang_code not in ("es", "es-ES", "es-419") and not lang_code.startswith("es-") and not lang_code.startswith("es."):
                    fallback_mark = " (fallback_any_auto)"
                header = [
                    f"PLAYLIST: {playlist_title}",
                    f"PLAYLIST_URL: {playlist_url}",
                    f"TITLE: {title}",
                    f"URL:   {url}",
                    f"ID:    {vid}",
                    f"SOURCE: yt-dlp [{lang_code}] client={client_used}{fallback_mark}",
                    "-" * 60,
                    ""
                ]
                txt_path.write_text("\n".join(header) + solid, encoding="utf-8")
                ok += 1
                print(f"  ‚úì Transcripci√≥n guardada (texto s√≥lido): {txt_path.name}")
        except Exception as e:
            failed += 1
            log_path.write_text(
                f"Error al convertir VTT->TXT: {e}\n"
                f"PLAYLIST:{playlist_title}\nPLAYLIST_URL:{playlist_url}\n"
                f"TITLE:{title}\nURL:{url}\nID:{vid}\n"
                f"VTT: {vtt_path.name} ({lang_code}) client={client_used}\n",
                encoding="utf-8"
            )
            print("  ‚úó Error VTT->TXT (registrado .log)")

        time.sleep(SLEEP_BETWEEN)

    print("\n=== RESUMEN ===")
    print(f"V√≠deos totales:   {total}")
    print(f"OK (transcritos): {ok}")
    print(f"Omitidos:         {skipped}  (sin subt√≠tulos utilizables)")
    print(f"Fallidos:         {failed}")


if __name__ == "__main__":
    main()
```

### resumenes

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Genera res√∫menes en espa√±ol de todas las transcripciones en la carpeta 'transcripts/'.

Estructura esperada:
transcripts/
  ‚îú‚îÄ‚îÄ subcarpeta1/
  ‚îÇ     ‚îú‚îÄ‚îÄ video1.txt
  ‚îÇ     ‚îú‚îÄ‚îÄ video2.txt
  ‚îÇ     ‚îî‚îÄ‚îÄ ...
  ‚îú‚îÄ‚îÄ subcarpeta2/
  ‚îÇ     ‚îú‚îÄ‚îÄ ...
  ‚îî‚îÄ‚îÄ ...

Para cada archivo .txt:
- Lee el contenido completo.
- Lo env√≠a a `ollama run qwen2.5:3b-instruct` para obtener un resumen en espa√±ol.
- Guarda el resultado en `<nombre_original>.summary.txt` en la misma carpeta.
- Si el archivo de resumen ya existe, lo salta.

Requisitos:
  - Tener instalado ollama y el modelo qwen2.5:3b-instruct:
      ollama pull qwen2.5:3b-instruct
"""

import os
import subprocess
from pathlib import Path

# Carpeta ra√≠z donde est√°n las transcripciones
TRANSCRIPTS_ROOT = Path("transcripts")

# Modelo de Ollama
OLLAMA_MODEL = "qwen2.5:3b-instruct"

# Longitud m√°xima aproximada del texto que se manda al modelo
# (por si tus transcripciones son muy largas; si quieres, pon None para desactivar)
MAX_CHARS = 150000


def call_ollama_summarize(text: str) -> str:
    """
    Llama a 'ollama run qwen2.5:3b-instruct' pasando el prompt por stdin
    y devuelve la respuesta en texto plano.
    """
    prompt = f"""
Eres un asistente experto en educaci√≥n y programaci√≥n que resume transcripciones de v√≠deos en espa√±ol.

A partir de la transcripci√≥n siguiente, escribe un resumen en ESPA√ëOL que:
- Sea claro y conciso.
- Use entre 5 y 10 vi√±etas con los puntos clave.
- Termine con un breve p√°rrafo final de s√≠ntesis.
- No invente contenido que no aparezca en la transcripci√≥n.
- Mantenga el tono docente y explicativo.

Transcripci√≥n:
\"\"\"{text}\"\"\"
"""

    result = subprocess.run(
        ["ollama", "run", OLLAMA_MODEL],
        input=prompt.encode("utf-8"),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )

    if result.returncode != 0:
        raise RuntimeError(
            f"Error ejecutando ollama:\nSTDERR:\n{result.stderr.decode('utf-8', 'ignore')}"
        )

    return result.stdout.decode("utf-8", "ignore").strip()


def process_transcript_file(txt_path: Path) -> None:
    """
    Lee un .txt de transcripci√≥n, lo resume con ollama y guarda el
    resultado en .summary.txt en la misma carpeta.
    """
    summary_path = txt_path.with_suffix(".summary.txt")

    if summary_path.exists():
        print(f"[SKIP] Ya existe resumen: {summary_path}")
        return

    print(f"[PROC] {txt_path}")

    content = txt_path.read_text(encoding="utf-8", errors="ignore").strip()

    if not content:
        print(f"  ¬∑ Archivo vac√≠o, se omite.")
        return

    # Si quieres recortar transcripciones muy largas para evitar superar contexto:
    if MAX_CHARS is not None and len(content) > MAX_CHARS:
        print(f"  ¬∑ Aviso: transcripci√≥n muy larga, recortando a {MAX_CHARS} caracteres.")
        content = content[:MAX_CHARS]

    try:
        summary = call_ollama_summarize(content)
    except Exception as e:
        print(f"  ‚úó Error al resumir con ollama: {e}")
        return

    header = (
        f"RESUMEN GENERADO CON OLLAMA ({OLLAMA_MODEL})\n"
        f"ORIGINAL: {txt_path.name}\n"
        f"{'-'*60}\n\n"
    )

    summary_path.write_text(header + summary + "\n", encoding="utf-8")
    print(f"  ‚úì Resumen guardado en: {summary_path}")


def main():
    if not TRANSCRIPTS_ROOT.is_dir():
        print(f"ERROR: No existe la carpeta '{TRANSCRIPTS_ROOT}'.")
        return

    print(f"Buscando transcripciones en: {TRANSCRIPTS_ROOT.resolve()}")

    count_total = 0
    count_done = 0

    # Recorre todas las subcarpetas y archivos
    for root, dirs, files in os.walk(TRANSCRIPTS_ROOT):
        root_path = Path(root)
        # Opcional: puedes saltarte carpetas de logs, etc., si quieres
        for fname in files:
            if not fname.lower().endswith(".txt"):
                continue
            if fname.lower().endswith(".summary.txt"):
                # No queremos re-resumir res√∫menes
                continue

            txt_path = root_path / fname
            count_total += 1
            process_transcript_file(txt_path)
            count_done += 1

    print("\n=== RESUMEN GLOBAL ===")
    print(f"Archivos .txt encontrados (sin contar .summary.txt): {count_total}")
    print(f"Procesados (intento de resumen): {count_done}")


if __name__ == "__main__":
    main()
```

### listas

```json
[
  "https://www.youtube.com/watch?v=4ouGatnuNKY&list=PLWKjZdWQCDC64UcPIMiF2YplFcKG2EWej"
]
```

<a id="generador-web-ia"></a>
## generador web IA

### app

```python
from flask import Flask, render_template, request, jsonify
import requests

app = Flask(__name__)

OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "qwen2.5-coder:7b"


def call_ollama(prompt: str) -> str:
    """
    Call local Ollama with Qwen2.5-Coder and return the generated HTML+CSS.
    """
    # You can tune this system-style prefix to control what the model outputs
    system_instruction = """
You are an AI that generates HTML and CSS only.

Requirements:
- Return a complete HTML document.
- Include CSS either inline or inside a <style> tag in the <head>.
- Do NOT include any <script> tags or JavaScript code.
- Do NOT include explanations or comments in natural language.
- Just respond with the pure HTML (and embedded CSS).
"""

    full_prompt = f"""{system_instruction}

User requirement:
{prompt}
"""

    payload = {
        "model": MODEL_NAME,
        "prompt": full_prompt,
        "stream": False
    }

    try:
        resp = requests.post(OLLAMA_URL, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
        # Ollama's /generate returns {"response": "...", "done": true, ...}
        return data.get("response", "").strip()
    except Exception as e:
        print("Error calling Ollama:", e)
        # Fallback: simple error page in HTML
        return f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Error</title>
    <style>
        body {{ font-family: sans-serif; background:#2b2b2b; color:#f5f5f5; padding:2rem; }}
        .error {{ color:#ff6b6b; font-weight:bold; }}
    </style>
</head>
<body>
    <h1>Ooops‚Ä¶</h1>
    <p class="error">No se ha podido generar la p√°gina con la IA.</p>
    <p>Detalles t√©cnicos: {e}</p>
</body>
</html>
        """


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/generate", methods=["POST"])
def generate():
    data = request.get_json(force=True)
    prompt = data.get("prompt", "").strip()
    if not prompt:
        return jsonify({"error": "Empty prompt"}), 400

    html = call_ollama(prompt)
    return jsonify({"html": html})


if __name__ == "__main__":
    # Run Flask dev server
    app.run(host="0.0.0.0", port=5000, debug=True)
```

<a id="entrenamiento-ia"></a>
## Entrenamiento IA

### entrenar

```python
#!/usr/bin/env python3
import os
import json
from dataclasses import dataclass

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# -------------------------------------------------------------------
# CONFIGURACI√ìN B√ÅSICA
# -------------------------------------------------------------------

DATA_PATH = "training_data.jsonl"
# Modelo peque√±o y abierto de Qwen2.5 (aj√∫stalo si quieres otro)
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = "./qwen25-05b-jvc"

MAX_LENGTH = 512        # suficiente para ejemplos cortos
NUM_EPOCHS = 3
LR = 2e-4
BATCH_SIZE = 1
GRAD_ACCUM = 4          # batch efectivo = 4


def main():
    print("üöÄ Inicio de entrenamiento Qwen2.5 (Python puro)")
    print(f"üìÅ Usando dataset: {DATA_PATH}")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print("-" * 60)

    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"No se encontr√≥ el dataset en {DATA_PATH}")

    # -------------------------------------------------------------------
    # DETECTAR DISPOSITIVO
    # -------------------------------------------------------------------
    if torch.cuda.is_available():
        device = "cuda"
        use_4bit = True
        print("üíª GPU detectada (CUDA). Entrenando con QLoRA 4-bit.")
    else:
        device = "cpu"
        use_4bit = False
        print("üíª No hay GPU CUDA. Entrenando en CPU en float32 (esto ser√° m√°s lento).")

    # -------------------------------------------------------------------
    # CARGAR DATASET
    # -------------------------------------------------------------------
    print("üì• Cargando dataset con datasets.load_dataset(...)")
    raw_dataset = load_dataset(
        "json",
        data_files=DATA_PATH,
        split="train"
    )
    print(f"‚úÖ Dataset cargado con {len(raw_dataset)} ejemplos.")

    # -------------------------------------------------------------------
    # CARGAR TOKENIZER Y MODELO
    # -------------------------------------------------------------------
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=True
    )

    # Asegurar que tenemos pad_token definido
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ Cargando modelo base...")
    if use_4bit:
        # QLoRA 4-bit en GPU
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        model = prepare_model_for_kbit_training(model)

        # Configuraci√≥n LoRA
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    else:
        # Entrenamiento cl√°sico en CPU
        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
        model.to(device)

    # -------------------------------------------------------------------
    # CONVERTIR messages ‚Üí texto usando plantilla de chat
    # -------------------------------------------------------------------
    print("üß± Transformando mensajes (user/assistant) en texto plano con plantilla de chat...")

    def messages_to_text(example):
        """
        Convierte el campo 'messages' del JSONL en un √∫nico texto
        usando la plantilla de chat del tokenizer.
        """
        conv = []
        for m in example["messages"]:
            role = m.get("role", "user")
            if role not in ("user", "assistant", "system"):
                role = "user"
            conv.append({"role": role, "content": m["content"]})

        try:
            text = tokenizer.apply_chat_template(
                conv,
                tokenize=False,
                add_generation_prompt=False,
            )
        except Exception:
            # Fallback sencillo si falla apply_chat_template
            partes = []
            for m in conv:
                if m["role"] == "user":
                    prefix = "Usuario"
                elif m["role"] == "assistant":
                    prefix = "Asistente"
                else:
                    prefix = m["role"]
                partes.append(f"{prefix}: {m['content']}")
            text = "\n".join(partes)
        return {"text": text}

    text_dataset = raw_dataset.map(
        messages_to_text,
        remove_columns=raw_dataset.column_names
    )

    # -------------------------------------------------------------------
    # TOKENIZACI√ìN
    # -------------------------------------------------------------------
    print("‚úÖ Tokenizando dataset...")

    def tokenize_fn(batch):
        out = tokenizer(
            batch["text"],
            truncation=True,
            max_length=MAX_LENGTH,
            padding="max_length",
        )
        out["labels"] = out["input_ids"].copy()
        return out

    tokenized_dataset = text_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=["text"]
    )

    # -------------------------------------------------------------------
    # TRAINING ARGUMENTS (SIN evaluation_strategy NI HISTORIAS RARAS)
    # -------------------------------------------------------------------
    print("‚úÖ Configurando argumentos de entrenamiento...")

    # Solo uno de fp16 / bf16 puede ser True. Usamos fp16 en GPU, nada en CPU.
    use_fp16 = (device == "cuda")
    use_bf16 = False

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LR,
        weight_decay=0.01,
        warmup_ratio=0.03,
        logging_steps=1,
        save_steps=20,
        save_total_limit=1,
        fp16=use_fp16,
        bf16=use_bf16,
        dataloader_pin_memory=False,
        report_to="none",  # sin wandb ni nada
    )

    # -------------------------------------------------------------------
    # TRAINER
    # -------------------------------------------------------------------
    print("‚úÖ Creando Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )

    # -------------------------------------------------------------------
    # ENTRENAR
    # -------------------------------------------------------------------
    print("üöÇ Comenzando entrenamiento...")
    trainer.train()
    print("üèÅ Entrenamiento terminado.")

    # -------------------------------------------------------------------
    # GUARDAR MODELO Y TOKENIZER
    # -------------------------------------------------------------------
    print("üíæ Guardando modelo y tokenizer en", OUTPUT_DIR)
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("‚úÖ Todo listo.")


if __name__ == "__main__":
    main()
```

### probar

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

try:
    from peft import PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False


# ==========================
# CONFIGURACI√ìN
# ==========================
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"        # mismo modelo base que en entrenar.py
LORA_DIR   = "./qwen25-05b-jvc"             # carpeta donde entrenar.py guard√≥ el LoRA o el modelo
MAX_NEW_TOKENS = 256
TEMPERATURE    = 0.7
TOP_P          = 0.9


def print_header():
    print("üöÄ Test de modelo Qwen2.5 fine-tune (LoRA)")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print(f"üìÅ Carpeta LoRA / modelo: {LORA_DIR}")
    print("-" * 60)


def detect_device():
    if torch.cuda.is_available():
        device = "cuda"
        torch_dtype = torch.float16
        print("üíª GPU detectada (CUDA). Usando GPU con float16.")
    else:
        device = "cpu"
        torch_dtype = torch.float32
        print("üíª No se detecta GPU. Usando CPU con float32.")
    return device, torch_dtype


def load_tokenizer(model_name):
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # Por si el tokenizer no tiene pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


def load_base_model(model_name, device, torch_dtype):
    print("‚úÖ Cargando modelo base...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch_dtype,
        device_map="auto" if device == "cuda" else None,
    )
    if device == "cpu":
        model.to(device)
    return model


def load_with_lora_if_possible(base_model, lora_dir, device):
    """
    Intenta cargar adaptador LoRA desde lora_dir.
    - Si hay adapter_config.json, se asume que es un LoRA PEFT.
    - Si no, se asume que lora_dir es un modelo HF completo (modelo fusionado).
    """
    if not os.path.isdir(lora_dir):
        print(f"‚ùå La carpeta '{lora_dir}' NO existe.")
        print("   Revisa en entrenar.py cu√°l fue el 'output_dir' / carpeta donde se guard√≥ el LoRA o el modelo.")
        sys.exit(1)

    adapter_config_path = os.path.join(lora_dir, "adapter_config.json")

    # Caso 1: LoRA PEFT (adapter_config.json presente)
    if os.path.exists(adapter_config_path):
        if not PEFT_AVAILABLE:
            print("‚ùå Se ha encontrado un adaptador LoRA (adapter_config.json),")
            print("   pero el paquete 'peft' no est√° instalado.")
            print("   Inst√°lalo con: pip install peft")
            sys.exit(1)

        print(f"‚úÖ Encontrado adaptador LoRA en: {lora_dir}")
        from peft import PeftModel

        model = PeftModel.from_pretrained(
            base_model,
            lora_dir,
            device_map="auto" if device == "cuda" else None,
        )
        # Opcional: fusionar LoRA para inferencia
        try:
            model = model.merge_and_unload()
            print("‚úÖ LoRA fusionado con el modelo base para inferencia.")
        except Exception:
            print("‚ÑπÔ∏è No se ha fusionado el LoRA (merge_and_unload fall√≥ o no est√° disponible).")
        return model

    # Caso 2: No hay adapter_config.json ‚Üí intentar cargar lora_dir como modelo completo
    print("‚ö†Ô∏è No se encontr√≥ 'adapter_config.json' en la carpeta LoRA.")
    print("   Se intentar√° cargar la carpeta como un modelo completo (modelo ya fusionado).")
    print("   Si esto falla, revisa que entrenar.py est√© llamando a model.save_pretrained(LORA_DIR).")

    try:
        model = AutoModelForCausalLM.from_pretrained(
            lora_dir,
            torch_dtype=base_model.dtype,
            device_map="auto" if device == "cuda" else None,
        )
        if device == "cpu":
            model.to(device)
        print("‚úÖ Modelo cargado directamente desde la carpeta.")
        return model
    except Exception as e:
        print("‚ùå No se ha podido cargar la carpeta como modelo completo.")
        print(f"   Error: {e}")
        sys.exit(1)


def build_prompt(tokenizer, user_text: str) -> str:
    """
    Construye el texto de entrada siguiendo la plantilla de chat de Qwen, si existe.
    Si no, usa un formato sencillo tipo chat.
    """
    if hasattr(tokenizer, "apply_chat_template"):
        messages = [
            {"role": "system", "content": "Eres un profesor de programaci√≥n llamado Jose Vicente. Explicas en espa√±ol de forma clara y did√°ctica."},
            {"role": "user",   "content": user_text},
        ]
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    else:
        # fallback simple
        return f"Sistema: Eres un profesor de programaci√≥n llamado Jose Vicente.\nUsuario: {user_text}\nAsistente:"


def generate_answer(tokenizer, model, device: str, prompt: str) -> str:
    model.eval()
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
    )

    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Nos quedamos solo con la parte generada (sin el prompt) para que sea m√°s limpio
    generated_ids = output_ids[0][inputs["input_ids"].shape[-1]:]
    text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return text.strip()


def main():
    print_header()

    device, torch_dtype = detect_device()

    # 1. Tokenizer y modelo base
    tokenizer = load_tokenizer(MODEL_NAME)
    base_model = load_base_model(MODEL_NAME, device, torch_dtype)

    # 2. Cargar LoRA o modelo completo desde LORA_DIR
    model = load_with_lora_if_possible(base_model, LORA_DIR, device)

    # 3. Bucle de prueba interactivo
    print("\n‚úÖ Modelo listo para probar.")
    print("Escribe una pregunta (en espa√±ol). Ctrl+C o l√≠nea vac√≠a para salir.\n")

    try:
        while True:
            user_text = input("üí¨ T√∫: ").strip()
            if not user_text:
                print("üëã Saliendo.")
                break

            prompt = build_prompt(tokenizer, user_text)
            print("\nü§ñ Modelo (pensando)...\n")
            answer = generate_answer(tokenizer, model, device, prompt)
            print("ü§ñ Respuesta:\n")
            print(answer)
            print("\n" + "-" * 60 + "\n")

    except KeyboardInterrupt:
        print("\nüëã Saliendo por Ctrl+C.")


if __name__ == "__main__":
    main()
```

### training_data

```
{"messages":[
  {"role":"user","content":"¬øQui√©n es Jose Vicente Carratal√° Sanchis?"},
  {"role":"assistant","content":"Jose Vicente Carratal√° Sanchis es un docente y desarrollador espa√±ol especializado en programaci√≥n, ciclos formativos de DAM y DAW, desarrollo de software empresarial, sistemas de informaci√≥n y proyectos de inteligencia artificial. Tambi√©n es autor de libros t√©cnicos, creador de cursos online y responsable del ecosistema Jocarsa de soluciones empresariales."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© tipo de contenido crea Jose Vicente?"},
  {"role":"assistant","content":"Jose Vicente produce contenido educativo sobre programaci√≥n, bases de datos, dise√±o web, inteligencia artificial, 3D, sistemas operativos y desarrollo empresarial. Ha creado miles de v√≠deos en YouTube, libros t√©cnicos, cursos completos, proyectos did√°cticos y herramientas para automatizar tareas docentes."}
]}
{"messages":[
  {"role":"user","content":"¬øA qu√© se dedica profesionalmente Jose Vicente Carratal√°?"},
  {"role":"assistant","content":"Jose Vicente trabaja como profesor en ciclos formativos de inform√°tica (DAM, DAW) y desarrolla software empresarial. Tambi√©n dirige proyectos como Jocarsa, crea soluciones SaaS, plataformas educativas, sistemas de evaluaci√≥n autom√°ticos y experimenta con IA generativa y modelos LLM."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© es Jocarsa y qu√© relaci√≥n tiene Jose Vicente con esa marca?"},
  {"role":"assistant","content":"Jocarsa es una marca creada por Jose Vicente Carratal√° dedicada al desarrollo de software de gesti√≥n empresarial (ERP, CRM, herramientas online, BI, mapas mentales, productividad, servidores...). Jose Vicente es el fundador y desarrolla tanto el backend como el frontend, la infraestructura y la documentaci√≥n t√©cnica y comercial."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© tipo de proyectos desarrolla Jose Vicente en clase?"},
  {"role":"assistant","content":"Desarrolla proyectos completos de programaci√≥n, bases de datos, interfaces, webs full-stack, simulaciones, IA, videojuegos educativos, automatizaci√≥n de sistemas y dashboards empresariales. Su metodolog√≠a se basa en ejercicios progresivos con ejemplos reales y documentaci√≥n clara."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© estilo docente tiene Jose Vicente?"},
  {"role":"assistant","content":"Su estilo docente es directo, pr√°ctico y orientado a la industria. Explica con sencillez, usa muchos ejemplos reales, proporciona c√≥digo limpio y anima a que cada alumno construya proyectos completos. Adem√°s integra IA, an√°lisis de datos, servidores, APIs y entornos profesionales en las clases."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© tecnolog√≠as domina Jose Vicente?"},
  {"role":"assistant","content":"Jose Vicente domina Python, PHP, Java, SQL, JavaScript, React, C++, CUDA, OpenCV, HTML, CSS, sistemas Linux, Apache, Flask, bases de datos, BI, WebSockets, Next.js, Three.js, Odoo, Moodle, entrenamiento de LLMs (QLoRA, GGUF, ctransformers) y frameworks modernos de desarrollo."}
]}
{"messages":[
  {"role":"user","content":"Dime algunos de los proyectos de inteligencia artificial que desarrolla Jose Vicente."},
  {"role":"assistant","content":"Jose Vicente desarrolla asistentes educativos, modelos LLM personalizados, sistemas de evaluaci√≥n autom√°tica con IA, chatbots entrenados con contenido propio, visualizaciones 3D sincronizadas con voz, training pipelines con QLoRA, datasets generados a partir de transcripciones, y modelos para programaci√≥n educativa."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© cursos ha creado Jose Vicente Carratal√°?"},
  {"role":"assistant","content":"Ha creado cursos de Python, SQL, HTML/CSS/JS, PHP, C++, 3D, IA generativa, dise√±o web, programaci√≥n multiplataforma, sistemas operativos, bases de datos, servidores Linux, frameworks frontend, visualizaci√≥n 3D, y proyectos integrados para DAM/DAW."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© libros ha escrito Jose Vicente?"},
  {"role":"assistant","content":"Jose Vicente es autor de libros como 'Aprende Programaci√≥n con Python', manuales de SQL, cursos t√©cnicos en formato libro, manuales de servidores Linux, desarrollo web y programaci√≥n educativa. Tambi√©n adapta cursos completos a formato libro para sus alumnos."}
]}
{"messages":[
  {"role":"user","content":"¬øC√≥mo organiza Jose Vicente sus proyectos educativos?"}, 
  {"role":"assistant","content":"Jose Vicente usa un sistema basado en JSON, PHP, Python y dashboards personalizados. Cada actividad tiene evaluaciones, criterios, m√©tricas, procesos de correcci√≥n y seguimiento. Sus aplicaciones permiten analizar calificaciones, progreso, feedback y estad√≠sticas docentes."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© plataformas administra Jose Vicente?"}, 
  {"role":"assistant","content":"Administra servidores Linux, Apache, Moodle, Odoo, webs desarrolladas a medida, aplicaciones Flask, APIs educativas y sistemas completos de gesti√≥n acad√©mica. Gestiona certificados SSL, backups autom√°ticos y despliegues remotos."}
]}
{"messages":[
  {"role":"user","content":"¬øC√≥mo es la filosof√≠a de Jose Vicente respecto al aprendizaje?"}, 
  {"role":"assistant","content":"Cree en el aprendizaje pr√°ctico, directo y progresivo. Prefiere que el alumnado construya proyectos reales en lugar de memorizar teor√≠a. Fomenta el pensamiento cr√≠tico, la autonom√≠a, la creatividad, y el uso de herramientas profesionales desde el primer d√≠a."}
]}
{"messages":[
  {"role":"user","content":"Describe el ecosistema de productos Jocarsa creado por Jose Vicente."},
  {"role":"assistant","content":"El ecosistema Jocarsa incluye CRM, ERP, herramientas online, hojas de c√°lculo, mapas mentales, asistentes empresariales, paneles de BI, utilidades de servidor, m√≥dulos educativos, un sistema SIS personalizado y soluciones SaaS orientadas a peque√±as y medianas empresas."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© trabajos de 3D desarrolla Jose Vicente Carratal√°?"}, 
  {"role":"assistant","content":"Desarrolla visualizaciones 3D en Blender, renderizados, animaciones, escenas interactivas WebGL, simulaciones f√≠sicas, interfaces 3D educativas, portfolios animados, path tracing, Gaussian Splatting y proyectos creativos para mostrar procesos t√©cnicos en clase."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© proyectos recientes est√° desarrollando Jose Vicente?"}, 
  {"role":"assistant","content":"Est√° desarrollando: una plataforma de BI en Java; un servidor WebSocket seguro; un asistente educativo LLM local; una web completa para Jocarsa; un nuevo sistema SIS‚ÄìMoodle; simulaciones fractales en C++ con OpenMP; un videojuego educativo 2D; y scripts de automatizaci√≥n para YouTube, Moodle y scraping industrial."}
]}
{"messages":[
  {"role":"user","content":"¬øJose Vicente trabaja con inteligencia artificial local?"}, 
  {"role":"assistant","content":"S√≠, utiliza modelos locales como Qwen, LLaMA, Gemma y Mistral, ejecutados en su propio hardware usando Ollama, ctransformers y GGUF. Ajusta modelos con QLoRA, crea datasets propios y entrena asistentes personalizados para programaci√≥n e investigaci√≥n."}
]}
{"messages":[
  {"role":"user","content":"¬øC√≥mo organiza Jose Vicente los proyectos de los alumnos?"}, 
  {"role":"assistant","content":"Usa un √°rbol de cursos en Markdown, c√≥digos de actividad, JSONs estructurados, un SIS personalizado, integraci√≥n con Moodle v√≠a APIs y scripts autom√°ticos para matricular, evaluar y revisar entregas. Adem√°s genera dashboards de actividad docente usando PHP y JavaScript."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© estilo tiene Jose Vicente al explicar c√≥digo?"}, 
  {"role":"assistant","content":"Explica con claridad, ejemplos concretos, comparaciones sencillas, y siempre conecta el c√≥digo con su aplicaci√≥n real. Prefiere ense√±ar progresivamente sin adelantar conceptos y mantiene un estilo cercano a sus cursos y libros t√©cnicos."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© papel tiene YouTube en el trabajo de Jose Vicente?"}, 
  {"role":"assistant","content":"YouTube es uno de sus pilares: tiene miles de v√≠deos educativos sobre programaci√≥n y tecnolog√≠a. Usa las transcripciones como dataset para entrenar sus propios modelos de IA, y muchos de sus cursos completos est√°n publicados all√≠ para sus estudiantes."}
]}
```

<a id="entrenar-chatbot-a-partir-de-whatsapp"></a>
## entrenar chatbot a partir de whatsapp

### whatsapp a jsonl

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Convierte un archivo de exportaci√≥n de WhatsApp (Android o iOS) a un JSONL
en formato ChatML con pares pregunta‚Äìrespuesta (Q&A).

- Lee INPUT_FILE (exportaci√≥n de WhatsApp en texto).
- Detecta formato Android e iOS (incluyendo: [28/09/2023 10:15] Ana: ...).
- Reconstruye mensajes multil√≠nea.
- Agrupa mensajes en bloques:
    usuarios (rol "user") -> asistente (rol "assistant")
- Genera OUTPUT_FILE en formato JSONL:

    {"messages":[
        {"role":"user","content":"..."},
        {"role":"assistant","content":"..."}
    ]}

Configura aqu√≠:

    INPUT_FILE
    OUTPUT_FILE
    ASSISTANT_NAME
"""

import re
import json
from datetime import datetime
from typing import Optional, List, Dict, Tuple

# Archivos de entrada/salida
INPUT_FILE = "conversacion.txt"
OUTPUT_FILE = "conversacion.jsonl"

# Nombre de la persona que act√∫a como "asistente" en la conversaci√≥n
ASSISTANT_NAME = "Mar√≠a"   # c√°mbialo si procede


# -----------------------------
# Patrones para reconocer INICIO de mensaje
# -----------------------------

# Android (espa√±ol/Europa):
# 01/01/24, 9:15 - Nombre: mensaje
ANDROID_USER_RE = re.compile(
    r'^(\d{1,2}/\d{1,2}/\d{2,4}), (\d{1,2}:\d{2}) - ([^:]+): (.*)$'
)

# Android sistema (sin "Nombre:"):
# 01/01/24, 9:15 - Mensaje del sistema...
ANDROID_SYSTEM_RE = re.compile(
    r'^(\d{1,2}/\d{1,2}/\d{2,4}), (\d{1,2}:\d{2}) - (.*)$'
)

# iOS usuario:
# [28/09/2023 10:15] Ana: mensaje
# [28/09/2023, 10:15] Ana: mensaje
IOS_USER_RE = re.compile(
    r'^\[(\d{1,2}/\d{1,2}/\d{2,4})[ ,]+(\d{1,2}:\d{2})\] ([^:]+): (.*)$'
)

# iOS sistema:
# [28/09/2023 10:15] Mensaje del sistema...
IOS_SYSTEM_RE = re.compile(
    r'^\[(\d{1,2}/\d{1,2}/\d{2,4})[ ,]+(\d{1,2}:\d{2})\] (.*)$'
)


def parse_datetime(date_str: str, time_str: str) -> Optional[datetime]:
    """
    Intenta parsear la fecha y hora de WhatsApp a datetime.
    Acepta:
        "dd/mm/aa, HH:MM"
        "dd/mm/aaaa, HH:MM"
        "dd/mm/aa HH:MM"
        "dd/mm/aaaa HH:MM"
    """
    candidates = [
        f"{date_str}, {time_str}",
        f"{date_str} {time_str}",
    ]
    formats = [
        "%d/%m/%y, %H:%M",
        "%d/%m/%Y, %H:%M",
        "%d/%m/%y %H:%M",
        "%d/%m/%Y %H:%M",
    ]

    for candidate in candidates:
        for fmt in formats:
            try:
                return datetime.strptime(candidate, fmt)
            except ValueError:
                continue

    return None


def parse_new_message_line(line: str) -> Optional[Tuple[Optional[datetime], Optional[str], str]]:
    """
    Intenta interpretar una l√≠nea como inicio de un nuevo mensaje.

    Devuelve:
        (dt, author, text)
    donde author puede ser None para mensajes del sistema.

    Si la l√≠nea no parece un inicio de mensaje nuevo, devuelve None.
    """

    # Android con autor
    m = ANDROID_USER_RE.match(line)
    if m:
        date_str, time_str, author, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, author.strip(), text

    # iOS con autor
    m = IOS_USER_RE.match(line)
    if m:
        date_str, time_str, author, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, author.strip(), text

    # Android sistema
    m = ANDROID_SYSTEM_RE.match(line)
    if m:
        date_str, time_str, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, None, text

    # iOS sistema
    m = IOS_SYSTEM_RE.match(line)
    if m:
        date_str, time_str, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, None, text

    return None


def whatsapp_txt_a_mensajes(lines: List[str]) -> List[Dict]:
    """
    Convierte las l√≠neas de un .txt de WhatsApp en una lista de mensajes estructurados.

    Cada mensaje:
    {
        "timestamp": datetime o None,
        "author": str o None (sistema),
        "text": str,
        "raw_line": str
    }
    """
    mensajes: List[Dict] = []
    actual: Optional[Dict] = None

    for raw_line in lines:
        line = raw_line.rstrip("\n\r")

        # Intentar detectar un nuevo mensaje
        parsed = parse_new_message_line(line)
        if parsed is not None:
            dt, author, text = parsed

            # Guardar el mensaje anterior si exist√≠a
            if actual is not None:
                mensajes.append(actual)

            # Crear nuevo mensaje
            actual = {
                "timestamp": dt,
                "author": author,
                "text": text if text is not None else "",
                "raw_line": line,
            }
        else:
            # Continuaci√≥n de mensaje anterior (multil√≠nea)
            if actual is not None:
                if actual["text"]:
                    actual["text"] += "\n" + line
                else:
                    actual["text"] = line
            else:
                # L√≠nea fuera de cualquier mensaje (t√≠tulo, etc.). Se ignora.
                continue

    # A√±adir √∫ltimo mensaje si existe
    if actual is not None:
        mensajes.append(actual)

    return mensajes


def mensajes_a_chatml_qna(mensajes: List[Dict]) -> List[Dict]:
    """
    A partir de la lista de mensajes "planos", genera pares Q&A
    en formato ChatML:

    {
      "messages": [
        {"role":"user", "content":"..."},
        {"role":"assistant", "content":"..."}
      ]
    }

    L√≥gica:
    - Se considera "assistant" cualquier mensaje cuyo author == ASSISTANT_NAME.
    - El resto de autores se consideran "user".
    - Se agrupan bloques:
        (uno o m√°s mensajes de usuarios) seguido de (uno o m√°s mensajes de asistente).
      Cada bloque genera un ejemplo Q&A.
    """
    # Filtrar mensajes de sistema (sin autor)
    msgs = [m for m in mensajes if m["author"] is not None]

    qna_examples: List[Dict] = []

    i = 0
    n = len(msgs)

    while i < n:
        m = msgs[i]
        # Si el mensaje actual es de la asistente sin bloque previo de usuario, se ignora
        if m["author"] == ASSISTANT_NAME:
            i += 1
            continue

        # Bloque de usuario(s): uno o m√°s mensajes consecutivos de autores != ASSISTANT_NAME
        user_texts = [m["text"]]
        i += 1
        while i < n and msgs[i]["author"] != ASSISTANT_NAME:
            user_texts.append(msgs[i]["text"])
            i += 1

        # A continuaci√≥n deber√≠a venir la asistente
        if i >= n or msgs[i]["author"] != ASSISTANT_NAME:
            # No hay respuesta de la asistente; no formamos Q&A
            continue

        # Bloque de asistente(s): uno o m√°s mensajes consecutivos de author == ASSISTANT_NAME
        assistant_texts = [msgs[i]["text"]]
        i += 1
        while i < n and msgs[i]["author"] == ASSISTANT_NAME:
            assistant_texts.append(msgs[i]["text"])
            i += 1

        user_content = "\n".join(user_texts).strip()
        assistant_content = "\n".join(assistant_texts).strip()

        if not user_content or not assistant_content:
            continue

        example = {
            "messages": [
                {"role": "user", "content": user_content},
                {"role": "assistant", "content": assistant_content},
            ]
        }
        qna_examples.append(example)

    return qna_examples


def escribir_jsonl_chatml_qna(examples: List[Dict], out_path: str) -> None:
    """
    Escribe la lista de ejemplos ChatML Q&A en un JSONL.
    """
    with open(out_path, "w", encoding="utf-8") as f_out:
        for ex in examples:
            f_out.write(json.dumps(ex, ensure_ascii=False) + "\n")


def main():
    print(f"Leyendo: {INPUT_FILE}")

    with open(INPUT_FILE, "r", encoding="utf-8") as f_in:
        lines = f_in.readlines()

    mensajes = whatsapp_txt_a_mensajes(lines)
    print(f"Mensajes totales parseados: {len(mensajes)}")

    qna_examples = mensajes_a_chatml_qna(mensajes)
    print(f"Pares Q&A generados: {len(qna_examples)}")

    escribir_jsonl_chatml_qna(qna_examples, OUTPUT_FILE)

    print(f"JSONL ChatML Q&A generado en: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
```

### entrenamiento

```python
#!/usr/bin/env python3
import os
import json
from dataclasses import dataclass

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# -------------------------------------------------------------------
# CONFIGURACI√ìN B√ÅSICA
# -------------------------------------------------------------------

DATA_PATH = "conversacion.jsonl"
# Modelo peque√±o y abierto de Qwen2.5 (aj√∫stalo si quieres otro)
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = "./qwen25-05b-jvc"

MAX_LENGTH = 512        # suficiente para ejemplos cortos
NUM_EPOCHS = 3
LR = 2e-4
BATCH_SIZE = 1
GRAD_ACCUM = 4          # batch efectivo = 4


def main():
    print("üöÄ Inicio de entrenamiento Qwen2.5 (Python puro)")
    print(f"üìÅ Usando dataset: {DATA_PATH}")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print("-" * 60)

    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"No se encontr√≥ el dataset en {DATA_PATH}")

    # -------------------------------------------------------------------
    # DETECTAR DISPOSITIVO
    # -------------------------------------------------------------------
    if torch.cuda.is_available():
        device = "cuda"
        use_4bit = True
        print("üíª GPU detectada (CUDA). Entrenando con QLoRA 4-bit.")
    else:
        device = "cpu"
        use_4bit = False
        print("üíª No hay GPU CUDA. Entrenando en CPU en float32 (esto ser√° m√°s lento).")

    # -------------------------------------------------------------------
    # CARGAR DATASET
    # -------------------------------------------------------------------
    print("üì• Cargando dataset con datasets.load_dataset(...)")
    raw_dataset = load_dataset(
        "json",
        data_files=DATA_PATH,
        split="train"
    )
    print(f"‚úÖ Dataset cargado con {len(raw_dataset)} ejemplos.")

    # -------------------------------------------------------------------
    # CARGAR TOKENIZER Y MODELO
    # -------------------------------------------------------------------
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=True
    )

    # Asegurar que tenemos pad_token definido
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ Cargando modelo base...")
    if use_4bit:
        # QLoRA 4-bit en GPU
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        model = prepare_model_for_kbit_training(model)

        # Configuraci√≥n LoRA
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    else:
        # Entrenamiento cl√°sico en CPU
        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
        model.to(device)

    # -------------------------------------------------------------------
    # CONVERTIR messages ‚Üí texto usando plantilla de chat
    # -------------------------------------------------------------------
    print("üß± Transformando mensajes (user/assistant) en texto plano con plantilla de chat...")

    def messages_to_text(example):
        """
        Convierte el campo 'messages' del JSONL en un √∫nico texto
        usando la plantilla de chat del tokenizer.
        """
        conv = []
        for m in example["messages"]:
            role = m.get("role", "user")
            if role not in ("user", "assistant", "system"):
                role = "user"
            conv.append({"role": role, "content": m["content"]})

        try:
            text = tokenizer.apply_chat_template(
                conv,
                tokenize=False,
                add_generation_prompt=False,
            )
        except Exception:
            # Fallback sencillo si falla apply_chat_template
            partes = []
            for m in conv:
                if m["role"] == "user":
                    prefix = "Usuario"
                elif m["role"] == "assistant":
                    prefix = "Asistente"
                else:
                    prefix = m["role"]
                partes.append(f"{prefix}: {m['content']}")
            text = "\n".join(partes)
        return {"text": text}

    text_dataset = raw_dataset.map(
        messages_to_text,
        remove_columns=raw_dataset.column_names
    )

    # -------------------------------------------------------------------
    # TOKENIZACI√ìN
    # -------------------------------------------------------------------
    print("‚úÖ Tokenizando dataset...")

    def tokenize_fn(batch):
        out = tokenizer(
            batch["text"],
            truncation=True,
            max_length=MAX_LENGTH,
            padding="max_length",
        )
        out["labels"] = out["input_ids"].copy()
        return out

    tokenized_dataset = text_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=["text"]
    )

    # -------------------------------------------------------------------
    # TRAINING ARGUMENTS (SIN evaluation_strategy NI HISTORIAS RARAS)
    # -------------------------------------------------------------------
    print("‚úÖ Configurando argumentos de entrenamiento...")

    # Solo uno de fp16 / bf16 puede ser True. Usamos fp16 en GPU, nada en CPU.
    use_fp16 = (device == "cuda")
    use_bf16 = False

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LR,
        weight_decay=0.01,
        warmup_ratio=0.03,
        logging_steps=1,
        save_steps=20,
        save_total_limit=1,
        fp16=use_fp16,
        bf16=use_bf16,
        dataloader_pin_memory=False,
        report_to="none",  # sin wandb ni nada
    )

    # -------------------------------------------------------------------
    # TRAINER
    # -------------------------------------------------------------------
    print("‚úÖ Creando Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )

    # -------------------------------------------------------------------
    # ENTRENAR
    # -------------------------------------------------------------------
    print("üöÇ Comenzando entrenamiento...")
    trainer.train()
    print("üèÅ Entrenamiento terminado.")

    # -------------------------------------------------------------------
    # GUARDAR MODELO Y TOKENIZER
    # -------------------------------------------------------------------
    print("üíæ Guardando modelo y tokenizer en", OUTPUT_DIR)
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("‚úÖ Todo listo.")


if __name__ == "__main__":
    main()
```

### probar

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

try:
    from peft import PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False


# ==========================
# CONFIGURACI√ìN
# ==========================
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"        # mismo modelo base que en entrenar.py
LORA_DIR   = "./qwen25-05b-jvc"             # carpeta donde entrenar.py guard√≥ el LoRA o el modelo
MAX_NEW_TOKENS = 256
TEMPERATURE    = 0.7
TOP_P          = 0.9


def print_header():
    print("üöÄ Test de modelo Qwen2.5 fine-tune (LoRA)")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print(f"üìÅ Carpeta LoRA / modelo: {LORA_DIR}")
    print("-" * 60)


def detect_device():
    if torch.cuda.is_available():
        device = "cuda"
        torch_dtype = torch.float16
        print("üíª GPU detectada (CUDA). Usando GPU con float16.")
    else:
        device = "cpu"
        torch_dtype = torch.float32
        print("üíª No se detecta GPU. Usando CPU con float32.")
    return device, torch_dtype


def load_tokenizer(model_name):
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # Por si el tokenizer no tiene pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


def load_base_model(model_name, device, torch_dtype):
    print("‚úÖ Cargando modelo base...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch_dtype,
        device_map="auto" if device == "cuda" else None,
    )
    if device == "cpu":
        model.to(device)
    return model


def load_with_lora_if_possible(base_model, lora_dir, device):
    """
    Intenta cargar adaptador LoRA desde lora_dir.
    - Si hay adapter_config.json, se asume que es un LoRA PEFT.
    - Si no, se asume que lora_dir es un modelo HF completo (modelo fusionado).
    """
    if not os.path.isdir(lora_dir):
        print(f"‚ùå La carpeta '{lora_dir}' NO existe.")
        print("   Revisa en entrenar.py cu√°l fue el 'output_dir' / carpeta donde se guard√≥ el LoRA o el modelo.")
        sys.exit(1)

    adapter_config_path = os.path.join(lora_dir, "adapter_config.json")

    # Caso 1: LoRA PEFT (adapter_config.json presente)
    if os.path.exists(adapter_config_path):
        if not PEFT_AVAILABLE:
            print("‚ùå Se ha encontrado un adaptador LoRA (adapter_config.json),")
            print("   pero el paquete 'peft' no est√° instalado.")
            print("   Inst√°lalo con: pip install peft")
            sys.exit(1)

        print(f"‚úÖ Encontrado adaptador LoRA en: {lora_dir}")
        from peft import PeftModel

        model = PeftModel.from_pretrained(
            base_model,
            lora_dir,
            device_map="auto" if device == "cuda" else None,
        )
        # Opcional: fusionar LoRA para inferencia
        try:
            model = model.merge_and_unload()
            print("‚úÖ LoRA fusionado con el modelo base para inferencia.")
        except Exception:
            print("‚ÑπÔ∏è No se ha fusionado el LoRA (merge_and_unload fall√≥ o no est√° disponible).")
        return model

    # Caso 2: No hay adapter_config.json ‚Üí intentar cargar lora_dir como modelo completo
    print("‚ö†Ô∏è No se encontr√≥ 'adapter_config.json' en la carpeta LoRA.")
    print("   Se intentar√° cargar la carpeta como un modelo completo (modelo ya fusionado).")
    print("   Si esto falla, revisa que entrenar.py est√© llamando a model.save_pretrained(LORA_DIR).")

    try:
        model = AutoModelForCausalLM.from_pretrained(
            lora_dir,
            torch_dtype=base_model.dtype,
            device_map="auto" if device == "cuda" else None,
        )
        if device == "cpu":
            model.to(device)
        print("‚úÖ Modelo cargado directamente desde la carpeta.")
        return model
    except Exception as e:
        print("‚ùå No se ha podido cargar la carpeta como modelo completo.")
        print(f"   Error: {e}")
        sys.exit(1)


def build_prompt(tokenizer, user_text: str) -> str:
    """
    Construye el texto de entrada siguiendo la plantilla de chat de Qwen, si existe.
    Si no, usa un formato sencillo tipo chat.
    """
    if hasattr(tokenizer, "apply_chat_template"):
        messages = [
            {"role": "system", "content": "Eres un profesor de programaci√≥n llamado Jose Vicente. Explicas en espa√±ol de forma clara y did√°ctica."},
            {"role": "user",   "content": user_text},
        ]
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    else:
        # fallback simple
        return f"Sistema: Eres un profesor de programaci√≥n llamado Jose Vicente.\nUsuario: {user_text}\nAsistente:"


def generate_answer(tokenizer, model, device: str, prompt: str) -> str:
    model.eval()
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
    )

    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Nos quedamos solo con la parte generada (sin el prompt) para que sea m√°s limpio
    generated_ids = output_ids[0][inputs["input_ids"].shape[-1]:]
    text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return text.strip()


def main():
    print_header()

    device, torch_dtype = detect_device()

    # 1. Tokenizer y modelo base
    tokenizer = load_tokenizer(MODEL_NAME)
    base_model = load_base_model(MODEL_NAME, device, torch_dtype)

    # 2. Cargar LoRA o modelo completo desde LORA_DIR
    model = load_with_lora_if_possible(base_model, LORA_DIR, device)

    # 3. Bucle de prueba interactivo
    print("\n‚úÖ Modelo listo para probar.")
    print("Escribe una pregunta (en espa√±ol). Ctrl+C o l√≠nea vac√≠a para salir.\n")

    try:
        while True:
            user_text = input("üí¨ T√∫: ").strip()
            if not user_text:
                print("üëã Saliendo.")
                break

            prompt = build_prompt(tokenizer, user_text)
            print("\nü§ñ Modelo (pensando)...\n")
            answer = generate_answer(tokenizer, model, device, prompt)
            print("ü§ñ Respuesta:\n")
            print(answer)
            print("\n" + "-" * 60 + "\n")

    except KeyboardInterrupt:
        print("\nüëã Saliendo por Ctrl+C.")


if __name__ == "__main__":
    main()
```

### probar interactivo

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------
BASE_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
MODEL_DIR       = "qwen25-05b-jvc"   # carpeta donde guardaste el modelo entrenado

MAX_NEW_TOKENS = 128
TEMPERATURE    = 0.7
TOP_P          = 0.9

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype  = torch.float16 if device == "cuda" else torch.float32

print("üîß Loading tokenizer and model...")
print(f"Using device: {device}")

# -------------------------------------------------------------------
# TOKENIZER
# -------------------------------------------------------------------
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# -------------------------------------------------------------------
# MODEL LOADING (AUTO-DETECT PEFT vs FULL MODEL)
# -------------------------------------------------------------------
adapter_config_path = os.path.join(MODEL_DIR, "adapter_config.json")

if os.path.exists(adapter_config_path):
    # Caso: LoRA / PEFT
    print(f"Detected PEFT adapter at '{MODEL_DIR}'. Loading base model + adapter...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        dtype=dtype,
    )
    model = PeftModel.from_pretrained(base_model, MODEL_DIR)
else:
    # Caso: modelo completo ya fine-tuned en MODEL_DIR
    print(f"No adapter_config.json found in '{MODEL_DIR}'.")
    print("Assuming full fine-tuned model directory. Loading directly...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        dtype=dtype,
    )

model.to(device)
model.eval()
print("‚úÖ Model loaded.")


def generate_reply(user_message: str) -> str:
    """
    Given a user message, build a chat-style conversation and
    generate the assistant reply using Qwen's chat template.
    """
    messages = [
        {"role": "user", "content": user_message}
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Keep only newly generated tokens (ignore the prompt part)
    generated_ids = output_ids[0, input_ids.shape[-1]:]
    reply = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return reply


# -------------------------------------------------------------------
# SIMPLE INTERACTIVE LOOP
# -------------------------------------------------------------------
def main():
    print("\nInteractive test with trained model.")
    print("Type your question and press Enter.")
    print("Type 'exit' or 'quit' to finish.\n")

    while True:
        try:
            user_input = input("You: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting.")
            break

        if not user_input:
            continue

        if user_input.lower() in ("exit", "quit"):
            print("Goodbye.")
            break

        reply = generate_reply(user_input)
        print(f"AI: {reply}\n")


if __name__ == "__main__":
    main()
```

### app

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from flask import Flask, render_template, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------
BASE_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
MODEL_DIR       = "qwen25-05b-jvc"   # carpeta del modelo entrenado

MAX_NEW_TOKENS = 128
TEMPERATURE    = 0.7
TOP_P          = 0.9

# -------------------------------------------------------------------
# LOAD TOKENIZER + MODEL ONCE
# -------------------------------------------------------------------
print("üîß Loading tokenizer and model for Flask app...")

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype  = torch.float16 if device == "cuda" else torch.float32

print(f"Using device: {device}")

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

adapter_config_path = os.path.join(MODEL_DIR, "adapter_config.json")

if os.path.exists(adapter_config_path):
    # LoRA / PEFT case
    print(f"Detected PEFT adapter at '{MODEL_DIR}'. Loading base model + adapter...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        dtype=dtype,
    )
    model = PeftModel.from_pretrained(base_model, MODEL_DIR)
else:
    # Full fine-tuned model case
    print(f"No adapter_config.json found in '{MODEL_DIR}'.")
    print("Assuming full fine-tuned model directory. Loading directly...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        dtype=dtype,
    )

model.to(device)
model.eval()
print("‚úÖ Model loaded for Flask app.")


def generate_reply(user_message: str) -> str:
    """
    Given a user message, build a chat-style conversation and
    generate the assistant reply using Qwen's chat template.
    """
    messages = [
        {"role": "user", "content": user_message}
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            pad_token_id=tokenizer.eos_token_id,
        )

    generated_ids = output_ids[0, input_ids.shape[-1]:]
    reply = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return reply


# -------------------------------------------------------------------
# FLASK APP
# -------------------------------------------------------------------
app = Flask(__name__)


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/chat", methods=["POST"])
def chat():
    data = request.get_json(force=True)
    user_message = (data.get("message") or "").strip()

    if not user_message:
        return jsonify({"reply": ""})

    print(f"User: {user_message}")
    reply = generate_reply(user_message)
    print(f"AI:   {reply}")

    return jsonify({"reply": reply})


if __name__ == "__main__":
    # Development server
    app.run(host="0.0.0.0", port=5000, debug=True)
```

### conversacion

```
{"messages": [{"role": "user", "content": "Hola Mar√≠a! ¬øC√≥mo est√°s? Quer√≠a preguntarte sobre los ciclos formativos porque s√© que est√°s estudiando uno"}, {"role": "assistant", "content": "Hola Ana! Bien, gracias :) S√≠, estoy en segundo de Desarrollo de Aplicaciones Web. ¬øEn qu√© puedo ayudarte?"}]}
{"messages": [{"role": "user", "content": "Me estoy planteando hacer un ciclo formativo pero tengo muchas dudas. No s√© si hacer uno de grado medio o superior"}, {"role": "assistant", "content": "Te explico: los grados medios duran 2 a√±os y te dan acceso a grados superiores. Los superiores son 2 a√±os y dan acceso a la universidad\nDepende de lo que quieras estudiar y si tienes el t√≠tulo de bachillerato o no"}]}
{"messages": [{"role": "user", "content": "Tengo bachillerato, pero no s√© si quiero ir a la universidad directamente. Me gusta m√°s la parte pr√°ctica"}, {"role": "assistant", "content": "Entonces los ciclos son perfectos! Son muy pr√°cticos y con buena salida laboral. ¬øQu√© te interesa?"}]}
{"messages": [{"role": "user", "content": "Me gusta la inform√°tica y tambi√©n he visto algo de marketing. ¬øQu√© ciclos hay en esas √°reas?"}, {"role": "assistant", "content": "En inform√°tica tienes:\n- CFGS Desarrollo de Aplicaciones Web\n- CFGS Desarrollo de Aplicaciones Multiplataforma\n- CFGS Administraci√≥n de Sistemas Inform√°ticos en Red\n- CFGS Ciberseguridad\nEn marketing:\n- CFGS Marketing y Publicidad\n- CFGS Comercio Internacional\n- CFGS Transporte y Log√≠stica"}]}
{"messages": [{"role": "user", "content": "Vaya, hay m√°s de lo que pensaba! ¬øY c√≥mo son las clases? ¬øMucha teor√≠a?"}, {"role": "assistant", "content": "En mi experiencia, es 70% pr√°ctica y 30% teor√≠a. Hacemos proyectos reales, programamos, configuramos servidores...\nAdem√°s, al final hay pr√°cticas en empresa de 3 meses, que es lo mejor para coger experiencia"}]}
{"messages": [{"role": "user", "content": "Suena bien! ¬øY es dif√≠cil entrar? ¬øHay que hacer alguna prueba?"}, {"role": "assistant", "content": "Con bachillerato entras directamente por nota media. Sin bachillerato hay que hacer prueba de acceso. La demanda es alta en inform√°tica"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© nota suele hacer falta? Mi media es de 7.8"}, {"role": "assistant", "content": "Con 7.8 entras en casi todos, aunque para Desarrollo Web el a√±o pasado la nota de corte fue 8.1 en algunos sitios\nPero hay muchos centros, seguro que encuentras plaza"}]}
{"messages": [{"role": "user", "content": "¬øY c√≥mo son los horarios? ¬øSe puede compaginar con trabajo?"}, {"role": "assistant", "content": "Hay turno de ma√±ana (8:00-15:00) y en algunos centros turno de tarde (15:00-22:00). Tambi√©n hay modalidad semipresencial\nYo trabajo los fines de semana y voy a clase por la ma√±ana. Es cansado pero se puede"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© salidas laborales tiene realmente? Mis padres est√°n preocupados por eso"}, {"role": "assistant", "content": "En inform√°tica hay mucha demanda. Mis compa√±eros de promoci√≥n anterior el 80% encontr√≥ trabajo en menos de 3 meses\nLos sueldos de entrada est√°n entre 18k-24k ‚Ç¨, y con experiencia suben r√°pido"}]}
{"messages": [{"role": "user", "content": "Eso est√° bien para empezar. ¬øY las pr√°cticas en empresa son remuneradas?"}, {"role": "assistant", "content": "S√≠, por ley tienen que darte una beca. En mi caso son 400‚Ç¨ al mes, pero en algunas empresas pagan m√°s\nLo importante es que muchas empresas cogen a los becarios despu√©s de las pr√°cticas"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© me recomiendas? ¬øGrado medio o ir directamente al superior?"}, {"role": "assistant", "content": "Con tu bachillerato ve directamente al superior. Los medios son m√°s b√°sicos y para gente sin bachillerato\nAdem√°s, con el superior tienes m√°s salidas y mejor sueldo"}]}
{"messages": [{"role": "user", "content": "Vale, tengo que mirar bien las fechas de preinscripci√≥n. ¬øCu√°ndo suele ser?"}, {"role": "assistant", "content": "Depende de cada comunidad, pero generalmente:\n- Junio/julio para el curso que viene\n- Septiembre para plazas sobrantes\nTe recomiendo que empieces a mirar en abril/mayo los plazos exactos"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© documentos necesito para la preinscripci√≥n?"}, {"role": "assistant", "content": "DNI, certificado de notas de bachillerato, y luego depende del centro. Algunos piden foto, solicitud espec√≠fica...\nLo mejor es mirar en la web de educaci√≥n de tu comunidad aut√≥noma"}]}
{"messages": [{"role": "user", "content": "¬øY se puede solicitar en varios centros a la vez?"}, {"role": "assistant", "content": "S√≠, haces una lista ordenada por preferencia. Te asignan plaza seg√∫n tu nota y tus preferencias"}]}
{"messages": [{"role": "user", "content": "Perfecto! Otra duda: ¬øhay que comprar muchos libros? ¬øEs caro el material?"}, {"role": "assistant", "content": "En mi ciclo casi todo el material es digital. Los profes comparten apuntes y usamos software libre. El mayor gasto es un port√°til bueno\nYo me gast√© 600‚Ç¨ en un port√°til y con eso tengo para todo el ciclo"}]}
{"messages": [{"role": "user", "content": "Eso est√° bien. ¬øY las asignaturas son muy dif√≠ciles? Me preocupa especialmente la programaci√≥n"}, {"role": "assistant", "content": "Al principio cuesta, pero los profes empiezan desde cero. Lo importante es practicar mucho y no dejar cosas para el final\nYo no sab√≠a programar nada y ahora me defiendo bien. Hay que ser constante nada m√°s"}]}
{"messages": [{"role": "user", "content": "¬øY hay mucho trabajo en grupo? Esa es otra de mis preocupaciones"}, {"role": "assistant", "content": "S√≠, hay bastantes proyectos en grupo. Es buena preparaci√≥n para el mundo laboral donde todo es en equipo\nAl principio puede ser un rollo si te toca gente que no trabaja, pero aprendes a organizarte"}]}
{"messages": [{"role": "user", "content": "Vale, me lo estoy pensando seriamente. ¬øCrees que podr√≠a visitar tu centro alg√∫n d√≠a?"}, {"role": "assistant", "content": "Claro! En marzo suelen hacer jornadas de puertas abiertas. Tambi√©n puedo preguntar si puedes venir un d√≠a conmigo a alguna clase"}]}
{"messages": [{"role": "user", "content": "Eso ser√≠a genial! As√≠ veo c√≥mo es el ambiente"}, {"role": "assistant", "content": "Te aviso cuando sepa algo. Mientras tanto, te paso unos enlaces con informaci√≥n oficial\nhttps://www.todofp.es (este es el oficial del ministerio)\nY te recomiendo seguir en Instagram a @fp_informatica_ que comparten mucha info √∫til"}]}
{"messages": [{"role": "user", "content": "Much√≠simas gracias Mar√≠a! Me has aclarado un mont√≥n de dudas. Voy a mirar toda la informaci√≥n"}, {"role": "assistant", "content": "De nada! Cualquier otra duda me preguntas. √Ånimo con la decisi√≥n!"}]}
{"messages": [{"role": "user", "content": "Gracias! Hablamos pronto üòä\n\nHola Mar√≠a! Perdona que te moleste otra vez. Estoy mirando los m√≥dulos del ciclo de DAW y tengo dudas con las asignaturas"}, {"role": "assistant", "content": "Hola! No es molestia :) ¬øQu√© dudas tienes?"}]}
{"messages": [{"role": "user", "content": "Veo que hay asignaturas de programaci√≥n, bases de datos, lenguajes de marcas... ¬øCu√°les son las m√°s dif√≠ciles?"}, {"role": "assistant", "content": "Depende de cada persona. A m√≠ me cost√≥ m√°s Bases de Datos al principio, pero luego le coges el tranquillo\nProgramaci√≥n es la m√°s importante, se da durante los dos a√±os. Empiezas con Java y luego PHP, JavaScript..."}]}
{"messages": [{"role": "user", "content": "¬øY se dan muchos proyectos reales? Me interesa eso de hacer cosas pr√°cticas"}, {"role": "assistant", "content": "S√≠, en primero hacemos una web completa para un cliente ficticio, y en segundo un proyecto m√°s completo con app m√≥vil\nLo bueno es que al terminar tienes portfolio para mostrar en las entrevistas"}]}
{"messages": [{"role": "user", "content": "Genial! Otra cosa: ¬øhay que saber ingl√©s? Veo que algunas asignaturas tienen nombres en ingl√©s"}, {"role": "assistant", "content": "No es obligatorio, pero ayuda mucho. La documentaci√≥n t√©cnica est√° en ingl√©s y las empresas valoran mucho el ingl√©s t√©cnico\nEn algunos centros hay asignatura de ingl√©s t√©cnico opcional"}]}
{"messages": [{"role": "user", "content": "Vale, mi ingl√©s es regular pero puedo mejorarlo. ¬øY qu√© me dices del horario de tarde? Es el que me ir√≠a mejor"}, {"role": "assistant", "content": "El de tarde suele tener menos plazas pero tambi√©n menos demanda. La calidad es la misma, mismos profes y mismo temario"}]}
{"messages": [{"role": "user", "content": "Perfecto! Creo que me decanto por DAW en turno de tarde. Ahora toca esperar a la preinscripci√≥n"}, {"role": "assistant", "content": "Me alegro! Si necesitas ayuda con la solicitud me dices. Yo pas√© por eso el a√±o pasado"}]}
{"messages": [{"role": "user", "content": "Gracias Mar√≠a, eres un sol! Te tendr√© al tanto de c√≥mo va todo"}, {"role": "assistant", "content": "De nada! Suerte con todo üí™"}]}
{"messages": [{"role": "user", "content": "Buenos d√≠as Mar√≠a! Ya se abri√≥ la preinscripci√≥n y he solicitado DAW en tres centros. Cruzando los dedos!"}, {"role": "assistant", "content": "Qu√© bien! ¬øEn qu√© centros has puesto?"}]}
{"messages": [{"role": "user", "content": "1. IES Tu centro (el tuyo)\n2. IES Tecnol√≥gico\n3. IES Sistemas Inform√°ticos"}, {"role": "assistant", "content": "Ojal√° entres en el m√≠o! Ser√≠a genial tenerte de compa√±era el a√±o que viene"}]}
{"messages": [{"role": "user", "content": "Ojal√°! Por cierto, ¬øsabes cu√°ndo suelen publicar las listas?"}, {"role": "assistant", "content": "El a√±o pasado fue a finales de junio. Te llegar√° un email cuando salgan"}]}
{"messages": [{"role": "user", "content": "Vale, toca esperar entonces. ¬°Muchas gracias por toda tu ayuda!"}, {"role": "assistant", "content": "Para eso estamos! Cualquier cosa me escribes üòä"}]}
{"messages": [{"role": "user", "content": "MAR√çA!!! ME HAN ADMITIDO EN TU CENTRO!!! üòç"}, {"role": "assistant", "content": "NO ME DIGAS!!! ENHORABUENA!!! üéâüéâüéâ\nEstoy super contenta! Vas a ver c√≥mo te encanta"}]}
{"messages": [{"role": "user", "content": "No me lo creo a√∫n! Ten√≠a nota justa pero al final entr√©. Ahora toca formalizar la matr√≠cula"}, {"role": "assistant", "content": "Si necesitas ayuda con los papeles me dices. La matr√≠cula suele ser en julio"}]}
{"messages": [{"role": "user", "content": "Gracias! Te aviso si tengo dudas. Qu√© ilusi√≥n ser compa√±eras!"}, {"role": "assistant", "content": "A m√≠ tambi√©n me hace mucha ilusi√≥n! Bienvenida al mundo FP! üíª"}]}
```

### conversacion

```
Conversaci√≥n con Mar√≠a - CFGS

[28/09/2023 10:15] Ana: Hola Mar√≠a! ¬øC√≥mo est√°s? Quer√≠a preguntarte sobre los ciclos formativos porque s√© que est√°s estudiando uno
[28/09/2023 10:16] Mar√≠a: Hola Ana! Bien, gracias :) S√≠, estoy en segundo de Desarrollo de Aplicaciones Web. ¬øEn qu√© puedo ayudarte?
[28/09/2023 10:17] Ana: Me estoy planteando hacer un ciclo formativo pero tengo muchas dudas. No s√© si hacer uno de grado medio o superior
[28/09/2023 10:18] Mar√≠a: Te explico: los grados medios duran 2 a√±os y te dan acceso a grados superiores. Los superiores son 2 a√±os y dan acceso a la universidad
[28/09/2023 10:19] Mar√≠a: Depende de lo que quieras estudiar y si tienes el t√≠tulo de bachillerato o no
[28/09/2023 10:20] Ana: Tengo bachillerato, pero no s√© si quiero ir a la universidad directamente. Me gusta m√°s la parte pr√°ctica
[28/09/2023 10:21] Mar√≠a: Entonces los ciclos son perfectos! Son muy pr√°cticos y con buena salida laboral. ¬øQu√© te interesa?
[28/09/2023 10:22] Ana: Me gusta la inform√°tica y tambi√©n he visto algo de marketing. ¬øQu√© ciclos hay en esas √°reas?
[28/09/2023 10:23] Mar√≠a: En inform√°tica tienes:
- CFGS Desarrollo de Aplicaciones Web
- CFGS Desarrollo de Aplicaciones Multiplataforma
- CFGS Administraci√≥n de Sistemas Inform√°ticos en Red
- CFGS Ciberseguridad
[28/09/2023 10:24] Mar√≠a: En marketing:
- CFGS Marketing y Publicidad
- CFGS Comercio Internacional
- CFGS Transporte y Log√≠stica
[28/09/2023 10:25] Ana: Vaya, hay m√°s de lo que pensaba! ¬øY c√≥mo son las clases? ¬øMucha teor√≠a?
[28/09/2023 10:26] Mar√≠a: En mi experiencia, es 70% pr√°ctica y 30% teor√≠a. Hacemos proyectos reales, programamos, configuramos servidores...
[28/09/2023 10:27] Mar√≠a: Adem√°s, al final hay pr√°cticas en empresa de 3 meses, que es lo mejor para coger experiencia
[28/09/2023 10:28] Ana: Suena bien! ¬øY es dif√≠cil entrar? ¬øHay que hacer alguna prueba?
[28/09/2023 10:29] Mar√≠a: Con bachillerato entras directamente por nota media. Sin bachillerato hay que hacer prueba de acceso. La demanda es alta en inform√°tica
[28/09/2023 10:30] Ana: ¬øY qu√© nota suele hacer falta? Mi media es de 7.8
[28/09/2023 10:31] Mar√≠a: Con 7.8 entras en casi todos, aunque para Desarrollo Web el a√±o pasado la nota de corte fue 8.1 en algunos sitios
[28/09/2023 10:32] Mar√≠a: Pero hay muchos centros, seguro que encuentras plaza
[28/09/2023 10:33] Ana: ¬øY c√≥mo son los horarios? ¬øSe puede compaginar con trabajo?
[28/09/2023 10:34] Mar√≠a: Hay turno de ma√±ana (8:00-15:00) y en algunos centros turno de tarde (15:00-22:00). Tambi√©n hay modalidad semipresencial
[28/09/2023 10:35] Mar√≠a: Yo trabajo los fines de semana y voy a clase por la ma√±ana. Es cansado pero se puede
[28/09/2023 10:36] Ana: ¬øY qu√© salidas laborales tiene realmente? Mis padres est√°n preocupados por eso
[28/09/2023 10:37] Mar√≠a: En inform√°tica hay mucha demanda. Mis compa√±eros de promoci√≥n anterior el 80% encontr√≥ trabajo en menos de 3 meses
[28/09/2023 10:38] Mar√≠a: Los sueldos de entrada est√°n entre 18k-24k ‚Ç¨, y con experiencia suben r√°pido
[28/09/2023 10:39] Ana: Eso est√° bien para empezar. ¬øY las pr√°cticas en empresa son remuneradas?
[28/09/2023 10:40] Mar√≠a: S√≠, por ley tienen que darte una beca. En mi caso son 400‚Ç¨ al mes, pero en algunas empresas pagan m√°s
[28/09/2023 10:41] Mar√≠a: Lo importante es que muchas empresas cogen a los becarios despu√©s de las pr√°cticas
[28/09/2023 10:42] Ana: ¬øY qu√© me recomiendas? ¬øGrado medio o ir directamente al superior?
[28/09/2023 10:43] Mar√≠a: Con tu bachillerato ve directamente al superior. Los medios son m√°s b√°sicos y para gente sin bachillerato
[28/09/2023 10:44] Mar√≠a: Adem√°s, con el superior tienes m√°s salidas y mejor sueldo
[28/09/2023 10:45] Ana: Vale, tengo que mirar bien las fechas de preinscripci√≥n. ¬øCu√°ndo suele ser?
[28/09/2023 10:46] Mar√≠a: Depende de cada comunidad, pero generalmente:
- Junio/julio para el curso que viene
- Septiembre para plazas sobrantes
[28/09/2023 10:47] Mar√≠a: Te recomiendo que empieces a mirar en abril/mayo los plazos exactos
[28/09/2023 10:48] Ana: ¬øY qu√© documentos necesito para la preinscripci√≥n?
[28/09/2023 10:49] Mar√≠a: DNI, certificado de notas de bachillerato, y luego depende del centro. Algunos piden foto, solicitud espec√≠fica...
[28/09/2023 10:50] Mar√≠a: Lo mejor es mirar en la web de educaci√≥n de tu comunidad aut√≥noma
[28/09/2023 10:51] Ana: ¬øY se puede solicitar en varios centros a la vez?
[28/09/2023 10:52] Mar√≠a: S√≠, haces una lista ordenada por preferencia. Te asignan plaza seg√∫n tu nota y tus preferencias
[28/09/2023 10:53] Ana: Perfecto! Otra duda: ¬øhay que comprar muchos libros? ¬øEs caro el material?
[28/09/2023 10:54] Mar√≠a: En mi ciclo casi todo el material es digital. Los profes comparten apuntes y usamos software libre. El mayor gasto es un port√°til bueno
[28/09/2023 10:55] Mar√≠a: Yo me gast√© 600‚Ç¨ en un port√°til y con eso tengo para todo el ciclo
[28/09/2023 10:56] Ana: Eso est√° bien. ¬øY las asignaturas son muy dif√≠ciles? Me preocupa especialmente la programaci√≥n
[28/09/2023 10:57] Mar√≠a: Al principio cuesta, pero los profes empiezan desde cero. Lo importante es practicar mucho y no dejar cosas para el final
[28/09/2023 10:58] Mar√≠a: Yo no sab√≠a programar nada y ahora me defiendo bien. Hay que ser constante nada m√°s
[28/09/2023 10:59] Ana: ¬øY hay mucho trabajo en grupo? Esa es otra de mis preocupaciones
[28/09/2023 11:00] Mar√≠a: S√≠, hay bastantes proyectos en grupo. Es buena preparaci√≥n para el mundo laboral donde todo es en equipo
[28/09/2023 11:01] Mar√≠a: Al principio puede ser un rollo si te toca gente que no trabaja, pero aprendes a organizarte
[28/09/2023 11:02] Ana: Vale, me lo estoy pensando seriamente. ¬øCrees que podr√≠a visitar tu centro alg√∫n d√≠a?
[28/09/2023 11:03] Mar√≠a: Claro! En marzo suelen hacer jornadas de puertas abiertas. Tambi√©n puedo preguntar si puedes venir un d√≠a conmigo a alguna clase
[28/09/2023 11:04] Ana: Eso ser√≠a genial! As√≠ veo c√≥mo es el ambiente
[28/09/2023 11:05] Mar√≠a: Te aviso cuando sepa algo. Mientras tanto, te paso unos enlaces con informaci√≥n oficial
[28/09/2023 11:06] Mar√≠a: https://www.todofp.es (este es el oficial del ministerio)
[28/09/2023 11:07] Mar√≠a: Y te recomiendo seguir en Instagram a @fp_informatica_ que comparten mucha info √∫til
[28/09/2023 11:08] Ana: Much√≠simas gracias Mar√≠a! Me has aclarado un mont√≥n de dudas. Voy a mirar toda la informaci√≥n
[28/09/2023 11:09] Mar√≠a: De nada! Cualquier otra duda me preguntas. √Ånimo con la decisi√≥n!
[28/09/2023 11:10] Ana: Gracias! Hablamos pronto üòä

[15/10/2023 16:20] Ana: Hola Mar√≠a! Perdona que te moleste otra vez. Estoy mirando los m√≥dulos del ciclo de DAW y tengo dudas con las asignaturas
[15/10/2023 16:21] Mar√≠a: Hola! No es molestia :) ¬øQu√© dudas tienes?
[15/10/2023 16:22] Ana: Veo que hay asignaturas de programaci√≥n, bases de datos, lenguajes de marcas... ¬øCu√°les son las m√°s dif√≠ciles?
[15/10/2023 16:23] Mar√≠a: Depende de cada persona. A m√≠ me cost√≥ m√°s Bases de Datos al principio, pero luego le coges el tranquillo
[15/10/2023 16:24] Mar√≠a: Programaci√≥n es la m√°s importante, se da durante los dos a√±os. Empiezas con Java y luego PHP, JavaScript...
[15/10/2023 16:25] Ana: ¬øY se dan muchos proyectos reales? Me interesa eso de hacer cosas pr√°cticas
[15/10/2023 16:26] Mar√≠a: S√≠, en primero hacemos una web completa para un cliente ficticio, y en segundo un proyecto m√°s completo con app m√≥vil
[15/10/2023 16:27] Mar√≠a: Lo bueno es que al terminar tienes portfolio para mostrar en las entrevistas
[15/10/2023 16:28] Ana: Genial! Otra cosa: ¬øhay que saber ingl√©s? Veo que algunas asignaturas tienen nombres en ingl√©s
[15/10/2023 16:29] Mar√≠a: No es obligatorio, pero ayuda mucho. La documentaci√≥n t√©cnica est√° en ingl√©s y las empresas valoran mucho el ingl√©s t√©cnico
[15/10/2023 16:30] Mar√≠a: En algunos centros hay asignatura de ingl√©s t√©cnico opcional
[15/10/2023 16:31] Ana: Vale, mi ingl√©s es regular pero puedo mejorarlo. ¬øY qu√© me dices del horario de tarde? Es el que me ir√≠a mejor
[15/10/2023 16:32] Mar√≠a: El de tarde suele tener menos plazas pero tambi√©n menos demanda. La calidad es la misma, mismos profes y mismo temario
[15/10/2023 16:33] Ana: Perfecto! Creo que me decanto por DAW en turno de tarde. Ahora toca esperar a la preinscripci√≥n
[15/10/2023 16:34] Mar√≠a: Me alegro! Si necesitas ayuda con la solicitud me dices. Yo pas√© por eso el a√±o pasado
[15/10/2023 16:35] Ana: Gracias Mar√≠a, eres un sol! Te tendr√© al tanto de c√≥mo va todo
[15/10/2023 16:36] Mar√≠a: De nada! Suerte con todo üí™

[12/04/2024 09:30] Ana: Buenos d√≠as Mar√≠a! Ya se abri√≥ la preinscripci√≥n y he solicitado DAW en tres centros. Cruzando los dedos!
[12/04/2024 09:31] Mar√≠a: Qu√© bien! ¬øEn qu√© centros has puesto?
[12/04/2024 09:32] Ana: 1. IES Tu centro (el tuyo)
2. IES Tecnol√≥gico
3. IES Sistemas Inform√°ticos
[12/04/2024 09:33] Mar√≠a: Ojal√° entres en el m√≠o! Ser√≠a genial tenerte de compa√±era el a√±o que viene
[12/04/2024 09:34] Ana: Ojal√°! Por cierto, ¬øsabes cu√°ndo suelen publicar las listas?
[12/04/2024 09:35] Mar√≠a: El a√±o pasado fue a finales de junio. Te llegar√° un email cuando salgan
[12/04/2024 09:36] Ana: Vale, toca esperar entonces. ¬°Muchas gracias por toda tu ayuda!
[12/04/2024 09:37] Mar√≠a: Para eso estamos! Cualquier cosa me escribes üòä

[28/06/2024 11:15] Ana: MAR√çA!!! ME HAN ADMITIDO EN TU CENTRO!!! üòç
[28/06/2024 11:16] Mar√≠a: NO ME DIGAS!!! ENHORABUENA!!! üéâüéâüéâ
[28/06/2024 11:17] Mar√≠a: Estoy super contenta! Vas a ver c√≥mo te encanta
[28/06/2024 11:18] Ana: No me lo creo a√∫n! Ten√≠a nota justa pero al final entr√©. Ahora toca formalizar la matr√≠cula
[28/06/2024 11:19] Mar√≠a: Si necesitas ayuda con los papeles me dices. La matr√≠cula suele ser en julio
[28/06/2024 11:20] Ana: Gracias! Te aviso si tengo dudas. Qu√© ilusi√≥n ser compa√±eras!
[28/06/2024 11:21] Mar√≠a: A m√≠ tambi√©n me hace mucha ilusi√≥n! Bienvenida al mundo FP! üíª
```

### instrucciones creacion de un entorno virtual

```markdown
python3 -m venv .venv

source .venv/bin/activate

pip install transformers datasets peft accelerate

pip install torch
```

<a id="entrenar-chatbot-a-partir-de-pdf"></a>
## entrenar chatbot a partir de pdf

### prompt

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano.

- Recorre todos los .txt de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama (Llama3 8B):
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Pensado para funcionar con:
- Ubuntu 24.04
- Ollama sirviendo en http://localhost:11434
- Modelo llama3:8b (o similar) cargado en GPU (GTX 1660).
"""

import os
import json
import requests
from typing import List, Dict

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Modelo de Ollama
OLLAMA_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "llama3:8b"   # ajusta si usas otro tag, p.ej. "llama3:8b-instruct"

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite de generaci√≥n aproximado (Ollama lo ajusta internamente)


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.txt",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# LLAMADA A OLLAMA
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": False,
        # "options": {"num_ctx": 2048, ...}  # opcional
    }

    resp = requests.post(OLLAMA_URL, json=payload)
    resp.raise_for_status()
    data = resp.json()
    return data["message"]["content"]


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, intents, etc. Intentamos limpiar.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            # Intenta recortar desde el primer '{' hasta el √∫ltimo '}'
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO
# =========================

def process_single_file(path: str) -> List[Dict[str, str]]:
    print(f"\n[INFO] Procesando archivo: {path}")
    raw_text = read_text_file(path)
    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignora.")
        return []

    text = normalize_whitespace(raw_text)
    blocks = split_into_blocks(text)
    print(f"[INFO] Texto troceado en {len(blocks)} bloques.")

    file_pairs: List[Dict[str, str]] = []

    for i, block in enumerate(blocks, start=1):
        print(f"[INFO]   Bloque {i}/{len(blocks)} (longitud {len(block)} chars)")
        block_pairs = generate_qa_for_block(block)
        print(f"[INFO]   -> {len(block_pairs)} pares Q/A generados para este bloque.")
        file_pairs.extend(block_pairs)

    print(f"[INFO] Total pares Q/A para {os.path.basename(path)}: {len(file_pairs)}")
    return file_pairs


# =========================
# ESCRITURA DE SALIDA
# =========================

def write_pairs_to_jsonl(pairs: List[Dict[str, str]], output_path: str):
    """
    Escribe TODOS los pares en un JSONL (sobrescribe si ya existe).
    """
    with open(output_path, "w", encoding="utf-8") as f:
        for p in pairs:
            f.write(json.dumps(p, ensure_ascii=False) + "\n")


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    txt_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith(".txt")
    ]

    if not txt_files:
        print(f"[INFO] No se han encontrado .txt en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = 0

    for path in sorted(txt_files):
        if path in processed_files:
            print(f"[INFO] Archivo ya procesado seg√∫n log.json, se omite: {path}")
            skipped_count += 1
            continue

        pairs = process_single_file(path)

        # Nombre del JSONL por archivo
        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        write_pairs_to_jsonl(pairs, per_file_output)
        total_pairs += len(pairs)

        # Actualizar log y guardar inmediatamente para no perder progreso
        log.setdefault("processed_files", []).append(path)
        save_log(log)
        processed_files.add(path)
        newly_processed_count += 1

        print(f"[INFO] JSONL generado para {path}: {per_file_output}")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(txt_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### pregunta inicial

```markdown
llama3.1:8b-instruct-q4_0

ollama run llama3.1:8b-instruct-q4_0
```

### pdf a md

```python
import fitz  # PyMuPDF
import pymupdf4llm

def pdf_to_markdown(input_pdf, output_md):
    # Open the PDF
    doc = fitz.open(input_pdf)
    
    # Convert entire document to Markdown
    md_text = pymupdf4llm.to_markdown(doc)
    
    # Save to .md file
    with open(output_md, "w", encoding="utf-8") as f:
        f.write(md_text)

if __name__ == "__main__":
    pdf_to_markdown("pdf/Resumen m√≥dulos profesionales SMR_DAM_DAW.pdf", "input/resumen.md")
```

### instalar entorno virtual

```markdown
python3 -m venv .venv

source .venv/bin/activate

pip install transformers datasets peft accelerate

pip install torch

pip install pymupdf pymupdf4llm
```

### prueba

```python
import requests
import json

OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL = "gemma2:9b-instruct-q4_0"

def ask_ollama(question: str) -> str:
    payload = {
        "model": MODEL,
        "prompt": question,
        "stream": False   # set to True if you want streaming tokens
    }

    response = requests.post(OLLAMA_URL, json=payload)
    response.raise_for_status()

    data = response.json()
    return data.get("response", "")

if __name__ == "__main__":
    user_input = input("Enter your question: ")
    answer = ask_ollama(user_input)
    print("\n=== Ollama Response ===\n")
    print(answer)
```

### md a jsonl

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano o Markdown.

- Recorre todos los .txt y .md de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.

Requisitos:
- Ollama sirviendo en http://localhost:11434
- Modelo MODEL_NAME presente en Ollama.
"""

import os
import re
import json
import requests
from typing import List, Dict, Optional

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.

    - Quita bloques de c√≥digo triple ``` ... ```
    - Quita c√≥digo en l√≠nea `...`
    - Convierte enlaces [texto](url) en solo "texto"
    - Quita im√°genes ![alt](url)
    - Quita cabeceras de Markdown (#, ##, ###)
    - Quita marcadores de lista iniciales (-, *, +) al inicio de l√≠nea
    - Quita negritas/cursivas (**texto**, *texto*, __texto__, _texto_)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.md",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.

    Devuelve True si alguno de los dos endpoints funciona, False si ninguno.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).

    - En modo "chat": usa /api/chat con messages.
    - En modo "generate": concatena system_prompt + user_prompt en un solo prompt
      y llama a /api/generate.
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, etc. Limpiamos.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str, output_path: str) -> int:
    """
    Procesa un √∫nico fichero de entrada y va escribiendo las Q/A
    en JSONL sobre la marcha en output_path.

    Devuelve el n√∫mero total de pares generados.
    """
    print(f"\n[INFO] Procesando archivo: {path}")
    raw_text = read_text_file(path)
    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignora.")
        return 0

    # 1) Limpiar Markdown
    cleaned = strip_markdown(raw_text)
    # 2) Normalizar espacios
    text = normalize_whitespace(cleaned)

    if not text.strip():
        print("[WARN] Tras limpiar Markdown el archivo qued√≥ vac√≠o, se ignora.")
        return 0

    blocks = split_into_blocks(text)
    print(f"[INFO] Texto troceado en {len(blocks)} bloques.")

    # Truncar el fichero de salida al inicio, por si existe de ejecuciones anteriores
    with open(output_path, "w", encoding="utf-8") as f:
        pass

    total_pairs_for_file = 0

    for i, block in enumerate(blocks, start=1):
        print(f"[INFO]   Bloque {i}/{len(blocks)} (longitud {len(block)} chars)")

        block_pairs = generate_qa_for_block(block)
        print(f"[INFO]   -> {len(block_pairs)} pares Q/A generados para este bloque.")

        # Guardar inmediatamente en el JSONL
        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

    print(f"[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    input_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith((".txt", ".md"))
    ]

    if not input_files:
        print(f"[INFO] No se han encontrado .txt ni .md en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = 0

    for path in sorted(input_files):
        if path in processed_files:
            print(f"[INFO] Archivo ya procesado seg√∫n log.json, se omite: {path}")
            skipped_count += 1
            continue

        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        pairs_count = process_single_file(path, per_file_output)
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        processed_files.add(path)
        newly_processed_count += 1

        print(f"[INFO] JSONL generado para {path}: {per_file_output}")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### streaming

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano o Markdown.

- Recorre todos los .txt y .md de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.

Requisitos:
- Ollama sirviendo en http://localhost:11434
- Modelo MODEL_NAME presente en Ollama.
"""

import os
import re
import json
import requests
from typing import List, Dict, Optional

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.

    - Quita bloques de c√≥digo triple ``` ... ```
    - Quita c√≥digo en l√≠nea `...`
    - Convierte enlaces [texto](url) en solo "texto"
    - Quita im√°genes ![alt](url)
    - Quita cabeceras de Markdown (#, ##, ###)
    - Quita marcadores de lista iniciales (-, *, +) al inicio de l√≠nea
    - Quita negritas/cursivas (**texto**, *texto*, __texto__, _texto_)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.md",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.

    Devuelve True si alguno de los dos endpoints funciona, False si ninguno.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).

    - En modo "chat": usa /api/chat con messages.
    - En modo "generate": concatena system_prompt + user_prompt en un solo prompt
      y llama a /api/generate.
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, etc. Limpiamos.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str, output_path: str) -> int:
    """
    Procesa un √∫nico fichero de entrada y va escribiendo las Q/A
    en JSONL sobre la marcha en output_path.

    Devuelve el n√∫mero total de pares generados.
    """
    print(f"\n[INFO] Procesando archivo: {path}")
    raw_text = read_text_file(path)
    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignora.")
        return 0

    # 1) Limpiar Markdown
    cleaned = strip_markdown(raw_text)
    # 2) Normalizar espacios
    text = normalize_whitespace(cleaned)

    if not text.strip():
        print("[WARN] Tras limpiar Markdown el archivo qued√≥ vac√≠o, se ignora.")
        return 0

    blocks = split_into_blocks(text)
    print(f"[INFO] Texto troceado en {len(blocks)} bloques.")

    # Truncar el fichero de salida al inicio, por si existe de ejecuciones anteriores
    with open(output_path, "w", encoding="utf-8") as f:
        pass

    total_pairs_for_file = 0

    for i, block in enumerate(blocks, start=1):
        print(f"[INFO]   Bloque {i}/{len(blocks)} (longitud {len(block)} chars)")

        block_pairs = generate_qa_for_block(block)
        print(f"[INFO]   -> {len(block_pairs)} pares Q/A generados para este bloque.")

        # Guardar inmediatamente en el JSONL
        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

    print(f"[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    input_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith((".txt", ".md"))
    ]

    if not input_files:
        print(f"[INFO] No se han encontrado .txt ni .md en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = 0

    for path in sorted(input_files):
        if path in processed_files:
            print(f"[INFO] Archivo ya procesado seg√∫n log.json, se omite: {path}")
            skipped_count += 1
            continue

        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        pairs_count = process_single_file(path, per_file_output)
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        processed_files.add(path)
        newly_processed_count += 1

        print(f"[INFO] JSONL generado para {path}: {per_file_output}")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### estadisticas

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano o Markdown.

- Recorre todos los .txt y .md de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.
- Muestra una barra de progreso global con:
    * porcentaje completado,
    * tiempo transcurrido,
    * tiempo estimado restante (ETA).
"""

import os
import re
import json
import time
import shutil
import requests
from typing import List, Dict, Optional

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.

    - Quita bloques de c√≥digo triple ``` ... ```
    - Quita c√≥digo en l√≠nea `...`
    - Convierte enlaces [texto](url) en solo "texto"
    - Quita im√°genes ![alt](url)
    - Quita cabeceras de Markdown (#, ##, ###)
    - Quita marcadores de lista iniciales (-, *, +) al inicio de l√≠nea
    - Quita negritas/cursivas (**texto**, *texto*, __texto__, _texto_)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text] if text else []

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.md",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# TRACKER DE PROGRESO
# =========================

class ProgressTracker:
    """
    Barra de progreso global basada en n√∫mero total de bloques.
    Muestra:
    - porcentaje completado
    - tiempo transcurrido
    - ETA estimada
    """

    def __init__(self, total_units: int):
        self.total = max(1, total_units)
        self.current = 0
        self.start_time = time.time()

    @staticmethod
    def _format_seconds(secs: float) -> str:
        secs = int(secs)
        h = secs // 3600
        m = (secs % 3600) // 60
        s = secs % 60
        if h > 0:
            return f"{h:02d}:{m:02d}:{s:02d}"
        return f"{m:02d}:{s:02d}"

    def update(self, step: int = 1, prefix: str = ""):
        self.current += step
        if self.current > self.total:
            self.current = self.total

        elapsed = time.time() - self.start_time
        percent = (self.current / self.total) * 100.0

        if self.current > 0:
            rate = elapsed / self.current
            remaining = rate * (self.total - self.current)
        else:
            remaining = 0.0

        try:
            term_width = shutil.get_terminal_size((80, 20)).columns
        except Exception:
            term_width = 80

        bar_len = max(10, term_width - 55)
        filled = int(bar_len * self.current / self.total)
        bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)

        msg = (
            f"{prefix} [{bar}] {percent:6.2f}% "
            f"| t+{self._format_seconds(elapsed)} "
            f"| ETA {self._format_seconds(remaining)}"
        )

        # Recortar al ancho de la terminal para evitar artefactos
        msg = msg[:term_width - 1]
        print("\r" + msg, end="", flush=True)

    def finish(self, prefix: str = ""):
        self.update(step=0, prefix=prefix)
        print()  # salto de l√≠nea final


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.

    Devuelve True si alguno de los dos endpoints funciona, False si ninguno.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).

    - En modo "chat": usa /api/chat con messages.
    - En modo "generate": concatena system_prompt + user_prompt en un solo prompt
      y llama a /api/generate.
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, etc. Limpiamos.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# PREPARACI√ìN DE CADA FICHERO (LECTURA + TROCEO)
# =========================

def prepare_blocks_for_file(path: str) -> List[str]:
    """
    Lee el fichero, limpia Markdown y espacios, y lo trocea en bloques.
    Si queda vac√≠o, devuelve [].
    """
    print(f"\n[INFO] Preparando archivo: {path}")
    raw_text = read_text_file(path)

    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignorar√° (0 bloques).")
        return []

    cleaned = strip_markdown(raw_text)
    text = normalize_whitespace(cleaned)

    if not text.strip():
        print("[WARN] Tras limpiar Markdown el archivo qued√≥ vac√≠o, se ignorar√° (0 bloques).")
        return []

    blocks = split_into_blocks(text)
    print(f"[INFO]   -> {len(blocks)} bloques detectados.")
    return blocks


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str,
                        output_path: str,
                        blocks: List[str],
                        tracker: Optional[ProgressTracker],
                        file_index: int,
                        total_files: int) -> int:
    """
    Procesa un √∫nico fichero de entrada y va escribiendo las Q/A
    en JSONL sobre la marcha en output_path.

    Devuelve el n√∫mero total de pares generados.
    """
    print(f"\n[INFO] Procesando archivo {file_index}/{total_files}: {path}")

    if not blocks:
        # Generar un JSONL vac√≠o para dejar constancia de que se proces√≥
        with open(output_path, "w", encoding="utf-8"):
            pass
        print("[INFO] Archivo sin bloques, JSONL vac√≠o generado.")
        return 0

    # Truncar el fichero de salida al inicio, por si existe de ejecuciones anteriores
    with open(output_path, "w", encoding="utf-8"):
        pass

    total_pairs_for_file = 0

    for block in blocks:
        block_pairs = generate_qa_for_block(block)

        # Guardar inmediatamente en el JSONL
        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

        # Actualizar progreso global (un "unit" por bloque)
        if tracker is not None:
            tracker.update(
                step=1,
                prefix=f"[PROGRESO] Archivo {file_index}/{total_files}"
            )

    print(f"\n[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    print(f"[INFO] JSONL generado para {path}: {output_path}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    input_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith((".txt", ".md"))
    ]

    if not input_files:
        print(f"[INFO] No se han encontrado .txt ni .md en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    # Filtrar solo los archivos pendientes de procesar
    pending_files = [p for p in sorted(input_files) if p not in processed_files]

    if not pending_files:
        print("[INFO] Todos los archivos presentes ya estaban procesados seg√∫n log.json.")
        print(f"[INFO] Log de materiales procesados: {LOG_FILE}")
        return

    # Primera pasada: preparar bloques por fichero y contar bloques totales
    print("\n[INFO] Calculando n√∫mero total de bloques para la barra de progreso global...")
    file_blocks_map: Dict[str, List[str]] = {}
    total_blocks = 0

    for path in pending_files:
        blocks = prepare_blocks_for_file(path)
        file_blocks_map[path] = blocks
        total_blocks += len(blocks)

    if total_blocks == 0:
        print("[WARN] No se han encontrado bloques de texto √∫tiles en los ficheros pendientes.")
        print("       Se actualizar√° el log, pero no se generar√°n Q/A.")
        # Aun as√≠ generamos JSONL vac√≠os para dejar constancia
        newly_processed_count = 0
        for path in pending_files:
            base_name = os.path.splitext(os.path.basename(path))[0]
            per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")
            with open(per_file_output, "w", encoding="utf-8"):
                pass
            log.setdefault("processed_files", []).append(path)
            save_log(log)
            newly_processed_count += 1

        print("\n[RESUMEN]")
        print(f"Archivos encontrados              : {len(input_files)}")
        print(f"Archivos ya procesados (skip)     : {len(input_files) - len(pending_files)}")
        print(f"Archivos procesados en esta run   : {newly_processed_count}")
        print(f"Pares Q/A generados en esta run   : 0")
        print(f"Log de materiales procesados      : {LOG_FILE}")
        print(f"JSONL individuales en             : {OUTPUT_DIR}")
        return

    tracker = ProgressTracker(total_blocks)

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = len(input_files) - len(pending_files)

    print(f"[INFO] Total de bloques a procesar: {total_blocks}")
    print("[INFO] Iniciando generaci√≥n de Q/A con barra de progreso global...\n")

    for idx, path in enumerate(pending_files, start=1):
        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        blocks = file_blocks_map.get(path, [])
        pairs_count = process_single_file(
            path=path,
            output_path=per_file_output,
            blocks=blocks,
            tracker=tracker,
            file_index=idx,
            total_files=len(pending_files)
        )
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        newly_processed_count += 1

    # Cerrar visualmente la barra de progreso
    tracker.finish(prefix="[PROGRESO]")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

<a id="scrapeador-web-y-entrenamiento"></a>
## scrapeador web y entrenamiento

### Entrar en una web

```python
import requests

url = "https://tameformacion.com/"

try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()  # Raises an error for 4xx/5xx responses

    html_content = response.text
    print(html_content)

except requests.exceptions.RequestException as e:
    print(f"Error obtaining the page: {e}")
```

### guardar web

```python
import requests
import os

url = "https://tameformacion.com/"
output_folder = "paginas_html"
output_file = "example.html"

# Create folder if it does not exist
os.makedirs(output_folder, exist_ok=True)

try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()

    html_content = response.text

    # Full path for the saved file
    filepath = os.path.join(output_folder, output_file)

    # Save HTML to disk
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"HTML saved to: {filepath}")

except requests.exceptions.RequestException as e:
    print(f"Error: {e}")
```

### adivinar nombre de la web

```python
import requests
import os
import hashlib
from datetime import datetime
from urllib.parse import urlparse

url = "https://tameformacion.com/"
output_folder = "paginas_html"

# Create folder if it does not exist
os.makedirs(output_folder, exist_ok=True)

def get_filename_from_url(url: str) -> str:
    """Returns a filename based on the URL path, if possible."""
    parsed = urlparse(url)
    path = parsed.path.strip("/")

    # If URL ends with a filename (e.g., page.php, index.html)
    if "." in path:
        return path

    # If URL has a path but no extension (e.g., /contact/)
    if path:
        return path + ".html"

    # If nothing can be extracted, return empty string
    return ""


def generate_hash_filename(url: str) -> str:
    """Generates filename using hash + datetime."""
    sha1 = hashlib.sha1(url.encode()).hexdigest()[:12]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{sha1}_{timestamp}.html"


try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    html_content = response.text

    # Determine filename
    filename = get_filename_from_url(url)
    if not filename:
        filename = generate_hash_filename(url)

    filepath = os.path.join(output_folder, filename)

    # Save HTML
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"HTML saved to: {filepath}")

except requests.exceptions.RequestException as e:
    print(f"Error: {e}")
```

### timeout entre paginas

```python
import requests
import os
import hashlib
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, urlunparse
from collections import deque

from bs4 import BeautifulSoup

# -------------------- CONFIGURATION --------------------
START_URL = "https://tameformacion.com/"
OUTPUT_FOLDER = "paginas_html"
MAX_PAGES = 200                     # Safety limit
REQUEST_DELAY_SECONDS = 1          # <<< Delay between pages
# -------------------------------------------------------


# ---------- Helpers for filenames ----------

def get_filename_from_url(url: str) -> str:
    parsed = urlparse(url)
    path = parsed.path.strip("/")

    if "." in path:
        return path

    if path:
        return path + ".html"

    return ""


def generate_hash_filename(url: str) -> str:
    sha1 = hashlib.sha1(url.encode()).hexdigest()[:12]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{sha1}_{timestamp}.html"


def get_output_path_for_url(url: str) -> str:
    filename = get_filename_from_url(url)
    if not filename:
        filename = generate_hash_filename(url)

    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    return os.path.join(OUTPUT_FOLDER, filename)


# ---------- Helpers for normalization / filtering ----------

def normalize_url(url: str) -> str:
    parsed = urlparse(url)
    parsed = parsed._replace(fragment="")
    return urlunparse(parsed)


def same_domain(url1: str, url2: str) -> bool:
    n1 = urlparse(url1).netloc.lower()
    n2 = urlparse(url2).netloc.lower()

    if n1.startswith("www."):
        n1 = n1[4:]
    if n2.startswith("www."):
        n2 = n2[4:]

    return n1 == n2


def is_interesting_link(href: str) -> bool:
    if not href:
        return False
    href = href.strip()
    if href.startswith("#"):
        return False
    if href.startswith("mailto:"):
        return False
    if href.startswith("tel:"):
        return False
    if href.lower().startswith("javascript:"):
        return False
    return True


# ---------- Main crawler ----------

def crawl(start_url: str):
    visited = set()
    queue = deque([start_url])

    while queue and len(visited) < MAX_PAGES:
        current_url = queue.popleft()
        current_url = normalize_url(current_url)

        if current_url in visited:
            continue

        print(f"[{len(visited)+1}] Fetching: {current_url}")
        visited.add(current_url)

        try:
            response = requests.get(current_url, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"  Error fetching {current_url}: {e}")
            continue

        # Respect delay between requests
        print(f"  Waiting {REQUEST_DELAY_SECONDS} seconds...")
        time.sleep(REQUEST_DELAY_SECONDS)

        content_type = response.headers.get("Content-Type", "")
        if "text/html" not in content_type:
            print(f"  Skipping non-HTML content: {content_type}")
            continue

        html_content = response.text

        filepath = get_output_path_for_url(current_url)
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"  Saved to: {filepath}")
        except OSError as e:
            print(f"  Error saving {filepath}: {e}")

        soup = BeautifulSoup(html_content, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not is_interesting_link(href):
                continue

            new_url = urljoin(current_url, href)
            new_url = normalize_url(new_url)

            if same_domain(start_url, new_url) and new_url not in visited:
                queue.append(new_url)


if __name__ == "__main__":
    crawl(START_URL)
    print("Crawl finished.")
```

### tambien quiero los pdf

```python
import requests
import os
import hashlib
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, urlunparse
from collections import deque

from bs4 import BeautifulSoup

# -------------------- CONFIGURATION --------------------
START_URL = "https://tameformacion.com/"
OUTPUT_FOLDER = "paginas_html"
MAX_PAGES = 200              # Safety limit
REQUEST_DELAY_SECONDS = 1    # Delay between requests
# -------------------------------------------------------


# ---------- Helpers for filenames ----------

def get_filename_from_url(url: str) -> str:
    """
    Try to extract a filename from the URL path.
    If the path contains a dot, we assume it has an extension (e.g. .html, .php, .pdf).
    """
    parsed = urlparse(url)
    path = parsed.path.lstrip("/")  # OJO: no strip(), solo lstrip para conservar subcarpetas

    if "." in os.path.basename(path):
        # Si la √∫ltima parte tiene extensi√≥n, devolvemos toda la ruta relativa
        return path  # p.ej. wp-content/uploads/2023/12/GD_xxx.pdf

    if path:
        # No hay extensi√≥n expl√≠cita, asumimos HTML y mantenemos ruta
        return os.path.join(path, "index.html") if path.endswith("/") else path + ".html"

    # Ra√≠z del sitio ‚Üí nombre por defecto
    return ""


def generate_hash_filename(url: str, ext: str = ".html") -> str:
    """
    Generates filename using SHA-1 hash of the URL + datetime, with the given extension.
    """
    sha1 = hashlib.sha1(url.encode()).hexdigest()[:12]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if not ext.startswith("."):
        ext = "." + ext
    return f"{sha1}_{timestamp}{ext}"


def get_output_path_for_url(url: str, content_type: str | None = None) -> str:
    """
    Decide the filename based on the URL and content type.
    - If URL already has a filename/path, use it.
    - If not, use hash + datetime with appropriate extension.
    """
    filename = get_filename_from_url(url)

    if not filename:
        # Decide extension based on content-type
        ext = ".html"
        if content_type:
            ct = content_type.lower()
            if "pdf" in ct:
                ext = ".pdf"
        filename = generate_hash_filename(url, ext=ext)

    # Ruta absoluta final
    full_path = os.path.join(OUTPUT_FOLDER, filename)

    # Asegurar que existe el directorio padre (paginas_html + posibles subcarpetas)
    parent_dir = os.path.dirname(full_path)
    if parent_dir:
        os.makedirs(parent_dir, exist_ok=True)

    return full_path


# ---------- Helpers for normalization / filtering ----------

def normalize_url(url: str) -> str:
    parsed = urlparse(url)
    # Remove fragment (#...)
    parsed = parsed._replace(fragment="")
    return urlunparse(parsed)


def same_domain(url1: str, url2: str) -> bool:
    n1 = urlparse(url1).netloc.lower()
    n2 = urlparse(url2).netloc.lower()

    if n1.startswith("www."):
        n1 = n1[4:]
    if n2.startswith("www."):
        n2 = n2[4:]

    return n1 == n2


def is_interesting_link(href: str) -> bool:
    if not href:
        return False
    href = href.strip()
    if href.startswith("#"):
        return False
    if href.startswith("mailto:"):
        return False
    if href.startswith("tel:"):
        return False
    if href.lower().startswith("javascript:"):
        return False
    return True


# ---------- Main crawler ----------

def crawl(start_url: str):
    visited = set()
    queue = deque([start_url])

    while queue and len(visited) < MAX_PAGES:
        current_url = queue.popleft()
        current_url = normalize_url(current_url)

        if current_url in visited:
            continue

        print(f"[{len(visited)+1}] Fetching: {current_url}")
        visited.add(current_url)

        try:
            response = requests.get(current_url, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"  Error fetching {current_url}: {e}")
            continue

        # Respect delay between requests
        print(f"  Waiting {REQUEST_DELAY_SECONDS} seconds...")
        time.sleep(REQUEST_DELAY_SECONDS)

        content_type = response.headers.get("Content-Type", "")
        ct_lower = content_type.lower()

        # -------- Save PDF files --------
        if "application/pdf" in ct_lower or current_url.lower().endswith(".pdf"):
            filepath = get_output_path_for_url(current_url, content_type=content_type)
            try:
                with open(filepath, "wb") as f:
                    f.write(response.content)
                print(f"  [PDF] Saved to: {filepath}")
            except OSError as e:
                print(f"  Error saving PDF {filepath}: {e}")
            # No parse links inside PDF
            continue

        # -------- Process HTML pages --------
        if "text/html" in ct_lower:
            html_content = response.text

            filepath = get_output_path_for_url(current_url, content_type=content_type)
            try:
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"  [HTML] Saved to: {filepath}")
            except OSError as e:
                print(f"  Error saving HTML {filepath}: {e}")
                continue

            # Parse links and enqueue same-domain URLs
            soup = BeautifulSoup(html_content, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"]
                if not is_interesting_link(href):
                    continue

                new_url = urljoin(current_url, href)
                new_url = normalize_url(new_url)

                if same_domain(start_url, new_url) and new_url not in visited:
                    queue.append(new_url)

        else:
            # Non-HTML, non-PDF content (images, css, etc.) ‚Üí ignore
            print(f"  Skipping content type: {content_type}")

    print("Crawl finished.")


if __name__ == "__main__":
    crawl(START_URL)
```

### entrenar IA con todos los materiales

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de HTML, PDF, texto plano o Markdown.

- Recorre recursivamente todos los ficheros de INPUT_DIR con extensi√≥n:
    .html, .htm, .pdf, .txt, .md
- Para HTML:
    * Lee el archivo.
    * Limpia el markup (quita <script>, <style>, etc.).
    * Extrae el texto relevante.
- Para PDF:
    * Extrae el texto con PyPDF2.
- Para .txt / .md:
    * Igual que antes: limpia Markdown si procede.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.
- Muestra una barra de progreso global con:
    * porcentaje completado,
    * tiempo transcurrido,
    * tiempo estimado restante (ETA).
"""

import os
import re
import json
import time
import shutil
import requests
from typing import List, Dict, Optional

from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpeta de entrada: aqu√≠ pondr√°s tu 'paginas_html'
INPUT_DIR = "paginas_html"

# Carpeta de salida (JSONL)
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados

# Extensiones aceptadas
VALID_EXTS = (".html", ".htm", ".pdf", ".txt", ".md")


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.
    (Igual que en tu script original)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text] if text else []

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# EXTRACCI√ìN DESDE HTML Y PDF
# =========================

def extract_text_from_html(path: str) -> str:
    """
    Extrae texto "importante" de un HTML:
    - Elimina <script>, <style>, <noscript>.
    - Se queda con encabezados, p√°rrafos, listas, etc.
    - Devuelve el texto plano concatenado.
    """
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            html = f.read()
    except Exception as e:
        print(f"[WARN] No se pudo leer HTML {path}: {e}")
        return ""

    soup = BeautifulSoup(html, "html.parser")

    # Eliminar ruido
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # Tambi√©n puedes eliminar nav/footer si quieres:
    for tag in soup(["nav", "footer"]):
        tag.decompose()

    text = soup.get_text(separator=" ", strip=True)
    return text


def extract_text_from_pdf(path: str) -> str:
    """
    Extrae texto de un PDF usando PyPDF2.
    """
    try:
        reader = PdfReader(path)
    except Exception as e:
        print(f"[WARN] No se pudo abrir PDF {path}: {e}")
        return ""

    pages_text = []
    for i, page in enumerate(reader.pages):
        try:
            t = page.extract_text()
        except Exception as e:
            print(f"[WARN] Error extrayendo texto de la p√°gina {i} en {path}: {e}")
            t = None
        if t:
            pages_text.append(t)

    return "\n".join(pages_text)


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "paginas_html/index.html",
            "paginas_html/wp-content/...",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# TRACKER DE PROGRESO
# =========================

class ProgressTracker:
    """
    Barra de progreso global basada en n√∫mero total de bloques.
    Muestra:
    - porcentaje completado
    - tiempo transcurrido
    - ETA estimada
    """

    def __init__(self, total_units: int):
        self.total = max(1, total_units)
        self.current = 0
        self.start_time = time.time()

    @staticmethod
    def _format_seconds(secs: float) -> str:
        secs = int(secs)
        h = secs // 3600
        m = (secs % 3600) // 60
        s = secs % 60
        if h > 0:
            return f"{h:02d}:{m:02d}:{s:02d}"
        return f"{m:02d}:{s:02d}"

    def update(self, step: int = 1, prefix: str = ""):
        self.current += step
        if self.current > self.total:
            self.current = self.total

        elapsed = time.time() - self.start_time
        percent = (self.current / self.total) * 100.0

        if self.current > 0:
            rate = elapsed / self.current
            remaining = rate * (self.total - self.current)
        else:
            remaining = 0.0

        try:
            term_width = shutil.get_terminal_size((80, 20)).columns
        except Exception:
            term_width = 80

        bar_len = max(10, term_width - 55)
        filled = int(bar_len * self.current / self.total)
        bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)

        msg = (
            f"{prefix} [{bar}] {percent:6.2f}% "
            f"| t+{self._format_seconds(elapsed)} "
            f"| ETA {self._format_seconds(remaining)}"
        )

        msg = msg[:term_width - 1]
        print("\r" + msg, end="", flush=True)

    def finish(self, prefix: str = ""):
        self.update(step=0, prefix=prefix)
        print()  # salto de l√≠nea final


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de texto t√©cnico o formativo en espa√±ol
  (puede venir originalmente de HTML o PDF).
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# PREPARACI√ìN DE CADA FICHERO (LECTURA + TROCEO)
# =========================

def prepare_blocks_for_file(path: str) -> List[str]:
    """
    Lee el fichero seg√∫n su extensi√≥n, extrae texto √∫til y lo trocea en bloques.
    """
    print(f"\n[INFO] Preparando archivo: {path}")
    ext = os.path.splitext(path)[1].lower()

    raw_text = ""

    if ext in (".txt", ".md"):
        raw_text = read_text_file(path)
        if ext == ".md":
            raw_text = strip_markdown(raw_text)
    elif ext in (".html", ".htm"):
        raw_text = extract_text_from_html(path)
    elif ext == ".pdf":
        raw_text = extract_text_from_pdf(path)
    else:
        print(f"[WARN] Extensi√≥n no soportada (deber√≠a haberse filtrado antes): {ext}")
        return []

    if not raw_text or not raw_text.strip():
        print("[WARN] Archivo vac√≠o o sin texto √∫til, se ignorar√° (0 bloques).")
        return []

    text = normalize_whitespace(raw_text)

    if not text.strip():
        print("[WARN] Tras normalizar espacios el archivo qued√≥ vac√≠o, se ignorar√° (0 bloques).")
        return []

    blocks = split_into_blocks(text)
    print(f"[INFO]   -> {len(blocks)} bloques detectados.")
    return blocks


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str,
                        output_path: str,
                        blocks: List[str],
                        tracker: Optional[ProgressTracker],
                        file_index: int,
                        total_files: int) -> int:
    print(f"\n[INFO] Procesando archivo {file_index}/{total_files}: {path}")

    if not blocks:
        with open(output_path, "w", encoding="utf-8"):
            pass
        print("[INFO] Archivo sin bloques, JSONL vac√≠o generado.")
        return 0

    with open(output_path, "w", encoding="utf-8"):
        pass

    total_pairs_for_file = 0

    for block in blocks:
        block_pairs = generate_qa_for_block(block)

        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

        if tracker is not None:
            tracker.update(
                step=1,
                prefix=f"[PROGRESO] Archivo {file_index}/{total_files}"
            )

    print(f"\n[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    print(f"[INFO] JSONL generado para {path}: {output_path}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    # Recorre INPUT_DIR de forma recursiva
    input_files = []
    for root, _, files in os.walk(INPUT_DIR):
        for fn in files:
            if fn.lower().endswith(VALID_EXTS):
                full_path = os.path.join(root, fn)
                input_files.append(full_path)

    if not input_files:
        print(f"[INFO] No se han encontrado ficheros v√°lidos en {INPUT_DIR}.")
        print("      Extensiones admitidas:", ", ".join(VALID_EXTS))
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    pending_files = [p for p in sorted(input_files) if p not in processed_files]

    if not pending_files:
        print("[INFO] Todos los archivos presentes ya estaban procesados seg√∫n log.json.")
        print(f"[INFO] Log de materiales procesados: {LOG_FILE}")
        return

    # Primera pasada: preparar bloques por fichero y contar bloques totales
    print("\n[INFO] Calculando n√∫mero total de bloques para la barra de progreso global...")
    file_blocks_map: Dict[str, List[str]] = {}
    total_blocks = 0

    for path in pending_files:
        blocks = prepare_blocks_for_file(path)
        file_blocks_map[path] = blocks
        total_blocks += len(blocks)

    if total_blocks == 0:
        print("[WARN] No se han encontrado bloques de texto √∫tiles en los ficheros pendientes.")
        print("       Se actualizar√° el log, pero no se generar√°n Q/A.")
        newly_processed_count = 0
        for path in pending_files:
            base_name = os.path.splitext(os.path.basename(path))[0]
            per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")
            with open(per_file_output, "w", encoding="utf-8"):
                pass
            log.setdefault("processed_files", []).append(path)
            save_log(log)
            newly_processed_count += 1

        print("\n[RESUMEN]")
        print(f"Archivos encontrados              : {len(input_files)}")
        print(f"Archivos ya procesados (skip)     : {len(input_files) - len(pending_files)}")
        print(f"Archivos procesados en esta run   : {newly_processed_count}")
        print(f"Pares Q/A generados en esta run   : 0")
        print(f"Log de materiales procesados      : {LOG_FILE}")
        print(f"JSONL individuales en             : {OUTPUT_DIR}")
        return

    tracker = ProgressTracker(total_blocks)

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = len(input_files) - len(pending_files)

    print(f"[INFO] Total de bloques a procesar: {total_blocks}")
    print("[INFO] Iniciando generaci√≥n de Q/A con barra de progreso global...\n")

    for idx, path in enumerate(pending_files, start=1):
        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        blocks = file_blocks_map.get(path, [])
        pairs_count = process_single_file(
            path=path,
            output_path=per_file_output,
            blocks=blocks,
            tracker=tracker,
            file_index=idx,
            total_files=len(pending_files)
        )
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        newly_processed_count += 1

    tracker.finish(prefix="[PROGRESO]")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

<a id="interfaz-natural-ia"></a>
## interfaz natural IA


<a id="actividad-libre-de-final-de-evaluacion-la-milla-extra"></a>
# Actividad libre de final de evaluaci√≥n - La milla extra

<a id="la-milla-extra-primera-evaluacion"></a>
## La Milla Extra - Primera evaluaci√≥n

### ejercicio

```markdown

```
