# Proyecto Intermodular II

**Author:** Jose Vicente Carratala Sanchis

## Table of contents

- [Introduccion](#introduccion)
- [An√°lisis](#analisis)
  - [Recopilaci√≥n de informaci√≥n](#recopilacion-de-informacion)
  - [Identificaci√≥n y priorizaci√≥n de necesidades.](#identificacion-y-priorizacion-de-necesidades)
  - [Identificaci√≥n de los aspectos que facilitan o dificultan el desarrollo de la posible intervenci√≥n](#identificacion-de-los-aspectos-que-facilitan-o-dificultan-el-desarrollo-de-la-posible-intervencion)
- [Dise√±o](#diseno)
  - [Definici√≥n o adaptaci√≥n de la intervencioÃÅn](#definicion-o-adaptacion-de-la-intervencion)
  - [Priorizaci√≥n y secuenciaci√≥n de las acciones.](#priorizacion-y-secuenciacion-de-las-acciones)
  - [La planificaci√≥n de la intervenci√≥n](#la-planificacion-de-la-intervencion)
  - [Determinaci√≥n de recursos.](#determinacion-de-recursos)
  - [Planificaci√≥n de la evaluacioÃÅn](#planificacion-de-la-evaluacion)
  - [Dise√±o de documentaci√≥n](#diseno-de-documentacion)
  - [Plan de atenci√≥n al cliente](#plan-de-atencion-al-cliente)
- [Organizacion](#organizacion)
  - [Detecci√≥n de demandas y necesidades](#deteccion-de-demandas-y-necesidades)
  - [Programaci√≥n](#programacion)
  - [Gesti√≥n](#gestion)
  - [Coordinaci√≥n y supervisi√≥n de la intervenci√≥n](#coordinacion-y-supervision-de-la-intervencion)
  - [Elaboraci√≥n de informes](#elaboracion-de-informes)
  - [Seguimiento y control](#seguimiento-y-control)
- [Actividades profesionales](#actividades-profesionales)
  - [√Åreas de sistemas y departamentos de inform√°tica en cualquier sector de actividad](#areas-de-sistemas-y-departamentos-de-informatica-en-cualquier-sector-de-actividad)
  - [Sector de servicios tecnol√≥gicos y comunicaciones](#sector-de-servicios-tecnologicos-y-comunicaciones)
  - [√Årea comercial con gesti√≥n de transacciones por Internet](#area-comercial-con-gestion-de-transacciones-por-internet)
- [Lineas de actuaci√≥n](#lineas-de-actuacion)
  - [La ejecuci√≥n de trabajos en equipo](#la-ejecucion-de-trabajos-en-equipo)
  - [La autoevaluaci√≥n del trabajo realizado](#la-autoevaluacion-del-trabajo-realizado)
  - [La autonom√≠a y la iniciativa](#la-autonomia-y-la-iniciativa)
  - [El uso de las TIC](#el-uso-de-las-tic)
- [Bibliograf√≠a](#bibliografia)
- [Proyectos](#proyectos)
  - [empresarialIA](#empresarialia)
  - [agente virtual](#agente-virtual)
  - [Ejercicio de final de unidad](#ejercicio-de-final-de-unidad)
  - [Examen final](#examen-final)
- [Proyectos segundo trimestre](#proyectos-segundo-trimestre)
  - [Extractor transcripciones](#extractor-transcripciones)
  - [generador web IA](#generador-web-ia)
  - [Entrenamiento IA](#entrenamiento-ia)
  - [entrenar chatbot a partir de whatsapp](#entrenar-chatbot-a-partir-de-whatsapp)
  - [entrenar chatbot a partir de pdf](#entrenar-chatbot-a-partir-de-pdf)
  - [scrapeador web y entrenamiento](#scrapeador-web-y-entrenamiento)
  - [interfaz natural IA](#interfaz-natural-ia)
- [Actividad libre de final de evaluaci√≥n - La milla extra](#actividad-libre-de-final-de-evaluacion-la-milla-extra)
  - [La Milla Extra - Primera evaluaci√≥n](#la-milla-extra-primera-evaluacion)

---

<a id="introduccion"></a>
# Introduccion


<a id="analisis"></a>
# An√°lisis

<a id="recopilacion-de-informacion"></a>
## Recopilaci√≥n de informaci√≥n

### Introducci√≥n a los ejercicios

Basado en los documentos proporcionados, puedo ofrecer un an√°lisis del entorno empresarial valenciano y c√≥mo una empresa de inteligencia artificial podr√≠a operar dentro de este contexto.

### Panorama General del Tejido Empresarial en la Comunitat Valenciana

La regi√≥n es principalmente dominada por pymes (peque√±as y medianas empresas) y microempresas. Estos negocios son √°giles pero con limitados recursos para innovaci√≥n tecnol√≥gica debido a su tama√±o. La mayor parte de estas empresas opera tradicionalmente en sectores como la industria manufacturera, comercio al por menor, servicios y turismo.

### Estructura Organizativa T√≠pica

- **Microempresas y peque√±as empresas**: Son operados com√∫nmente por propietarios o gerentes con un peque√±o equipo de administraci√≥n t√©cnica. La toma de decisiones es centralizada y no formalizada, y la digitalizaci√≥n depende en gran medida del asesoramiento externo.
  
- **Pymes Medianas (50‚Äì250 trabajadores)**: Estos negocios suelen tener estructuras m√°s definidas con √°reas separadas como producci√≥n, comercial y administraci√≥n/finanzas. Podr√≠an contar con un responsable de TIC o sistemas pero la integraci√≥n de la inteligencia artificial a√∫n est√° en pa√±ales.
  
- **Grandes Empresas**: Son menos comunes pero tienen un gran impacto debido a su compleja estructura jer√°rquica y capacidad para proyectos personalizados.

### Particularidades de las Empresas de IA Valencianas

Las empresas de inteligencia artificial nacen como spin-offs universitarios o startups con equipos muy especializados, dependiendo inicialmente del asesoramiento externo y financiaci√≥n. Si son exitosas, pueden convertirse en scale-ups con estructuras definidas y roles jer√°rquicos m√°s claros.

### An√°lisis de la Empresa de IA Valenciana

Dado el panorama empresarial valenciano, una empresa de inteligencia artificial local tendr√≠a como mercado principal a las pymes locales que necesitan soluciones tecnol√≥gicas empaquetadas y accesibles. Adem√°s, existen oportunidades significativas para alianzas con grandes compa√±√≠as valencianas.

#### Implicaciones Operativas

- **Mercado Principal**: Ofrecer soluciones de inteligencia artificial empaquetadas y asequibles para pymes locales.
  
- **Alianzas Corporativas**: Colaborar con empresas grandes para proyectos piloto que generen un impacto significativo. Estos pueden ser proyectos de gemelos digitales, log√≠stica predictiva, mantenimiento preventivo, etc.
  
- **Crecimiento Org√°nico**: Iniciar como una empresa de consultor√≠a o servicios SaaS dirigidos a pymes y luego expandirse hacia clientes corporativos o incluso exportar soluciones al resto de Espa√±a/Europa.

#### Estructura Empresarial

1. **Etapa Semilla (3‚Äì10 personas)**: Conformada por un equipo mixto con perfiles en negocio y tecnolog√≠a, responsable t√©cnico del producto/modelos, data scientists/ingenieros ML, desarrolladores backend/frontend y roles de desarrollo comercial/marketing.
   
2. **Etapa Early Growth (10‚Äì30 personas)**: A√±ade niveles gerenciales como CEO, COO/CTO con equipos t√©cnicos divididos en IA/Data, software/producto, ventas y operaciones.
   
3. **Scale-up (30‚Äì100 personas)**: Definen roles de alto nivel (CEO, CTO, COO, CFO) con departamentos especializados en I+D, desarrollo de software, comercializaci√≥n y marketing.

En conclusi√≥n, una empresa de IA valenciana puede tener √©xito si enfoca sus esfuerzos en soluciones asequibles para pymes locales mientras construye alianzas estrat√©gicas con grandes corporaciones. La estructura organizativa debe adaptarse gradualmente seg√∫n crece la empresa, movi√©ndose desde operaciones muy horizontales hasta una jerarqu√≠a m√°s definida y profesionalizada.

### Actividades propuestas

Basado en los documentos proporcionados, se puede dise√±ar una estrategia para una empresa de inteligencia artificial (IA) emergente en Valencia que busque expandirse y obtener clientes tanto entre las peque√±as y medianas empresas (PYMEs) locales como entre las grandes corporaciones. La siguiente es una sugerencia estructurada bas√°ndose en el an√°lisis proporcionado:

### 1. Identificaci√≥n del Mercado Objetivo

**Microempresas y Pymes Peque√±as:**

- **Perfil:** Empresas familiares, industria manufacturera (cer√°mica, textil, calzado), comercio minorista.
- **Requisitos:** Soluciones tecnol√≥gicas simples, asequibles, que no requieren una gran inversi√≥n inicial. Servicios de asesoramiento y soporte t√©cnico para la implementaci√≥n y uso continuo de IA.
  
**Pymes Medianas:**

- **Perfil:** Empresas familiares consolidadas con departamentos administrativos separados.
- **Requisitos:** Soluciones m√°s avanzadas que incluyan el entrenamiento y la integraci√≥n en sistemas existentes. Necesidad de formaci√≥n continua para los empleados.

### 2. Estructura Organizativa Propuesta

Para abordar eficazmente estas necesidades, es importante estructurar nuestra empresa de IA con flexibilidad y escalabilidad:

- **Etapa Semilla (3‚Äì10 personas):**
  - CEO/Fundador: Con experiencia en tecnolog√≠a e inteligencia empresarial.
  - CTO/CTI: Lidera la investigaci√≥n y desarrollo del producto.
  - Data Scientists / ML Engineers: Responsables de crear modelos predictivos avanzados.
  - Desarrolladores Front-End y Back-End: Construyen la interfaz de usuario y la infraestructura backend.

- **Etapa Crecimiento Inicial (10‚Äì30 personas):**
  - A√±adir un COO para manejar operaciones, y definir roles m√°s especializados dentro del equipo t√©cnico.
  - Crear un departamento dedicado a ventas y alianzas estrat√©gicas para abordar el mercado de PYMES medianas.

- **Etapa de Escalamiento (30‚Äì100 personas):**
  - Establecer puestos directivos adicionales como CFO, CPO, etc., para gestionar los distintos departamentos.
  - Crear un departamento de soporte y operaciones para mantener el servicio al cliente y garantizar la continuidad operativa.

### 3. Ofertas y Servicios a Clientes

**Para Microempresas:**
- Soluciones de IA como SaaS con poco requerimiento inicial en t√©rminos de formaci√≥n y mantenimiento.
  
**Para Pymes Medianas:**
- Servicios de consultor√≠a y asesoramiento en la implementaci√≥n de soluciones de IA, que pueden ser personalizadas seg√∫n las necesidades espec√≠ficas del cliente.

**Para Grandes Empresas:**
- Proporcionar un marco completo para la innovaci√≥n basada en la IA, incluyendo la integraci√≥n con sistemas existentes y formaci√≥n continua a los empleados.

### 4. Estrategia de Marketing y Comunicaci√≥n

- **Identidad Corporativa:** Establecer una marca fuerte que refleje el compromiso con la innovaci√≥n y la accesibilidad.
- **Promoci√≥n en Medios Locales:** Colaborar con medios locales para aumentar la visibilidad del negocio entre PYMES y grandes corporaciones.
- **Participaci√≥n en Eventos de Negocio:** Asistir a conferencias, ferias y eventos empresariales para promover las soluciones de IA.

### 5. Desarrollo Tecnol√≥gico

- **Innovaci√≥n Continua:** Mantenerse al d√≠a con las √∫ltimas tendencias tecnol√≥gicas y aplicarlas en el desarrollo de nuevos productos o mejoras.
  
**Integraciones:** Trabajar en la creaci√≥n de APIs para permitir la f√°cil integraci√≥n de soluciones de IA con otros sistemas empresariales.

### 6. Alianzas Estrat√©gicas

- **Colaboraci√≥n con Universidades y Centros Tecnol√≥gicos:** Establecer relaciones s√≥lidas con instituciones educativas y de investigaci√≥n en Valencia, como ELLIS Alicante o la UPV.
  
**Partnerships B2B:** Buscar acuerdos estrat√©gicos con empresas l√≠deres en tecnolog√≠a para ampliar el alcance del negocio.

### 7. Evaluaci√≥n y Mejora Continua

- **Recopilaci√≥n de Feedback:** Solicitar regularmente a los clientes sus impresiones sobre la calidad del servicio recibido.
  
**An√°lisis de KPIs:** Monitorizar y analizar indicadores clave de rendimiento (KPIs) para medir el progreso hacia los objetivos establecidos.

Al adoptar esta estructura organizativa y estrat√©gica, nuestra empresa de IA estar√° bien posicionada para capturar el mercado tanto entre las PYMEs locales como entre las grandes corporaciones en Valencia.


<a id="identificacion-y-priorizacion-de-necesidades"></a>
## Identificaci√≥n y priorizaci√≥n de necesidades.

### Introducci√≥n a los ejercicios

Esta carpeta contiene ejercicios que exploran la identificaci√≥n y priorizaci√≥n de necesidades para el desarrollo de productos de Inteligencia Artificial (IA) orientados a peque√±as y medianas empresas (Pymes) en Valencia. Los problemas abordados giran en torno al an√°lisis del mercado, la detecci√≥n de oportunidades de negocio basadas en IA, y la definici√≥n de proyectos tecnol√≥gicos que responden a las demandas espec√≠ficas de estas empresas. Estos ejercicios ayudar√°n a los estudiantes a comprender c√≥mo identificar necesidades empresariales reales, dise√±ar soluciones pr√°cticas y escalables, y estructurar modelos de negocio viables para la implementaci√≥n de tecnolog√≠as emergentes en entornos Pyme.

### Oportunidades de negocio previsibles en el sector
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este documento presenta dos propuestas de productos m√≠nimo viables (MVP) orientados a peque√±as y medianas empresas (pymes) en Valencia. La primera opci√≥n es un clasificador autom√°tico de facturas y documentos que utiliza tecnolog√≠as como el reconocimiento √≥ptico de caracteres (OCR) e inteligencia artificial para extraer informaci√≥n relevante de los documentos digitales o im√°genes, facilitando la gesti√≥n administrativa y contable para las pymes.

La segunda propuesta es un chatbot multiling√ºe dise√±ado para ofrecer atenci√≥n al cliente en varios idiomas, incluyendo valenciano. Este chatbot automatiza respuestas a preguntas frecuentes de los clientes, lo que libera tiempo a los empleados y mejora la experiencia del cliente con respuestas inmediatas.

Ambas propuestas vienen acompa√±adas de modelos de negocio detallados, indicando posibles planes de suscripci√≥n y opciones de pago. Tambi√©n se discute c√≥mo cada producto podr√≠a escalar en el futuro, incluyendo la integraci√≥n con sistemas ya existentes como software de gesti√≥n empresarial (ERP) o plataformas de comercio electr√≥nico.

Adem√°s, el texto sugiere varias formas de diferenciar el chatbot para que sea m√°s atractivo y √∫til para las pymes valencianas, incluyendo su integraci√≥n directa con WhatsApp Business y la capacidad de realizar acciones concretas como confirmar reservas o generar presupuestos. Tambi√©n se menciona una alternativa al chatbot: un dashboard de ventas inteligente que proporciona informaci√≥n anal√≠tica sencilla a las pymes para mejorar sus decisiones comerciales.

Finalmente, el texto propone crear una matriz comparativa entre los dos productos (chatbot y dashboard) para evaluar aspectos como la inversi√≥n inicial necesaria, la facilidad de venta en Valencia, el tiempo necesario para ver un retorno sobre la inversi√≥n (ROI), y las posibilidades de escalabilidad.

`002-Oportunidades de negocio previsibles en el sector`

```
IA

Big Data

Robotica

Comp Cuantica

Meta ha presentado sus gafas

Ideas generadas:

Un **buen producto para empezar como MVP en Valencia** ser√≠a:

## üì¶ MVP: Clasificador autom√°tico de facturas y tickets (document AI sencillo)

### ‚úÖ Por qu√© este producto

* **Problema claro y universal**: todas las pymes manejan facturas, tickets y documentos contables.
* **Dolor inmediato**: pierden tiempo introduciendo datos a mano en ERP, facturaci√≥n o Excel.
* **Beneficio directo**: ahorro de horas administrativas, reducci√≥n de errores, cumplimiento con Hacienda.
* **Mercado amplio**: desde microempresas hasta asesor√≠as que gestionan decenas de clientes.
* **Tecnolog√≠a asequible**: no requiere entrenar un gran modelo desde cero, se puede montar con OCR + modelos de clasificaci√≥n/extracci√≥n + integraci√≥n sencilla.

### üöÄ C√≥mo plantearlo en versi√≥n m√≠nima viable

1. **Subida de documento (PDF, foto de ticket)** ‚Üí v√≠a web o email.
2. **OCR e IA** ‚Üí extrae: proveedor, fecha, base imponible, IVA, total.
3. **Exportaci√≥n** ‚Üí a CSV/Excel o integraci√≥n con software de facturaci√≥n b√°sico.
4. **Interfaz simple** ‚Üí el usuario valida y corrige si hace falta.

### üí∞ Modelo de negocio

* Suscripci√≥n mensual: **50‚Äì150 ‚Ç¨/mes** seg√∫n volumen de documentos.
* Opci√≥n de **pago por uso** para micropymes (ej. 0,10 ‚Ç¨ por factura procesada).
* Hosting en servidores europeos ‚Üí se cumple GDPR, algo muy sensible para asesor√≠as y pymes.

### üå± Escalabilidad futura

* M√≥dulos adicionales: gesti√≥n de contratos, clasificaci√≥n de correos, conciliaci√≥n bancaria.
* Integraciones con ERP/CRM locales (ej. Aitana, Sage, Holded, Odoo).
* Verticalizaci√≥n: adaptaciones para comercio, industria, asesor√≠as.

---

üëâ Con este MVP arrancas con **algo sencillo, necesario, r√°pido de probar y mostrar ROI inmediato**.
Despu√©s puedes abrir otros verticales (chatbots, visi√≥n artificial industrial, predicci√≥n de demanda).

¬øQuieres que te dise√±e un **prototipo funcional paso a paso** (stack t√©cnico, arquitectura y demo m√≠nima en Python/Flask o Node) para que lo uses con empresas piloto en Valencia?

-----------------------------------

Otro proyecto:

Perfecto üöÄ, aqu√≠ va otra idea distinta de MVP para pymes en Valencia:

---

## ü§ñ MVP: Chatbot multiling√ºe de atenci√≥n al cliente (espa√±ol/valenciano/ingl√©s)

### ‚úÖ Por qu√© este producto

* **Dolor inmediato**: muchas pymes (tiendas, turismo, restauraci√≥n, academias) reciben decenas de consultas repetitivas (horarios, reservas, precios, disponibilidad).
* **Beneficio directo**: ahorran tiempo del personal, dan respuesta inmediata 24/7 y mejoran la experiencia del cliente.
* **Diferenciaci√≥n local**: soporte en **valenciano** y adaptaci√≥n cultural ‚Üí ventaja frente a chatbots gen√©ricos de grandes plataformas.

### üöÄ C√≥mo plantearlo en versi√≥n m√≠nima viable

1. **Widget web + WhatsApp/Telegram** ‚Üí integraci√≥n r√°pida en la web de la pyme o en su canal de mensajer√≠a.
2. **FAQ b√°sicas cargadas en el sistema** ‚Üí horarios, direcciones, precios, productos, reservas.
3. **IA ligera** para responder variaciones ‚Üí con un modelo preentrenado (no hace falta entrenar desde cero).
4. **Transferencia a humano** ‚Üí si la pregunta no se entiende, el chatbot env√≠a la consulta al email/WhatsApp del negocio.

### üí∞ Modelo de negocio

* Suscripci√≥n mensual: **30‚Äì100 ‚Ç¨/mes** por pyme seg√∫n n√∫mero de interacciones.
* Valor a√±adido: personalizaci√≥n con logo, colores, tono de comunicaci√≥n en valenciano/castellano.
* Coste muy bajo para arrancar: un solo servidor puede dar servicio a muchas pymes.

### üå± Escalabilidad futura

* Conectar con reservas online (hosteler√≠a, academias, turismo).
* Integrar con CRM/ERP (ej. registrar leads, abrir tickets).
* A√±adir anal√≠ticas: qu√© preguntan m√°s los clientes, picos de demanda, idiomas m√°s usados.

---

üëâ Este MVP es ideal porque combina **impacto r√°pido, bajo coste de implantaci√≥n y atractivo para comercio/turismo valenciano**, que son sectores muy potentes en la regi√≥n.

¬øQuieres que te prepare una **comparativa r√°pida** entre este chatbot y el de facturas (coste de desarrollo, rapidez en conseguir clientes, potencial de escalado) para decidir cu√°l atacar primero?


sobre la diferenciaci√≥n:

Muy buena observaci√≥n üëå. Lo de ‚Äúmultiling√ºe valenciano‚Äù es un **plus simp√°tico**, pero no es un **driver de compra fuerte** para la pyme. Si quieres que este chatbot tenga **peso real en el mercado local**, la diferenciaci√≥n debe atacar **dolores que de verdad importan a las empresas**: tiempo, dinero, clientes.

---

## üîë Diferenciaciones m√°s potentes para el chatbot

1. **Integraci√≥n directa con WhatsApp Business**

   * En Valencia (y en toda Espa√±a), WhatsApp es el **canal de comunicaci√≥n N¬∫1** de las pymes con clientes.
   * Si el chatbot funciona **dentro de WhatsApp**, sin obligar a instalar nada m√°s ‚Üí √©xito seguro.
   * Ejemplo: restaurantes, talleres, academias ‚Üí reservas, citas y consultas llegan ya por WhatsApp.

2. **Conexi√≥n con herramientas que ya usan**

   * Exportar leads a Excel, enviar facturas, registrar citas en Google Calendar o directamente en el ERP.
   * Si ahorras trabajo administrativo **adem√°s de contestar**, pasas de ‚Äúgadget‚Äù a ‚Äúherramienta necesaria‚Äù.

3. **Chatbot con ventas integradas**

   * Que no solo responda, sino que **concrete una acci√≥n**:

     * Enviar link de pago.
     * Confirmar reserva.
     * Generar presupuesto b√°sico.
   * Eso s√≠ impacta en ingresos, y por tanto justifica el gasto.

4. **Anal√≠tica de clientes**

   * Panel donde la pyme vea qu√© preguntan m√°s los clientes, a qu√© horas, en qu√© idioma.
   * Informaci√≥n pr√°ctica para mejorar negocio (ej: ‚Äúla mayor√≠a preguntan por producto X que no tenemos en web‚Äù).

---

## üí° Alternativa de proyecto (si quieres pivotar)

Otro MVP muy viable ser√≠a:

### üìä Dashboard de ventas inteligente para pymes

* Se conecta a su facturaci√≥n (muchas usan programas sencillos o incluso Excel).
* Les da **gr√°ficas simples pero √∫tiles**:

  * Qu√© productos se venden m√°s.
  * Qu√© clientes compran m√°s.
  * Predicci√≥n de demanda (b√°sica, con hist√≥rico de ventas).
* Diferencia: la mayor√≠a de pymes no tienen BI ‚Üí un ‚ÄúPower BI a medida y f√°cil‚Äù para micronegocios puede ser brutal.

---

üëâ En resumen:

* El **chatbot** es muy viable si lo diferencias con **WhatsApp + acciones concretas + anal√≠tica de clientes**.
* El **dashboard de ventas** puede ser incluso m√°s atractivo para empresarios cl√°sicos que **quieren ver n√∫meros claros y tomar decisiones**.

¬øQuieres que te arme una **matriz comparativa r√°pida (chatbot vs dashboard)** con: inversi√≥n inicial, facilidad de venta en Valencia, tiempo para ver ROI y escalabilidad?
```

### Actividades propuestas

### Actividad 1: Identificaci√≥n y Priorizaci√≥n de Necesidades Empresariales

**Descripci√≥n:** Los alumnos deben analizar el archivo "001-Necesidades m√° demandadas a las empresas.md" para identificar las necesidades clave que una empresa valenciana necesita cubrir al implementar productos basados en IA. Deben priorizar estas necesidades y explicar por qu√© son importantes para la empresa.

**Objetivo:** Aprender c√≥mo diagnosticar problemas empresariales y priorizar soluciones tecnol√≥gicas seg√∫n su impacto.

### Actividad 2: Dise√±o de una Soluci√≥n Empresarial

**Descripci√≥n:** Los estudiantes deben dise√±ar un producto de IA que resuelva uno o m√°s de los desaf√≠os identificados en el archivo "001-Necesidades m√° demandadas a las empresas.md". Deben describir brevemente c√≥mo funciona y cu√°les son sus beneficios para la empresa.

**Objetivo:** Entender el proceso de desarrollo de productos tecnol√≥gicos que responden a necesidades empresariales concretas.

### Actividad 3: An√°lisis de Casos de Uso

**Descripci√≥n:** Los alumnos deben analizar los ejemplos proporcionados en "002-Oportunidades de negocio previsibles en el sector" y crear un diagrama de casos de uso para uno de estos productos (chatbot, dashboard, etc.). Deben incluir los actores involucrados y las interacciones entre ellos.

**Objetivo:** Aprender a documentar y analizar la funcionalidad de sistemas inform√°ticos bas√°ndose en necesidades empresariales.

### Actividad 4: Dise√±o de Arquitectura Tecnol√≥gica

**Descripci√≥n:** Los estudiantes deben dise√±ar una arquitectura tecnol√≥gica b√°sica para uno de los productos descritos ("chatbot", "dashboard", etc.), utilizando Python y Flask o Node.js. Deben incluir diagramas que muestran las interacciones entre el front-end, back-end y API externa.

**Objetivo:** Aprender a dise√±ar sistemas inform√°ticos complejos bas√°ndose en necesidades empresariales espec√≠ficas.

### Actividad 5: Plan de Negocio Inicial

**Descripci√≥n:** Los alumnos deben crear un plan de negocio inicial para uno de los productos propuestos, incluyendo detalles sobre la inversi√≥n inicial, costos recurrentes, ingresos y el tiempo estimado para obtener beneficios significativos (ROI).

**Objetivo:** Entender c√≥mo se relacionan las soluciones tecnol√≥gicas con aspectos comerciales como marketing, ventas y finanzas.

### Actividad 6: Evaluaci√≥n de Escalabilidad

**Descripci√≥n:** Los estudiantes deben evaluar la escalabilidad del producto propuesto, considerando factores t√©cnicos (como el uso de servidores) y empresariales (como necesidades de personal comercial e inform√°tico).

**Objetivo:** Aprender a prever los desaf√≠os de crecimiento en proyectos tecnol√≥gicos y c√≥mo abordarlos desde una perspectiva empresarial. 

### Actividad 7: Presentaci√≥n Final del Proyecto

**Descripci√≥n:** Los alumnos deben preparar una presentaci√≥n final que incluya todos los aspectos anteriores (necesidades identificadas, soluci√≥n propuesta, diagramas de casos de uso, arquitectura tecnol√≥gica y plan de negocio) para un supuesto cliente empresarial.

**Objetivo:** Desarrollar habilidades de comunicaci√≥n efectiva al presentar proyectos complejos a clientes no t√©cnicos. 

Estas actividades pretenden cubrir desde la identificaci√≥n de necesidades hasta el desarrollo final del producto, pasando por aspectos tecnol√≥gicos y comerciales, para proporcionar una experiencia completa en el desarrollo de soluciones basadas en IA para empresas locales.


<a id="identificacion-de-los-aspectos-que-facilitan-o-dificultan-el-desarrollo-de-la-posible-intervencion"></a>
## Identificaci√≥n de los aspectos que facilitan o dificultan el desarrollo de la posible intervenci√≥n

### Introducci√≥n a los ejercicios

Entiendo tus preocupaciones sobre los riesgos potenciales asociados al desarrollo excesivo y la posible "burbuja" en torno a la Inteligencia Artificial (IA). Es cierto que existen paralelos con otros ciclos econ√≥micos pasados, como la burbuja de internet de 1999-2000. Vamos a analizar estos riesgos y c√≥mo manejarlos:

### Riesgo: Sobrevaloraci√≥n de empresas de IA

1. **Sobrevaluaci√≥n de Empresas**:
   - Muchas startups de IA han recibido financiamiento sin probar su viabilidad comercial.
   - Esto puede llevar a una sobreestimaci√≥n del valor real de estas empresas.

2. **Investigaci√≥n No Comercializable**:
   - Algunos proyectos de IA pueden ser avanzados tecnol√≥gicamente, pero no tienen aplicaciones comerciales inmediatas o viables.

### Riesgo: Explosi√≥n de la Burbuja

1. **Colapso Financiero**:
   - Un pinchazo en el mercado podr√≠a causar una crisis financiera similar a la que ocurri√≥ con la burbuja de internet.
   
2. **Valle del Silencio**:
   - Despu√©s de un pinchazo, puede surgir un "valle del silencio" donde las inversiones se vuelven m√°s cautelosas y el desarrollo tecnol√≥gico se ralentiza.

### Ejemplos Hist√≥ricos

1. **Burbuja.com (1999-2000)**:
   - Empresas como [Terra] fueron financiadas masivamente sin un plan claro de negocio sostenible.
   - La sobrevaloraci√≥n llev√≥ a la explosi√≥n y posterior recesi√≥n.

### Manejo del Riesgo

1. **Evaluaci√≥n de Viabilidad Comercial**:
   - Antes de invertir, evaluar cuidadosamente si el proyecto tiene un modelo de negocio viable en el corto y mediano plazo.
   
2. **Foco en Aplicaciones Pr√°cticas**:
   - Priorizar proyectos que tengan aplicaciones pr√°cticas y beneficios inmediatos.

3. **Diversificaci√≥n de Inversiones**:
   - Diversificar las inversiones para minimizar el riesgo de una √∫nica burbuja tecnol√≥gica.

4. **Estrategia a Largo Plazo**:
   - Pensar en estrategias que consideren tanto el corto como el largo plazo, no solo el r√°pido retorno financiero.

### Consideraciones Adicionales

1. **Transparencia y Rendici√≥n de Cuentas**:
   - Mejorar la transparencia en las operaciones y rendir cuentas sobre los avances tecnol√≥gicos.
   
2. **Regulaci√≥n y √âtica**:
   - Implementar medidas regulatorias y √©ticas para garantizar que el desarrollo de IA sea responsable y beneficioso a largo plazo.

3. **Cultura Empresarial Inclusiva**:
   - Fomentar una cultura empresarial inclusiva y colaborativa, en lugar de un enfoque competitivo excesivo.

### Conclusi√≥n

Aunque existen riesgos significativos asociados con la posible explosi√≥n de una burbuja financiera en torno a la IA, estos pueden mitigarse con estrategias prudentes. El objetivo es aprovechar el potencial de la tecnolog√≠a sin caer en los excesos del pasado.

Si tienes m√°s preguntas o necesitas un an√°lisis m√°s detallado de alguno de estos puntos, no dudes en dec√≠rmelo.

### Actividades propuestas

Entiendo tus preocupaciones sobre los riesgos asociados con una posible "burbuja financiera" en torno a la inteligencia artificial (IA). Vamos a estructurar esta informaci√≥n para proporcionar un contexto claro y analizar algunos aspectos clave:

### Riesgo de Expansi√≥n Excesiva

1. **Sobrevaloraci√≥n de Empresas de IA**:
   - Al igual que sucedi√≥ con las empresas relacionadas con Internet durante la burbuja del dot-com en 1999-2000, muchas startups y empresas de tecnolog√≠a se est√°n capitalizando a un ritmo muy r√°pido.
   - Esto puede llevar a una sobrevaloraci√≥n exagerada del potencial real de estas empresas.

2. **Financiamiento Excesivo**:
   - Se est√° invirtiendo dinero en proyectos que pueden no tener una base s√≥lida o que a√∫n est√°n en etapas inmaduras de desarrollo.
   - Esto puede llevar a un crecimiento r√°pido pero poco sostenible, similar al que se observ√≥ durante la burbuja del dot-com.

### Precedente: La Burbuja del Dot-Com

1. **Sobrevaloraci√≥n**:
   - Durante la burbuja del dot-com, las empresas relacionadas con Internet fueron financiadas en masa.
   - Se cre√≠a que cualquier proyecto basado en Internet ser√≠a exitoso sin importar su viabilidad real.

2. **Caso Terra**:
   - Un ejemplo notorio fue el caso de "Terra", una empresa que combin√≥ operaciones de medios tradicionales con sitios web.
   - A pesar de ser criticada por sus m√©todos de negocio poco claros, Terra obtuvo inversiones significativas antes del pinchazo.

### Consecuencias Post-Burbuja

1. **Valle de Precauci√≥n**:
   - Despu√©s del pinchazo de la burbuja del dot-com, se observ√≥ una fase prolongada de recesi√≥n en el sector tecnol√≥gico.
   - Durante este per√≠odo, las empresas y los inversores fueron m√°s cautelosos.

2. **Rebrote Sostenible**:
   - Con el tiempo, despu√©s del valle de precauci√≥n, la industria tecnol√≥gica se recuper√≥ pero con un crecimiento m√°s estable y sostenido.
   - Las empresas que sobrevivieron a la crisis se centraron en modelos comerciales s√≥lidos.

### Aplicaci√≥n al Sector de IA

1. **Riesgo de Pinchazo**:
   - El sector de IA est√° actualmente experimentando un crecimiento acelerado y una financiaci√≥n masiva.
   - Sin embargo, si no se gestiona adecuadamente, este r√°pido aumento puede llevar a una eventual ca√≠da repentina.

2. **Valle de Precauci√≥n en IA**:
   - Si la burbuja de IA explota, es probable que veamos un per√≠odo similar al valle de precauci√≥n.
   - Los inversores y las empresas ser√°n m√°s cautelosos con respecto a nuevas tecnolog√≠as emergentes.

3. **Crecimiento Sostenible Post-Pinchazo**:
   - Una vez superada esta fase inicial, el sector puede experimentar un crecimiento m√°s estable y sostenido.
   - Las empresas que sobreviven se centrar√°n en modelos comerciales s√≥lidos y aplicaciones pr√°cticas de IA.

### Mitigaci√≥n de Riesgos

1. **An√°lisis Financiero Riguroso**:
   - Inversores e inversores deben realizar an√°lisis financieros rigurosos antes de invertir.
   
2. **Modelo Comercial Claro**:
   - Las empresas necesitan tener modelos comerciales claros y viables que no dependan solo del hype tecnol√≥gico.

3. **√ânfasis en Aplicaciones Pr√°cticas**:
   - El foco debe estar en desarrollar aplicaciones pr√°cticas de IA con un valor real para los usuarios finales.
   
4. **Regulaci√≥n Transparente**:
   - Una regulaci√≥n transparente y efectiva puede ayudar a mitigar riesgos innecesarios.

### Conclusi√≥n

El sector de la inteligencia artificial est√° en una etapa similar a la burbuja del dot-com, con un crecimiento acelerado pero potencialmente excesivo. Sin embargo, siguiendo las lecciones aprendidas de anteriores burbujas, es posible mitigar estos riesgos y asegurar un futuro sostenible para la IA.

¬øQuieres profundizar en alg√∫n aspecto particular o necesitas m√°s detalles sobre c√≥mo prepararse para este tipo de escenario?



<a id="diseno"></a>
# Dise√±o

<a id="definicion-o-adaptacion-de-la-intervencion"></a>
## Definici√≥n o adaptaci√≥n de la intervencioÃÅn

### Introducci√≥n a los ejercicios

Este conjunto de ejercicios se centra en el dise√±o y planificaci√≥n de un proyecto intermodular relacionado con la implementaci√≥n de un sistema de inteligencia artificial (IA) para empresas en la zona de Valencia. El objetivo principal es crear un agente de IA que proporcione servicios personalizados de atenci√≥n al cliente, cumpliendo con normativas espec√≠ficas y promoviendo la mejora competitiva del tejido empresarial local. Los ejercicios abordar√°n aspectos como el cumplimiento legal, la √©tica en el uso de sistemas AI, as√≠ como la integraci√≥n tecnol√≥gica para ofrecer un servicio eficaz pero tambi√©n responsable. Se practican competencias clave como la comprensi√≥n y aplicaci√≥n de normativas, dise√±o de infraestructuras tecnol√≥gicas, y desarrollo web para interacciones con plataformas populares como WhatsApp.

### Actividades propuestas

1. **An√°lisis y resumen del caso**
   - Descripci√≥n: Los estudiantes deben leer atentamente el documento proporcionado y redactar un breve resumen que destaque los objetivos principales del proyecto, las caracter√≠sticas esenciales de la soluci√≥n propuesta y su impacto en el tejido empresarial valenciano. Este ejercicio ayudar√° a mejorar sus habilidades de comprensi√≥n lectora t√©cnica y s√≠ntesis.

2. **Identificaci√≥n de requerimientos**
   - Descripci√≥n: Los estudiantes deben identificar y listar los requisitos funcionales del sistema descrito, bas√°ndose en las descripciones proporcionadas en el documento. Esto incluir√° la detecci√≥n de qu√© funciones espec√≠ficas se necesitan para cumplir con los objetivos del proyecto.

3. **Dise√±o arquitect√≥nico**
   - Descripci√≥n: Proporciona una representaci√≥n visual del sistema descrito, incluyendo componentes como el servidor web, API de Whatsapp y modelos de IA. Este ejercicio permitir√° a los estudiantes comprender mejor la interacci√≥n entre los diferentes elementos del sistema.

4. **Entorno de desarrollo**
   - Descripci√≥n: Los alumnos deben investigar e identificar las herramientas necesarias para implementar un entorno de desarrollo que cumpla con las especificaciones geogr√°ficas y legales descritas en el proyecto, as√≠ como para permitir el entrenamiento del modelo de IA.

5. **Implementaci√≥n de modelo de IA**
   - Descripci√≥n: Se requiere que los estudiantes describan brevemente c√≥mo podr√≠an implementar un modelo de inteligencia artificial pre-entrenado y luego ajustarlo (fine-tuning) con datos espec√≠ficos para un cliente, en l√≠nea con la especificaci√≥n del proyecto.

6. **Pruebas unitarias**
   - Descripci√≥n: Los estudiantes deben dise√±ar pruebas unitarias que puedan utilizarse para verificar el funcionamiento correcto de las piezas clave del sistema propuesto (por ejemplo, el servidor web o la conexi√≥n a la API de Whatsapp).

7. **Documentaci√≥n t√©cnica**
   - Descripci√≥n: Redacta una gu√≠a r√°pida sobre c√≥mo configurar y ejecutar el sistema descrito en el documento proporcionado. Este ejercicio mejorar√° las habilidades de los estudiantes en cuanto a redacci√≥n clara y concisa.

8. **An√°lisis de impacto √©tico**
   - Descripci√≥n: Los alumnos deben investigar y discutir c√≥mo la implementaci√≥n del proyecto podr√≠a tener un impacto positivo o negativo desde una perspectiva √©tica, centr√°ndose en aspectos como la privacidad de los datos y el empleo local.

9. **Comunicaci√≥n con clientes**
   - Descripci√≥n: Escribir gu√≠as o manuales para ayudar a potenciales clientes a entender c√≥mo pueden beneficiarse del servicio proporcionado por el sistema descrito, enfoc√°ndose en los beneficios pr√°cticos y las ventajas sobre opciones tradicionales.

10. **Seguridad de la informaci√≥n**
    - Descripci√≥n: Identificar y discutir medidas clave para asegurar que la implementaci√≥n del sistema respete y cumpla con todos los requisitos normativos relacionados con el almacenamiento seguro de datos, considerando las especificaciones geogr√°ficas proporcionadas.


<a id="priorizacion-y-secuenciacion-de-las-acciones"></a>
## Priorizaci√≥n y secuenciaci√≥n de las acciones.

### Introducci√≥n a los ejercicios

El conjunto de ejercicios en esta carpeta se centra en el dise√±o y desarrollo de una aplicaci√≥n o servicio basado en inteligencia artificial, espec√≠ficamente usando motores de lenguaje grande (LLM). Los estudiantes trabajar√°n a trav√©s de varios pasos que incluyen el an√°lisis de diferentes modelos de LLM, su elecci√≥n, entrenamiento, comprobaci√≥n t√©cnica, instalaci√≥n y finalmente la creaci√≥n de un prototipo m√≠nimo viable (MVP) con una interfaz web. Este conjunto de ejercicios ayuda a los estudiantes a desarrollar competencias en dise√±o de sistemas, priorizaci√≥n de tareas, desarrollo backend y frontend, as√≠ como integraci√≥n con APIs externas.

### Actividades propuestas

### Actividad 1: An√°lisis y Documentaci√≥n de Motores LLM

**Descripci√≥n:** Los alumnos deben analizar el contenido proporcionado sobre diferentes motores de lenguaje generativo (LLM) y documentar los pasos necesarios para la descarga, preparaci√≥n de pruebas y evaluaci√≥n. Se espera que los estudiantes aprendan a organizar un proceso complejo en subpuntos claros y coherentes.

### Actividad 2: Desarrollo de Pruebas Automatizadas

**Descripci√≥n:** Los alumnos deben desarrollar una bater√≠a de tests automatizados para evaluar el rendimiento de los motores LLM descargados. El objetivo es que comprendan la importancia de las pruebas en el desarrollo y mantenimiento de software.

### Actividad 3: Dise√±o de Preguntas Tipo

**Descripci√≥n:** Los estudiantes deben dise√±ar un conjunto de preguntas tipo para evaluar las capacidades de los motores LLM. Este ejercicio permitir√° que los alumnos entiendan c√≥mo se estructuran pruebas efectivas y objetivas.

### Actividad 4: Contacto con Empresas

**Descripci√≥n:** Se les solicita a los estudiantes que busquen informaci√≥n sobre empresas potenciales para colaborar en la evaluaci√≥n de los motores LLM. A trav√©s de esta actividad, aprender√°n t√©cnicas b√°sicas de networking y contacto profesional.

### Actividad 5: Evaluaci√≥n de Resultados

**Descripci√≥n:** Los alumnos deben analizar los resultados obtenidos tras el lanzamiento de las pruebas automatizadas. El objetivo es que aprendan a interpretar datos y tomar decisiones basadas en la evaluaci√≥n de rendimiento del software.

### Actividad 6: Entrenamiento de Modelos

**Descripci√≥n:** Se les pide a los estudiantes que realicen un entrenamiento con el modelo de lenguaje seleccionado. Esta actividad ayudar√° a entender los procesos y conceptos b√°sicos de la formaci√≥n de modelos de inteligencia artificial.

### Actividad 7: Viabilidad T√©cnica

**Descripci√≥n:** Los alumnos deben evaluar si es t√©cnicamente viable implementar un motor LLM en el entorno establecido. Esta actividad permitir√° que comprendan los aspectos t√©cnicos y log√≠sticos clave en la implementaci√≥n de software.

### Actividad 8: Instalaci√≥n en Servidor

**Descripci√≥n:** Los estudiantes deben realizar una instalaci√≥n b√°sica del motor seleccionado en un servidor proporcionado. Este ejercicio familiarizar√° a los alumnos con las pr√°cticas comunes de administraci√≥n de sistemas y configuraci√≥n de servidores.

### Actividad 9: Desarrollo Web

**Descripci√≥n:** Se les pide que desarrollen un widget web para interactuar con el motor LLM instalado. A trav√©s de esta actividad, aprender√°n a crear interfaces de usuario simples y efectivas utilizando tecnolog√≠as web modernas.

### Actividad 10: Integraci√≥n con API

**Descripci√≥n:** Los alumnos deben integrar el motor LLM con la API de WhatsApp para permitir comunicaciones automatizadas. Esta tarea ayudar√° a los estudiantes a entender c√≥mo se integran diferentes servicios y APIs en aplicaciones pr√°cticas.


<a id="la-planificacion-de-la-intervencion"></a>
## La planificaci√≥n de la intervenci√≥n

### Introducci√≥n a los ejercicios

Este conjunto de ejercicios est√° dise√±ado para que comprendas los diferentes modelos de licencias en software y sus implicaciones, centr√°ndote especialmente en el modelo SaaS (Software as a Service) y AIaaS (Artificial Intelligence as a Service). Se enfatiza la importancia de elegir adecuadamente el hardware necesario para sostener estos servicios, teniendo en cuenta las necesidades espec√≠ficas del software que se va a implementar. A trav√©s de estos ejercicios, adquirir√°s competencias en la elecci√≥n y configuraci√≥n de servidores para proyectos tecnol√≥gicos, as√≠ como en la comprensi√≥n financiera asociada a los modelos de negocio basados en tecnolog√≠a.

### Actividades propuestas

### Actividad 1: Identificar Modelos de Licencias
**Descripci√≥n:** El alumnado debe identificar y clasificar diferentes tipos de licencias de software mencionadas en el archivo "001-Introduccion.md". Se espera que los estudiantes reconozcan las diferencias entre modelos como GPL, Software Privativo y SaaS.

### Actividad 2: An√°lisis Comparativo de Servicios
**Descripci√≥n:** Los alumnos deben comparar los costos y beneficios del modelo de licencia SaaS con respecto al software privativo bas√°ndose en la informaci√≥n proporcionada. El objetivo es entender las ventajas y desventajas de cada sistema.

### Actividad 3: Propuesta de Arquitectura de Servidor
**Descripci√≥n:** A partir de los requisitos descritos en "002-en mi caso concreto.md", el alumnado debe proponer una arquitectura de servidor adecuada para un proyecto SaaS. Se deben considerar tanto las necesidades b√°sicas como las espec√≠ficas para la implementaci√≥n de IA.

### Actividad 4: Planificaci√≥n Econ√≥mica
**Descripci√≥n:** Los estudiantes calculan y presentan una planificaci√≥n econ√≥mica basada en los costos proporcionados por cada tipo de servidor necesario para el proyecto SaaS. Se espera que incluyan detalles sobre costes iniciales, mensuales y de mantenimiento.

### Actividad 5: Evaluaci√≥n de Hardware
**Descripci√≥n:** A partir del hardware descrito en "002-en mi caso concreto.md", los alumnos deben evaluar su adecuaci√≥n para el proyecto SaaS e IA. Se les pide que propongan mejoras o ajustes necesarios.

### Actividad 6: Descripci√≥n de Requisitos T√©cnicos
**Descripci√≥n:** Los estudiantes elaboran un documento descriptivo de los requisitos t√©cnicos espec√≠ficos del servidor y la infraestructura necesaria para soportar el proyecto SaaS e IA. Se debe incluir informaci√≥n sobre software, hardware y servicios.

### Actividad 7: An√°lisis de Riesgos
**Descripci√≥n:** El alumnado identifica y documenta posibles riesgos relacionados con la implementaci√≥n del modelo SaaS y AIaaS para su proyecto orientado a centros educativos. Se espera una evaluaci√≥n detallada de cada riesgo propuesto.

### Actividad 8: Propuesta Comercial
**Descripci√≥n:** Los estudiantes deben dise√±ar una propuesta comercial b√°sica que explique c√≥mo el servicio SaaS con IA puede ser beneficioso para los centros educativos, incluyendo aspectos como la usabilidad y ventajas comparativas frente a otros modelos de software.

Estas actividades est√°n dise√±adas para reforzar conceptos clave relacionados con licencias de software, arquitectura de servidores, planificaci√≥n econ√≥mica y an√°lisis de riesgos en el contexto de proyectos SaaS e IA.


<a id="determinacion-de-recursos"></a>
## Determinaci√≥n de recursos.

### Introducci√≥n a los ejercicios

Este conjunto de ejercicios se centra en la determinaci√≥n y an√°lisis de recursos necesarios para el desarrollo y mantenimiento de un proyecto inform√°tico, principalmente enfocado en aplicaciones basadas en inteligencia artificial. Se trabajan aspectos como los costes materiales, humanos y econ√≥micos que implican diferentes escenarios de implementaci√≥n (desarrollo en casa vs alquiler de oficinas), as√≠ como la elecci√≥n entre comprar o alquilar hardware necesario para el proyecto. Los estudiantes practican habilidades de c√°lculo financiero, an√°lisis costo-beneficio y planificaci√≥n estrat√©gica de recursos t√©cnicos y humanos.

Los ejercicios abordan tambi√©n las decisiones sobre c√≥mo asegurar ingresos suficientes para equilibrar los costes iniciales mediante ventas de licencias o servicios. Esto ayuda a entender la importancia de una gesti√≥n efectiva de recursos en el lanzamiento y sostenibilidad a largo plazo de un proyecto tecnol√≥gico.

### Actividades propuestas

### Actividad 1: An√°lisis de Costos Iniciales
**Descripci√≥n:** Los estudiantes deben calcular los costes iniciales necesarios para desarrollar e implantar un proyecto inform√°tico basado en las cifras proporcionadas en los ejercicios. Se pretende que aprendan a realizar estimaciones financieras y entender la importancia de presupuestos precisos en proyectos tecnol√≥gicos.

### Actividad 2: Planificaci√≥n Econ√≥mica
**Descripci√≥n:** Los alumnos deben dise√±ar un plan econ√≥mico para el primer a√±o del proyecto, incluyendo c√°lculos de ingresos necesarios y estrategias para asegurar estos ingresos. Se espera que comprendan c√≥mo equilibrar gastos con expectativas de beneficio.

### Actividad 3: Comparativa Coste-Beneficio
**Descripci√≥n:** Los estudiantes deben realizar una comparaci√≥n entre comprar y alquilar el equipo necesario, bas√°ndose en los costes detallados proporcionados. Se espera que aprendan a valorar las ventajas y desventajas de cada opci√≥n desde una perspectiva econ√≥mica.

### Actividad 4: Estimaci√≥n Presupuestaria
**Descripci√≥n:** Los estudiantes deben estimar el presupuesto total para la adquisici√≥n, mantenimiento y operaci√≥n de un servidor dedicado bas√°ndose en los costes proporcionados. Se pretende que comprendan c√≥mo hacer c√°lculos precisos y planificar correctamente las finanzas.

### Actividad 5: An√°lisis de Recursos Humanos
**Descripci√≥n:** Los alumnos deben calcular el costo total mensual y anual del personal necesario para soportar un servicio de aplicaci√≥n inform√°tica, incluyendo turnos y salarios. Se espera que entiendan la importancia del coste laboral en proyectos tecnol√≥gicos.

### Actividad 6: Evaluaci√≥n de Servidores
**Descripci√≥n:** Los estudiantes deben evaluar las especificaciones y costes de diferentes servidores necesarios para su proyecto, incluyendo tanto la compra como el alquiler. Se pretende que aprendan a seleccionar equipos seg√∫n sus necesidades espec√≠ficas.

### Actividad 7: Planificaci√≥n del Entrenamiento
**Descripci√≥n:** Los alumnos deben planificar las horas y costes de entrenamiento de IA para su proyecto, bas√°ndose en los detalles proporcionados en el ejercicio. Se espera que comprendan la importancia de este aspecto en proyectos tecnol√≥gicos.

### Actividad 8: Gesti√≥n Financiera del Proyecto
**Descripci√≥n:** Los estudiantes deben dise√±ar un plan financiero completo para un proyecto inform√°tico, incluyendo c√°lculos de costos iniciales y estrategias de ingresos. Se pretende que aprendan a gestionar los aspectos financieros de un proyecto desde el inicio hasta la fase final.

Estas actividades est√°n dise√±adas para que los estudiantes apliquen los conocimientos te√≥ricos adquiridos en su ciclo formativo a situaciones pr√°cticas, mejorando as√≠ sus habilidades de planificaci√≥n y gesti√≥n financiera en proyectos tecnol√≥gicos.


<a id="planificacion-de-la-evaluacion"></a>
## Planificaci√≥n de la evaluacioÃÅn


<a id="diseno-de-documentacion"></a>
## Dise√±o de documentaci√≥n


<a id="plan-de-atencion-al-cliente"></a>
## Plan de atenci√≥n al cliente



<a id="organizacion"></a>
# Organizacion

<a id="deteccion-de-demandas-y-necesidades"></a>
## Detecci√≥n de demandas y necesidades


<a id="programacion"></a>
## Programaci√≥n


<a id="gestion"></a>
## Gesti√≥n


<a id="coordinacion-y-supervision-de-la-intervencion"></a>
## Coordinaci√≥n y supervisi√≥n de la intervenci√≥n


<a id="elaboracion-de-informes"></a>
## Elaboraci√≥n de informes


<a id="seguimiento-y-control"></a>
## Seguimiento y control



<a id="actividades-profesionales"></a>
# Actividades profesionales

<a id="areas-de-sistemas-y-departamentos-de-informatica-en-cualquier-sector-de-actividad"></a>
## √Åreas de sistemas y departamentos de inform√°tica en cualquier sector de actividad


<a id="sector-de-servicios-tecnologicos-y-comunicaciones"></a>
## Sector de servicios tecnol√≥gicos y comunicaciones


<a id="area-comercial-con-gestion-de-transacciones-por-internet"></a>
## √Årea comercial con gesti√≥n de transacciones por Internet



<a id="lineas-de-actuacion"></a>
# Lineas de actuaci√≥n

<a id="la-ejecucion-de-trabajos-en-equipo"></a>
## La ejecuci√≥n de trabajos en equipo


<a id="la-autoevaluacion-del-trabajo-realizado"></a>
## La autoevaluaci√≥n del trabajo realizado


<a id="la-autonomia-y-la-iniciativa"></a>
## La autonom√≠a y la iniciativa


<a id="el-uso-de-las-tic"></a>
## El uso de las TIC



<a id="bibliografia"></a>
# Bibliograf√≠a


<a id="proyectos"></a>
# Proyectos

<a id="empresarialia"></a>
## empresarialIA


<a id="agente-virtual"></a>
## agente virtual


<a id="ejercicio-de-final-de-unidad"></a>
## Ejercicio de final de unidad

### Introducci√≥n a los ejercicios

El archivo dentro de esta carpeta, llamado "ejercicio.md", contiene una serie de problemas dise√±ados para reforzar los conocimientos adquiridos a lo largo del curso en el m√≥dulo de programaci√≥n. Los ejercicios se centran principalmente en la aplicaci√≥n pr√°ctica de estructuras de datos y control de flujo, as√≠ como en la resoluci√≥n de problemas complejos que implican la integraci√≥n de m√∫ltiples conceptos aprendidos anteriormente. Estos ejercicios est√°n orientados a ayudar a los estudiantes a consolidar sus habilidades en programaci√≥n y a prepararse para proyectos m√°s grandes y desafiantes en el futuro.

### Actividades propuestas

Bas√°ndome en el contexto proporcionado, donde se menciona que hay un archivo markdown en la carpeta y considerando que los estudiantes est√°n cursando Formaci√≥n Profesional en Programaci√≥n (posiblemente DAM - Desarrollo de Aplicaciones Multiplataforma), aqu√≠ van algunas actividades propuestas para ellos:

1. **An√°lisis del Ejercicio**
   - **Descripci√≥n**: Los alumnos deben leer el archivo `ejercicio.md` y comprender los requerimientos que contiene. Se espera que extraigan las necesidades del ejercicio y las anoten en un documento o en una plataforma de codificaci√≥n como GitHub.
   
2. **Estructura del C√≥digo**
   - **Descripci√≥n**: Deben identificar la estructura general y el flujo l√≥gico propuesto en `ejercicio.md`. Los estudiantes deber√°n dibujar diagramas de secuencia y/o flujo que reflejen los pasos necesarios para completar el ejercicio.

3. **Implementaci√≥n B√°sica**
   - **Descripci√≥n**: Tras entender la tarea, los alumnos deben escribir una versi√≥n b√°sica del c√≥digo en un lenguaje de programaci√≥n apropiado (por ejemplo Python o Java). Se les pide que respeten las reglas y estructura proporcionadas en el archivo markdown.

4. **Refactorizaci√≥n**
   - **Descripci√≥n**: Una vez implementada la soluci√≥n b√°sica, los estudiantes deber√°n refactorizar su c√≥digo para mejorar su calidad, manteniendo la funcionalidad original. Esto incluye reducir duplicaci√≥n de c√≥digo, a√±adir comentarios claros y seguir buenas pr√°cticas.

5. **Pruebas Unitarias**
   - **Descripci√≥n**: Los alumnos deben escribir pruebas unitarias para verificar que el c√≥digo cumple con todos los requerimientos especificados en `ejercicio.md`. Utilizar√°n un marco de pruebas como JUnit para Java o pytest para Python.

6. **Documentaci√≥n y Explicaci√≥n Oral**
   - **Descripci√≥n**: Los estudiantes deben crear una peque√±a presentaci√≥n donde explican verbalmente c√≥mo funciona su c√≥digo, los desaf√≠os que encontraron durante el proceso, y c√≥mo resolvieron estos problemas. Tambi√©n deber√°n escribir un breve documento markdown detallando sus decisiones de dise√±o.

7. **Revisi√≥n Peer-to-Peer**
   - **Descripci√≥n**: Los estudiantes intercambiar√°n sus proyectos con compa√±eros para recibir retroalimentaci√≥n constructiva sobre su c√≥digo y presentaciones orales. Cada estudiante deber√° preparar preguntas cr√≠ticas basadas en las observaciones de los dem√°s.

8. **Iteraciones Mejoradas**
   - **Descripci√≥n**: Bas√°ndose en la retroalimentaci√≥n recibida, cada alumno mejorar√° sus entregables originales para incluir sugerencias √∫tiles y corregir errores potenciales. Finalmente, deber√°n subir una versi√≥n final a GitHub o un repositorio similar.

Estas actividades est√°n dise√±adas no solo para que los estudiantes comprendan c√≥mo funcionar con problemas de programaci√≥n espec√≠ficos, sino tambi√©n para desarrollar habilidades de trabajo en equipo, comunicaci√≥n efectiva y buenas pr√°cticas de desarrollo.


<a id="examen-final"></a>
## Examen final

### Introducci√≥n a los ejercicios

Esta carpeta contiene una serie de ejercicios SQL dise√±ados para familiarizarte con los conceptos b√°sicos y avanzados del manejo de bases de datos. Los problemas trabajan desde la creaci√≥n de tablas, inserci√≥n de datos hasta la implementaci√≥n de claves for√°neas y consultas JOIN. Tambi√©n se practica la creaci√≥n de vistas que combinan datos de m√∫ltiples tablas para facilitar la visualizaci√≥n y el acceso a informaci√≥n relevante. Finalmente, los ejercicios incluyen la gesti√≥n de usuarios y privilegios en MySQL, lo cual es crucial para entender c√≥mo controlar el acceso a las bases de datos.

### crear tablas
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este fragmento de c√≥digo SQL se utiliza para crear una nueva base de datos y dos tablas dentro de ella. Primero, crea una base de datos llamada `portafolioceac`. Luego, selecciona esta base de datos con el comando `USE`. Despu√©s, define dos tablas: `Piezas` y `Categorias`.

La tabla `Piezas` contiene informaci√≥n sobre diferentes piezas o elementos que probablemente forman parte del portafolio. Cada registro en esta tabla tiene un identificador √∫nico generado autom√°ticamente, un t√≠tulo, una descripci√≥n, la ruta a su imagen, un URL relacionado y un ID de categor√≠a para indicar a qu√© categor√≠a pertenece cada pieza.

Por otro lado, la tabla `Categorias` almacena informaci√≥n sobre las diferentes categor√≠as en las que se pueden clasificar las piezas. Cada registro en esta tabla tiene tambi√©n un identificador √∫nico generado autom√°ticamente, un t√≠tulo y una descripci√≥n.

Este c√≥digo es importante porque establece la estructura b√°sica de los datos que necesitar√°s para gestionar tu portafolio o proyecto relacionado con el √°rea creativa o t√©cnica a la que pertenece este ejercicio.

`001-crear tablas.sql`

```sql
CREATE DATABASE portafolioceac;

USE portafolioceac;


CREATE TABLE Piezas(
  Identificador INT auto_increment PRIMARY KEY,
  titulo VARCHAR(255),
  descripcion VARCHAR(255),
  imagen VARCHAR(255),
  url VARCHAR(255),
  id_categoria INT
);

CREATE TABLE Categorias(
  Identificador INT auto_increment PRIMARY KEY,
  titulo VARCHAR(255),
  descripcion VARCHAR(255)
);
```

### insertar
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este fragmento de c√≥digo SQL est√° insertando dos nuevos registros en una base de datos. En primer lugar, se a√±ade un nuevo registro a la tabla `Categorias`. La sentencia `INSERT INTO Categorias VALUES` indica que estamos agregando informaci√≥n a esta tabla espec√≠fica. El valor `NULL` es para el campo identificador (siendo auto-incremental), 'General' es el nombre de la categor√≠a, y la descripci√≥n 'Esta es la categoria general'. Esto crea una nueva entrada en la tabla `Categorias`.

A continuaci√≥n, se realiza otra inserci√≥n pero esta vez en la tabla `Piezas`. La sentencia `INSERT INTO Piezas VALUES` agrega un nuevo registro a esta tabla. Similar al caso anterior, comienza con `NULL`, indicando que el sistema manejar√° autom√°ticamente el identificador √∫nico para este registro. Los campos restantes son 'Primera pieza' (nombre de la pieza), 'Esta es la descripcion de la primera pieza' (descripci√≥n), 'josevicente.jpg' (posiblemente una imagen relacionada con la pieza) y 'https://jocarsa.com' (tal vez un enlace web asociado). El √∫ltimo valor, `1`, se refiere a la categor√≠a de esta pieza, que probablemente hace referencia al registro reci√©n creado en la tabla `Categorias`. Este c√≥digo es importante porque establece los datos iniciales necesarios para el funcionamiento correcto del sistema.

`002-insertar.sql`

```sql
INSERT INTO Categorias VALUES(
  NULL,
  'General',
  'Esta es la categoria general'
);

INSERT INTO Piezas VALUES(
  NULL,
  'Primera pieza',
  'Esta es la descripcion de la primera pieza',
  'josevicente.jpg',
  'https://jocarsa.com',
  1
);
```

### fk
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este fragmento de c√≥digo SQL est√° realizando una operaci√≥n muy importante en la gesti√≥n de bases de datos relacionales. En concreto, se a√±ade un tipo especial de restricci√≥n a la tabla llamada `Piezas`. Esta restricci√≥n se denomina clave for√°nea (Foreign Key) y es crucial para mantener las relaciones entre tablas y garantizar la integridad referencial.

La instrucci√≥n `ALTER TABLE Piezas` indica que estamos modificando la tabla `Piezas`, a√±adi√©ndole una nueva restricci√≥n llamada `fk_piezas_categorias`. Esta restricci√≥n establece que la columna `id_categoria` en la tabla `Piezas` debe tener valores que coincidan con los de la columna `identificador` en la tabla `Categorias`.

Las cl√°usulas `ON DELETE CASCADE` y `ON UPDATE CASCADE` son especialmente importantes. La primera, `ON DELETE CASCADE`, significa que si se elimina un registro en la tabla `Categorias`, tambi√©n se eliminar√°n autom√°ticamente todos los registros relacionados en la tabla `Piezas`. De manera similar, `ON UPDATE CASCADE` asegura que si el valor de una categor√≠a cambia (por ejemplo, si su identificador es actualizado), ese cambio ser√° autom√°ticamente reflejado en todas las entradas correspondientes en `Piezas`.

Esta operaci√≥n es vital para mantener la consistencia y coherencia entre las tablas relacionadas, lo cual es fundamental en el dise√±o de bases de datos normalizadas.

`003-fk.sql`

```sql
ALTER TABLE Piezas
ADD CONSTRAINT fk_piezas_categorias
FOREIGN KEY (id_categoria) REFERENCES Categorias(identificador)
ON DELETE CASCADE
ON UPDATE CASCADE;
```

### selecciones
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este fragmento de c√≥digo SQL contiene dos consultas simples que se utilizan para recuperar datos de una base de datos. La primera consulta, `SELECT * FROM Categorias;`, selecciona todas las filas y columnas de la tabla llamada "Categorias". Esto significa que obtendr√°s toda la informaci√≥n almacenada en esta tabla.

La segunda consulta, `SELECT * FROM Piezas;`, hace lo mismo pero con la tabla "Piezas". De nuevo, se seleccionan todas las filas y columnas de esta tabla espec√≠fica. 

Estos comandos son importantes porque permiten a un usuario o programador acceder r√°pidamente a toda la informaci√≥n almacenada en estas tablas sin necesidad de especificar cada columna individualmente. Es √∫til para una r√°pida revisi√≥n o depuraci√≥n de datos durante el desarrollo y pruebas de aplicaciones relacionales.

`004-selecciones.sql`

```sql
SELECT * FROM Categorias;

SELECT * FROM Piezas;
```

### left join
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este fragmento de c√≥digo SQL realiza una operaci√≥n llamada "left join" entre dos tablas: `Piezas` y `Categorias`. La idea principal es combinar los datos de ambas tablas bas√°ndose en la relaci√≥n existente entre ellas, que se define mediante la columna `id_categoria` en la tabla `Piezas` y la columna `Identificador` en la tabla `Categorias`.

El left join asegura que todas las filas de la tabla `Piezas` aparezcan en el resultado, incluso si no hay correspondencia con alguna fila en la tabla `Categorias`. Esto significa que para cada pieza, se mostrar√°n sus datos junto a los datos de su categor√≠a asociada (si existe), o bien, si no existe una categor√≠a relacionada, las columnas de la tabla `Categorias` aparecer√°n como valores nulos.

Esta consulta es √∫til cuando necesitas obtener informaci√≥n completa sobre las piezas y tambi√©n quieres saber qu√© categor√≠as tienen asignadas, sin perder ninguna pieza por falta de correspondencia en la categor√≠a. Es importante para entender c√≥mo est√°n relacionados los datos en diferentes tablas del sistema de gesti√≥n de base de datos que est√°s utilizando.

`005-left join.sql`

```sql
SELECT 
* 
FROM Piezas
LEFT JOIN Categorias
ON Piezas.id_categoria = Categorias.Identificador;
```

### ahora creo la vista
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este c√≥digo SQL crea una vista llamada `piezas_y_categorias` que combina informaci√≥n de dos tablas: `Piezas` y `Categorias`. La vista selecciona columnas espec√≠ficas como el t√≠tulo y la descripci√≥n tanto para las categor√≠as como para las piezas, adem√°s del nombre de la imagen y la URL. Se utiliza una operaci√≥n `LEFT JOIN` entre estas dos tablas basada en la relaci√≥n que existe mediante la llave for√°nea `id_categoria` en la tabla `Piezas`, que se une con el identificador (`Identificador`) en la tabla `Categorias`.

La consulta final `SELECT * FROM piezas_y_categorias;` simplemente muestra todos los registros de esta vista reci√©n creada, proporcionando una visi√≥n combinada y simplificada de la informaci√≥n almacenada en las tablas originales. Esta estrategia es √∫til para facilitar el acceso a datos relacionados sin tener que escribir constantemente consultas complejas o repetitivas.

La creaci√≥n de vistas como esta puede ser especialmente valiosa en proyectos grandes donde varios usuarios necesiten acceder regularmente a la misma combinaci√≥n de informaci√≥n, ya que permite una manipulaci√≥n m√°s sencilla y mantenimiento m√°s eficiente del c√≥digo.

`006-ahora creo la vista.sql`

```sql
CREATE VIEW piezas_y_categorias AS 
SELECT 
Categorias.titulo AS categoriatitulo,
Categorias.descripcion AS categoriadescripcion,
Piezas.titulo AS piezatitulo,
Piezas.descripcion AS piezadescripcion,
imagen,
url
FROM Piezas
LEFT JOIN Categorias
ON Piezas.id_categoria = Categorias.Identificador;

SELECT * FROM piezas_y_categorias;
```

### usuario
<small>Creado: 2025-11-24 18:59</small>

#### Explicaci√≥n

Este fragmento de c√≥digo SQL tiene como objetivo crear un nuevo usuario en el sistema de bases de datos y otorgarle los permisos necesarios para acceder a una base de datos espec√≠fica. Comenzamos creando un nuevo usuario llamado 'portafolioceac' con una contrase√±a establecida, lo que permite al usuario conectarse desde el localhost.

Luego, se le otorgan ciertos l√≠mites y configuraciones al usuario reci√©n creado para asegurar que no tenga restricciones en cuanto a la cantidad de consultas o conexiones simult√°neas. Esto es importante para garantizar que el usuario pueda trabajar sin interrupciones dentro del sistema.

Finalmente, se conceden todos los privilegios sobre la base de datos 'portafolioceac' al nuevo usuario creado, permiti√©ndole realizar cualquier tipo de operaci√≥n en ella. La √∫ltima l√≠nea `FLUSH PRIVILEGES` asegura que estos cambios sean aplicados inmediatamente por el servidor SQL.

Este c√≥digo es crucial para establecer un entorno seguro y controlado donde los usuarios espec√≠ficos puedan acceder a las bases de datos seg√∫n sus necesidades, manteniendo la integridad y seguridad del sistema.

`007-usuario.sql`

```sql
-- crea usuario nuevo con contrase√±a
-- creamos el nombre de usuario que queramos
CREATE USER 
'portafolioceac'@'localhost' 
IDENTIFIED  BY 'portafolioceac';

-- permite acceso a ese usuario
GRANT USAGE ON *.* TO 'portafolioceac'@'localhost';
--[tuservidor] == localhost
-- La contrase√±a puede requerir Mayus, minus, numeros, caracteres, min len

-- quitale todos los limites que tenga
ALTER USER 'portafolioceac'@'localhost' 
REQUIRE NONE 
WITH MAX_QUERIES_PER_HOUR 0 
MAX_CONNECTIONS_PER_HOUR 0 
MAX_UPDATES_PER_HOUR 0 
MAX_USER_CONNECTIONS 0;

-- dale acceso a la base de datos empresadam
GRANT ALL PRIVILEGES ON portafolioceac.* 
TO 'portafolioceac'@'localhost';

-- recarga la tabla de privilegios
FLUSH PRIVILEGES;
```

### Actividades propuestas

1. **Creaci√≥n de Tablas en SQL**
   - **Descripci√≥n:** Los alumnos deber√°n crear una base de datos y definir esquemas para tablas similares a las existentes en los ejercicios proporcionados, como categor√≠as y piezas. El objetivo es que comprendan la estructura b√°sica de una base de datos relacional.

2. **Inserci√≥n de Datos**
   - **Descripci√≥n:** Los estudiantes deben insertar registros en las tablas creadas siguiendo el ejemplo del archivo 002-insertar.sql. Se espera que aprendan a gestionar la inserci√≥n de datos en esquemas previamente definidos.

3. **Relaciones entre Tablas**
   - **Descripci√≥n:** Bas√°ndose en el c√≥digo del archivo 003-fk.sql, los alumnos deben agregar restricciones de clave for√°nea para asegurar relaciones coherentes entre las tablas creadas. Este ejercicio busca que comprendan la importancia de mantener integridad referencial.

4. **Consultas B√°sicas con SELECT**
   - **Descripci√≥n:** Utilizando el ejemplo del archivo 004-selecciones.sql, los estudiantes deber√°n escribir consultas para recuperar datos completos de ambas tablas creadas en sus propios esquemas. Se espera que entiendan c√≥mo seleccionar y visualizar datos desde una base de datos.

5. **Operaci√≥n LEFT JOIN**
   - **Descripci√≥n:** Los alumnos deben replicar la consulta LEFT JOIN del archivo 005-left join.sql, combinando informaci√≥n de m√∫ltiples tablas. Esto permitir√° que comprendan las operaciones entre conjuntos y c√≥mo recuperar datos relacionados eficazmente.

6. **Creaci√≥n y Consulta de Vistas**
   - **Descripci√≥n:** Siguiendo el ejemplo del archivo 006-ahora creo la vista.sql, los estudiantes deben crear una vista personalizada que combine informaci√≥n de las tablas creadas. Se espera que aprendan c√≥mo simplificar consultas complejas con vistas.

7. **Administraci√≥n de Usuarios en MySQL**
   - **Descripci√≥n:** Bas√°ndose en el archivo 007-usuario.sql, los alumnos deber√°n practicar la creaci√≥n y administraci√≥n de usuarios en su propia base de datos SQL, incluyendo configuraciones de privilegios. Este ejercicio busca que adquieran habilidades para gestionar m√∫ltiples cuentas de acceso a bases de datos.

8. **Pr√°ctica Integrada**
   - **Descripci√≥n:** Los estudiantes tendr√°n que combinar todo lo aprendido en las actividades anteriores, creando un conjunto de tablas y procedimientos SQL propios, desde la creaci√≥n hasta la consulta, pasando por la administraci√≥n de usuarios. Este ejercicio permite aplicar todos los conceptos adquiridos de manera pr√°ctica e integrada.

Estas actividades est√°n dise√±adas para asegurar que los estudiantes comprendan y puedan aplicar en su totalidad el conocimiento sobre bases de datos SQL, desde la creaci√≥n hasta la manipulaci√≥n avanzada de datos.



<a id="proyectos-segundo-trimestre"></a>
# Proyectos segundo trimestre

<a id="extractor-transcripciones"></a>
## Extractor transcripciones

### Introducci√≥n a los ejercicios

Este es un conjunto de scripts y archivos para automatizar el proceso completo de transcripci√≥n y resumen de una lista dada de videos de YouTube en espa√±ol. Aqu√≠ est√° la descripci√≥n general del flujo de trabajo:

1. **001-listas.py**: Este script recorre una lista JSON de URLs de listas de reproducci√≥n de YouTube, descarga los metadatos de cada video y guarda el resultado en archivos .json.

2. **002-transcripciones.py**: A partir del output de `001-listas.py`, este script:
   - Descarga transcripciones subtituladas (si est√°n disponibles) para los videos.
   - Convierte las transcripciones de VTT a texto s√≥lido y guarda el resultado en archivos .txt.

3. **003-resumenes.py**: Este script toma los archivos .txt generados por `002-transcripciones.py` y:
   - Utiliza un modelo de lenguaje granular (presumiblemente entrenado para espa√±ol) llamado 'qwen2.5:3b-instruct' a trav√©s del servicio Ollama.
   - Genera res√∫menes en espa√±ol concisos y docentes basados en el contenido de las transcripciones.

4. **listas.json**: Este archivo contiene la lista de URLs de las listas de reproducci√≥n de YouTube que se van a procesar con los scripts anteriores.

### Requisitos Previos:
- Instalaci√≥n de Python 3.x
- Dependencias de paquetes de python (pandas, requests)
- Ollama instalado y el modelo qwen2.5:3b-instruct disponible

### Estructura del Proyecto:
```
mi_proyecto/
‚îÇ
‚îú‚îÄ‚îÄ listas.json      # Lista de URLs a procesar
‚îú‚îÄ‚îÄ 001-listas.py    # Descarga metadatos de las listas de reproducci√≥n
‚îú‚îÄ‚îÄ 002-transcripciones.py   # Genera transcripciones en texto s√≥lido
‚îî‚îÄ‚îÄ 003-resumenes.py   # Crea res√∫menes basados en las transcripciones generadas
```

### Ejecuci√≥n:
1. **Configurar** `listas.json` con las URLs de los videos que deseas procesar.
2. Ejecutar `001-listas.py`. Este script crear√° carpetas y archivos .json para cada lista de reproducci√≥n en el directorio actual (generalmente una carpeta llamada "videos" si no se ha modificado).
3. Luego ejecuta `002-transcripciones.py`, que procesa los metadatos descargados por `001-listas.py` y genera archivos .txt de transcripci√≥n en la carpeta 'transcripts/'.
4. Finalmente, ejecuta `003-resumenes.py`. Este script recorrer√° los archivos .txt generados por el paso anterior para crear res√∫menes en espa√±ol.

### Notas:
- Los scripts est√°n configurados con pausas entre cada video (por ejemplo, 5 segundos) para evitar ser bloqueado por YouTube.
- Aseg√∫rate de tener suficiente espacio en disco y memoria RAM para manejar los archivos grandes resultantes.
- El modelo `qwen2.5:3b-instruct` utilizado para generar res√∫menes debe estar disponible localmente o como un servicio accesible.

### Consideraciones:
- Este flujo puede tomar bastante tiempo si hay muchos videos largos, especialmente la etapa de transcripci√≥n y resumen.
- Es recomendable tener acceso a una computadora con conexi√≥n estable y suficientes recursos para completar el proceso sin interrupciones.

### videos
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este script en Python se encarga de procesar una lista de URLs de playlists de YouTube y extraer informaci√≥n detallada sobre cada video dentro de esas playlists. La entrada del programa es un archivo JSON llamado `listas.json` que contiene una lista de URLs a diferentes playlists de YouTube.

El c√≥digo principal hace lo siguiente:

1. **Inicializaci√≥n**: Importa las librer√≠as necesarias, incluyendo `yt-dlp`, que se usa para extraer informaci√≥n detallada del video desde la URL de la playlist.
2. **Funciones auxiliares**:
   - `safe_filename`: Esta funci√≥n asegura que los t√≠tulos generados no contengan caracteres que puedan ser problem√°ticos en un sistema de archivos, como barra diagonal o caracteres especiales.
   - `extract_playlist_id`: Extrae el ID √∫nico de una playlist a partir de su URL.
   - `fetch_playlist_title_and_items`: Utiliza la biblioteca `yt-dlp` para obtener tanto el t√≠tulo de la playlist como los detalles individuales (t√≠tulo, ID y URL) de cada video dentro de ella.
3. **Procesamiento principal**:
   - Se abre el archivo `listas.json`, que contiene las URLs de las playlists.
   - Por cada URL en este archivo, se obtiene el t√≠tulo √∫nico de la playlist y los detalles de sus videos.
   - Crea un archivo JSON por cada playlist con toda esta informaci√≥n, almacenada en una carpeta llamada `playlists_expandidas`.
4. **Finalizaci√≥n**: El script imprime un resumen del proceso, incluyendo cu√°ntas playlists se han procesado y cu√°ntos v√≠deos se han encontrado en total.

Este script es √∫til para proyectos que requieren el an√°lisis o procesamiento de grandes cantidades de contenido multimedia desde YouTube, como parte de investigaciones acad√©micas, an√°lisis de audiencia o cualquier otro tipo de estudio relacionado con contenidos en l√≠nea.

`001-videos.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Expande una lista de playlists de YouTube (array de URLs) en un JSON por playlist.

Entrada:
  playlists_in.json  ->  [ "https://www.youtube.com/playlist?list=...", ... ]

Salida:
  Carpeta playlists_expandidas/ con un archivo JSON por playlist:

  {
    "id": "<playlist_id>",
    "title": "<playlist_title>",
    "url": "https://www.youtube.com/playlist?list=<playlist_id>",
    "items": [
      { "id": "...", "title": "...", "url": "...", "type": "video" },
      ...
    ]
  }

Requisitos: yt-dlp  (pip install yt-dlp)
"""

import json
import sys
import os
import re
from pathlib import Path
from urllib.parse import urlparse, parse_qs

try:
    from yt_dlp import YoutubeDL
except ImportError:
    print("ERROR: yt-dlp is required. Install with: pip install yt-dlp", file=sys.stderr)
    sys.exit(1)

INPUT_FILE = "listas.json"        # JSON con array de URLs de playlists
OUTPUT_DIR = Path("playlists_expandidas")

SAFE_CHARS = r"[^A-Za-z0-9√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú()_. -]"


def safe_filename(name: str) -> str:
    """Sanitiza un texto para usarlo como nombre de archivo."""
    name = name.strip().replace("/", "-").replace("\\", "-")
    name = re.sub(SAFE_CHARS, "", name)
    name = re.sub(r"\s+", " ", name)
    return name[:200] if len(name) > 200 else name


def extract_playlist_id(url):
    """Return playlist ID from any YouTube URL (via 'list=' query param), or None."""
    try:
        qs = parse_qs(urlparse(url).query)
        return qs.get("list", [None])[0]
    except Exception:
        return None


def fetch_playlist_title_and_items(playlist_url):
    """
    Return (title, items) for a playlist URL.
    items = [{id, title, url, type:"video"}, ...]
    """
    ydl_opts = {
        "extract_flat": True,
        "skip_download": True,
        "quiet": True,
        "nocheckcertificate": True,
        "noprogress": True,
    }

    with YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(playlist_url, download=False)

    if not info:
        return None, []

    playlist_title = info.get("title")
    entries = []
    if info.get("_type") == "playlist":
        entries = info.get("entries") or []
    else:
        entries = info.get("entries") or []
        if not entries and info.get("id") and info.get("title"):
            entries = [info]
            playlist_title = playlist_title or "Sin t√≠tulo"

    items = []
    for e in entries:
        vid = e.get("id")
        vtitle = e.get("title")
        vurl = f"https://www.youtube.com/watch?v={vid}" if vid else e.get("url")
        if vid and vtitle and vurl:
            items.append({"id": vid, "title": vtitle, "url": vurl, "type": "video"})

    if not playlist_title:
        playlist_title = "Sin t√≠tulo"

    return playlist_title, items


def disambiguate_title(title, existing_titles):
    """Ensure unique title within existing_titles (append ' (2)', ' (3)', ... if needed)."""
    if title not in existing_titles:
        return title
    i = 2
    while True:
        candidate = f"{title} ({i})"
        if candidate not in existing_titles:
            return candidate
        i += 1


def main():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    try:
        with open(INPUT_FILE, "r", encoding="utf-8") as f:
            playlists = json.load(f)
    except Exception as e:
        print(f"ERROR: no se pudo leer '{INPUT_FILE}': {e}", file=sys.stderr)
        sys.exit(1)

    if not isinstance(playlists, list):
        print(f"ERROR: '{INPUT_FILE}' debe ser un array JSON de URLs de playlists.", file=sys.stderr)
        sys.exit(1)

    used_titles = set()
    total_playlists = 0
    total_videos = 0

    seen_playlist_ids = set()

    for url in playlists:
        if not isinstance(url, str):
            print(f"WARNING: elemento no cadena en JSON, se ignora: {url}", file=sys.stderr)
            continue

        plist = extract_playlist_id(url)
        if not plist:
            print(f"WARNING: URL sin 'list=': {url}", file=sys.stderr)
            continue
        if plist in seen_playlist_ids:
            print(f"INFO: playlist ya procesada, se ignora duplicado: {plist}", file=sys.stderr)
            continue
        seen_playlist_ids.add(plist)

        normalized = f"https://www.youtube.com/playlist?list={plist}"
        print(f"Procesando playlist: {normalized}")

        try:
            ptitle, videos = fetch_playlist_title_and_items(normalized)
        except Exception as e:
            print(f"WARNING: could not fetch playlist {plist}: {e}", file=sys.stderr)
            ptitle, videos = f"Playlist {plist}", []

        unique_title = disambiguate_title(ptitle, used_titles)
        used_titles.add(unique_title)

        safe_title = safe_filename(unique_title)
        out_path = OUTPUT_DIR / f"{safe_title}.json"

        playlist_obj = {
            "id": plist,
            "title": unique_title,
            "url": normalized,
            "items": videos,
        }

        with out_path.open("w", encoding="utf-8") as f:
            json.dump(playlist_obj, f, ensure_ascii=False, indent=2)

        total_playlists += 1
        total_videos += len(videos)
        print(f"  -> Guardado {out_path.name} ({len(videos)} v√≠deos)")

    print("\nDone.")
    print(f"Playlists procesadas: {total_playlists}")
    print(f"V√≠deos totales:       {total_videos}")


if __name__ == "__main__":
    main()
```

### transcripciones
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

El script que proporcionaste es una herramienta compleja para descargar y procesar transcripciones de videos desde YouTube. Para mejorar la legibilidad y mantenimiento del c√≥digo, aqu√≠ hay algunas sugerencias:

1. **Documentaci√≥n Mejorada:** A√±adir comentarios m√°s detallados al principio del archivo explicando el prop√≥sito del script, las variables principales utilizadas y c√≥mo funciona en general.

2. **Divisi√≥n de Funciones:** Dividir el c√≥digo en funciones m√°s peque√±as y manejables para cada tarea espec√≠fica (por ejemplo, descarga de subt√≠tulos, conversi√≥n VTT a texto s√≥lido).

3. **Manejo de Excepciones Mejorado:** Asegurarse de que las excepciones capturadas proporcionen informaci√≥n √∫til tanto para el usuario como para la depuraci√≥n.

4. **Uso de Constantes y Variables Bien Documentadas:** Usar constantes para configuraciones fijas (como rutas, nombres de archivos) y documentar bien las variables que se usan en diferentes partes del script.

5. **Logs Detallados y Descriptivos:** Mejorar el registro para proporcionar informaci√≥n m√°s detallada sobre qu√© est√° pasando en cada paso del proceso.

6. **Optimizaci√≥n de Consultas:** En lugar de realizar m√∫ltiples llamadas a `iter_videos_from_playlists_folder`, considera almacenar los resultados en una estructura de datos y trabajar con ellos para evitar la repetici√≥n innecesaria.

### Ejemplo de Refactorizaci√≥n

A continuaci√≥n, se muestra un ejemplo simplificado de c√≥mo podr√≠as refactorizar partes del c√≥digo para mejorar su legibilidad:

```python
import os
from pathlib import Path

PLAYLISTS_DIR = Path('playlists')
OUT_BASE_DIR = Path('output')

PLAYER_CLIENTS = ['ANDROID', 'WEB']

def list_videos_from_playlists(playlists_dir: Path) -> tuple:
    """Lista los videos de las carpetas de playlist."""
    for playlist in playlists_dir.iterdir():
        if not playlist.is_dir():
            continue
        for video_file in playlist.glob('*.txt'):
            with open(video_file, 'r') as f:
                yield from parse_video_data(f.read())

def download_and_process_subtitles(url: str, base_out: Path) -> tuple:
    """Descarga y procesa los subt√≠tulos del video."""
    pick = download_spanish_with_ytdlp(url, base_out)
    if not pick:
        return None
    vtt_path, lang_code, client_used = pick
    solid_text = convert_vtt_to_solid_text(vtt_path)
    return (vtt_path, lang_code, client_used, solid_text)

def parse_video_data(video_info: str) -> tuple:
    """Parsea la informaci√≥n del video a partir de un archivo .txt."""
    # Implementar el parsing seg√∫n el formato esperado
    pass

def download_spanish_with_ytdlp(url: str, base_out: Path) -> tuple:
    """Descarga los subt√≠tulos en espa√±ol o cualquier otro disponible."""
    for client in PLAYER_CLIENTS:
        result = perform_download_with_langs(['es'], client, url, base_out)
        if result:
            return result
    # Realizar intentos adicionales si no se descargaron subt√≠tulos
    pass

def convert_vtt_to_solid_text(vtt_path: Path) -> str:
    """Convierte un archivo VTT a texto s√≥lido."""
    with open(vtt_path, 'r', encoding='utf-8') as file:
        return vtt_to_solid_text(file.read())

def main():
    playlist_videos = list(list_videos_from_playlists(PLAYLISTS_DIR))
    
    total, ok, skipped, failed = 0, 0, 0, 0

    for vid_data in playlist_videos:
        total += 1
        title, url, vid, playlist_title, playlist_url = vid_data
        
        out_dir = OUT_BASE_DIR / safe_filename(playlist_title)
        txt_path = out_dir / f"{safe_filename(title)}.txt"
        
        if txt_path.exists():
            skipped += 1
            continue

        print(f"[{total:03d}] [{playlist_title}] {title} -> {url}")
        
        result = download_and_process_subtitles(url, out_dir)
        
        if not result:
            failed += 1
            create_failure_log(out_dir / f"{safe_filename(title)}.log", playlist_title, url, title, vid_data, "Subt√≠tulos no disponibles")
            continue
        
        vtt_path, lang_code, client_used, solid_text = result
        create_txt_file(txt_path, solid_text)
        
        ok += 1

    print("\n=== RESUMEN ===")
    print(f"V√≠deos totales:   {total}")
    print(f"OK (transcritos): {ok}")
    print(f"Omitidos:         {skipped}  (sin subt√≠tulos utilizables)")
    print(f"Fallidos:         {failed}")

if __name__ == "__main__":
    main()
```

Este ejemplo muestra c√≥mo podr√≠as dividir tu script en funciones m√°s peque√±as y manejables, mejorando la legibilidad y el mantenimiento del c√≥digo. Aseg√∫rate de implementar las partes espec√≠ficas que se han omitido (como `parse_video_data`, `perform_download_with_langs`, etc.) bas√°ndote en tu l√≥gica espec√≠fica.

### Mejoramientos Adicionales:

- **Manejo de Errores:** Implementa manejo de errores m√°s robusto para capturar y registrar cualquier excepci√≥n que pueda ocurrir.
- **Configuraci√≥n Flexibles:** Considerar el uso de variables de entorno o un archivo de configuraci√≥n para las rutas y otros par√°metros.
- **Logging en Tiempo Real:** A√±adir logs en tiempo real para proporcionar informaci√≥n detallada sobre cada paso del proceso, lo que puede ser √∫til durante la depuraci√≥n.

`002-transcripciones.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Transcribe espa√±ol desde YouTube con yt-dlp (con fallback robusto).

Nueva l√≥gica:
- Lee TODOS los JSON de la carpeta 'playlists_expandidas/' (un JSON por playlist).
- Cada JSON tiene:
    {
      "id": "<playlist_id>",
      "title": "<playlist_title>",
      "url": "https://www.youtube.com/playlist?list=...",
      "items": [
        { "id", "title", "url", "type": "video" },
        ...
      ]
    }
- Para cada playlist crea una carpeta dentro de 'transcripts/' con el nombre saneado.
- Genera un .txt por cada v√≠deo (y .log en caso de problemas) dentro de esa carpeta.

L√≥gica de subt√≠tulos:
- Prueba player_client=mweb -> tv -> web -> android (android al final por PO token).
- Prioriza subt√≠tulos en espa√±ol (es, es-ES, es-419, es.*).
- Si no hay espa√±ol: toma el primer auto-caption disponible (p.ej. 'en').
- Como rescate final: --all-subs y luego filtra el mejor candidato.
- Convierte .vtt a texto s√≥lido (un p√°rrafo) sin timestamps, etiquetas, ni roll-up.
- Registra en .log los idiomas disponibles si no se consigue nada.

Requisitos:
  pip install -U yt-dlp
  (opcional) cookies.txt v√°lido (exportado del navegador)
"""

import json
import re
import subprocess
import sys
import time
from collections import deque
from pathlib import Path
from typing import Dict, Iterable, List, Tuple, Optional

# =======================
# CONFIG
# =======================
PLAYLISTS_DIR = Path("playlists_expandidas")  # entrada: JSON por playlist
OUT_BASE_DIR = Path("transcripts")            # salida: carpeta por playlist
SLEEP_BETWEEN = 2.0

COOKIES_FILE = Path("cookies.txt")  # cambia si lo tienes en otra ruta
USER_AGENT = "Mozilla/5.0"

# Ponemos android al final para evitar el problema del PO token si no lo usas
PLAYER_CLIENTS = ["mweb", "tv", "web", "android"]

SAFE_CHARS = r"[^A-Za-z0-9√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú()_. -]"

# Candidatos de idioma (prefijos y exactos) para espa√±ol:
SPANISH_LANG_CANDIDATES = [
    "es-ES",
    "es-419",
    "es",
    "es.*",   # es-AR, es-MX, es-CU, es.automatic, etc.
    "spa",    # poco com√∫n, pero a veces aparece
]

# Prioridad para elegir el VTT si hay varios
LANG_PRIORITY = [
    "es-ES",
    "es-419",
    "es",
    # variantes es.* despu√©s
]

# Fallback si no hay espa√±ol: coger el primer idioma disponible de automatic_captions
FALLBACK_TO_ANY_AUTO = True

# Si hay que caer a un auto-captions no espa√±ol, prioriza en este orden
FALLBACK_AUTO_PRIORITY = ["es-ES", "es-419", "es", "pt", "en"]

# =======================
# UTILIDADES GENERALES
# =======================
WS_RE   = re.compile(r"\s+")
_TS_RE  = re.compile(r"-->\s")         # l√≠neas de tiempo
_NUM_RE = re.compile(r"^\d+$")         # numeraci√≥n de cues
_TAG_RE = re.compile(r"<[^>]+>")       # etiquetas HTML/Style

IGNORE_PREFIXES = (
    "WEBVTT", "NOTE", "X-TIMESTAMP-MAP", "Kind:", "Language:",
    "STYLE", "REGION", "##", "align:", "position:", "line:", "size:"
)

NOISE_TOKENS = ("‚ô™", "‚ô´")


def safe_filename(name: str) -> str:
    name = name.strip().replace("/", "-").replace("\\", "-")
    name = re.sub(SAFE_CHARS, "", name)
    name = re.sub(r"\s+", " ", name)
    return name[:200] if len(name) > 200 else name


def load_json(path: Path) -> Dict:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def iter_videos_from_playlists_folder(folder: Path):
    """
    Recorre todos los JSON de 'folder' y genera:
      (playlist_title, playlist_safe, playlist_url, video_title, video_url, video_id)
    """
    if not folder.is_dir():
        raise FileNotFoundError(f"La carpeta de playlists '{folder}' no existe.")

    for json_file in sorted(folder.glob("*.json")):
        try:
            data = load_json(json_file)
        except Exception as e:
            print(f"WARNING: no se pudo leer {json_file.name}: {e}", file=sys.stderr)
            continue

        playlist_title = data.get("title") or json_file.stem
        playlist_id = data.get("id", "")
        playlist_url = data.get("url", "")
        playlist_safe = safe_filename(playlist_title or playlist_id or json_file.stem)

        items = data.get("items") or []
        for it in items:
            if not isinstance(it, dict):
                continue
            if it.get("type") and it.get("type") != "video":
                continue
            title = it.get("title") or f"video_{it.get('id', '')}"
            url = it.get("url") or (f"https://www.youtube.com/watch?v={it.get('id')}" if it.get("id") else "")
            vid = it.get("id") or ""
            if url and vid and title and "[Deleted video]" not in title:
                yield playlist_title, playlist_safe, playlist_url, title, url, vid


def normalize_line(s: str) -> str:
    s = _TAG_RE.sub("", s)
    for tok in NOISE_TOKENS:
        s = s.replace(tok, "")
    s = s.strip()
    s = WS_RE.sub(" ", s)
    return s


def vtt_to_solid_text(vtt_path: Path) -> str:
    if not vtt_path.exists():
        return ""
    lines_clean: List[str] = []
    last_seen = deque(maxlen=3)
    with vtt_path.open("r", encoding="utf-8", errors="ignore") as f:
        for raw in f:
            s = raw.strip()
            if not s:
                continue
            if s.startswith(IGNORE_PREFIXES):
                continue
            if _NUM_RE.fullmatch(s):
                continue
            if _TS_RE.search(s):
                continue
            s = normalize_line(s)
            if not s:
                continue
            if s in last_seen:
                continue
            lines_clean.append(s)
            last_seen.append(s)
    if not lines_clean:
        return ""
    text = " ".join(lines_clean)
    text = re.sub(r"\s+([,.!?;:])", r"\1", text)
    text = re.sub(r"\(\s+", "(", text)
    text = re.sub(r"\s+\)", ")", text)
    text = re.sub(WS_RE, " ", text).strip()
    return text

# =======================
# YT-DLP HELPERS
# =======================
def _build_common_cmd(url: str, base_out: Path) -> List[str]:
    cmd = [
        "yt-dlp",
        "--user-agent", USER_AGENT,
        "--skip-download",
        "--sub-format", "vtt/srv3",
        "--convert-subs", "vtt",
        "--output", str(base_out) + ".%(ext)s",
        url,
    ]
    if COOKIES_FILE.exists():
        cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
    return cmd


def _list_subs(url: str, player_client: str) -> str:
    """Devuelve el texto de --list-subs para logging."""
    cmd = _build_common_cmd(url, Path("dummy"))
    cmd += ["--list-subs", "--extractor-args", f"youtube:player_client={player_client}"]
    try:
        p = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        return p.stdout.decode("utf-8", errors="ignore")
    except subprocess.CalledProcessError as e:
        return e.stdout.decode("utf-8", errors="ignore") if e.stdout else str(e)


def _spanish_glob_candidates(base_out: Path) -> List[Path]:
    # Cualquier archivo que empiece por el nombre base y tenga .es*.vtt
    pattern = base_out.name + ".es*.vtt"
    return sorted(base_out.parent.glob(pattern))


def _lang_from_filename(path: Path, base_stem: str) -> str:
    # "<base>.<lang>.vtt" -> extrae <lang>
    name = path.name
    if name.startswith(base_stem + "."):
        rest = name[len(base_stem) + 1 :]
    else:
        rest = name
    if rest.lower().endswith(".vtt"):
        rest = rest[:-4]
    return rest


def _pick_best_spanish_vtt(base_out: Path) -> Optional[Tuple[Path, str]]:
    """Elige el mejor VTT espa√±ol disponible y devuelve (ruta, lang_code)."""
    candidates = _spanish_glob_candidates(base_out)
    if not candidates:
        return None

    def score(p: Path) -> Tuple[int, str]:
        lang = _lang_from_filename(p, base_out.name)
        for idx, tag in enumerate(LANG_PRIORITY):
            if lang == tag:
                return (idx, lang)
        if lang.startswith("es.") or lang.startswith("es-"):
            return (len(LANG_PRIORITY), lang)
        if lang == "es":
            return (2, lang)
        return (999, lang)

    ranked = sorted(((score(p), p) for p in candidates), key=lambda x: x[0])
    best = ranked[0][1]
    best_lang = _lang_from_filename(best, base_out.name)
    return best, best_lang

# =======================
# DESCARGA PRINCIPAL (con fallback)
# =======================
def download_spanish_with_ytdlp(url: str, base_out: Path) -> Optional[Tuple[Path, str, str]]:
    """
    1) Sonda -J para ver claves reales (subtitles + automatic_captions) y pide EXACTAMENTE las espa√±olas.
    2) Si no hay espa√±ol y FALLBACK_TO_ANY_AUTO=True: baja el primer auto-captions disponible
       (prioriza FALLBACK_AUTO_PRIORITY).
    3) Si aun as√≠ no hay nada usable, intenta rescate con --all-subs y filtra.
    Devuelve (ruta_vtt, lang_code, player_client) o None.
    """
    def probe_json(client: str) -> dict:
        cmd = [
            "yt-dlp",
            "--user-agent", USER_AGENT,
            "-J", url,
            "--extractor-args", f"youtube:player_client={client}",
        ]
        if COOKIES_FILE.exists():
            cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if p.returncode != 0:
            return {}
        try:
            return json.loads(p.stdout.decode("utf-8", "ignore"))
        except Exception:
            return {}

    def pick_spanish_keys(info: dict) -> List[str]:
        keys = set()
        for field in ("subtitles", "automatic_captions"):
            d = info.get(field) or {}
            for lang in d.keys():
                if lang == "es" or lang.startswith("es-") or lang.startswith("es.") or lang == "spa":
                    keys.add(lang)
        ordered = ["es-ES", "es-419", "es", *sorted([k for k in keys if k not in {"es-ES","es-419","es"}])]
        return [k for k in ordered if k in keys]

    def pick_fallback_auto_key(info: dict) -> Optional[str]:
        auto = info.get("automatic_captions") or {}
        if not auto:
            return None
        # 1) prioridad preferida
        for pref in FALLBACK_AUTO_PRIORITY:
            for lang in auto.keys():
                if lang == pref or lang.startswith(pref + ".") or (pref == "es" and (lang.startswith("es-") or lang.startswith("es."))):
                    return lang
        # 2) si no, la primera clave disponible
        for lang in auto.keys():
            return lang
        return None

    def run_download_with_langs(langs: List[str], client: str) -> Optional[Tuple[Path, str, str]]:
        cmd = [
            "yt-dlp",
            "--user-agent", USER_AGENT,
            "--skip-download",
            "--write-subs", "--write-auto-subs",
            "--sub-format", "vtt/srv3",
            "--convert-subs", "vtt",
            "--sub-langs", ",".join(langs),
            "--output", str(base_out) + ".%(ext)s",
            "--extractor-args", f"youtube:player_client={client}",
            url,
        ]
        if COOKIES_FILE.exists():
            cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            pick = _pick_best_spanish_vtt(base_out)
            if pick:
                vtt_path, lang_code = pick
                return (vtt_path, lang_code, client)
            # Si no hay .es*.vtt, quiz√° el idioma fue otro (fallback): busca cualquier .<lang>.vtt solicitado
            for lang in langs:
                maybe = base_out.with_name(base_out.name + f".{lang}.vtt")
                if maybe.exists():
                    return (maybe, lang, client)
        except subprocess.CalledProcessError:
            pass
        return None

    def all_subs_rescue(client: str) -> Optional[Tuple[Path, str, str]]:
        cmd = [
            "yt-dlp",
            "--user-agent", USER_AGENT,
            "--skip-download",
            "--all-subs",
            "--sub-format", "vtt/srv3",
            "--convert-subs", "vtt",
            "--output", str(base_out) + ".%(ext)s",
            "--extractor-args", f"youtube:player_client={client}",
            url,
        ]
        if COOKIES_FILE.exists():
            cmd = ["yt-dlp", "--cookies", str(COOKIES_FILE)] + cmd[1:]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            # 1¬∫ intenta espa√±ol
            pick = _pick_best_spanish_vtt(base_out)
            if pick:
                vtt_path, lang_code = pick
                return (vtt_path, lang_code, client)
            # 2¬∫ si no hay espa√±ol, cualquier vtt priorizado
            vtts = sorted(base_out.parent.glob(base_out.name + ".*.vtt"))
            langs_to_paths = {p.name[len(base_out.name)+1:-4]: p for p in vtts}
            for pref in FALLBACK_AUTO_PRIORITY:
                for lang, path in langs_to_paths.items():
                    if lang == pref or lang.startswith(pref + ".") or (pref == "es" and (lang.startswith("es-") or lang.startswith("es."))):
                        return (path, lang, client)
            if vtts:
                p = vtts[0]
                lang = p.name[len(base_out.name)+1:-4]
                return (p, lang, client)
        except subprocess.CalledProcessError:
            pass
        return None

    # === Flujo principal por clientes ===
    for client in PLAYER_CLIENTS:
        info = probe_json(client)
        # 1) Idiomas espa√±oles exactos
        es_langs = pick_spanish_keys(info)
        if es_langs:
            got = run_download_with_langs(es_langs, client)
            if got:
                return got

        # 2) Fallback: primer auto disponible (p.ej. 'en'), si est√° activado
        if FALLBACK_TO_ANY_AUTO:
            fb = pick_fallback_auto_key(info)
            if fb:
                got = run_download_with_langs([fb], client)
                if got:
                    return got

        # 3) Rescate --all-subs y luego elegir
        got = all_subs_rescue(client)
        if got:
            return got

    # Segundo intento de rescate global si nada funcion√≥
    for client in PLAYER_CLIENTS:
        got = all_subs_rescue(client)
        if got:
            return got

    return None

# =======================
# MAIN
# =======================
def main():
    if not PLAYLISTS_DIR.is_dir():
        print(f"ERROR: la carpeta '{PLAYLISTS_DIR}' no existe. Ejecuta antes el script de expansi√≥n.", file=sys.stderr)
        sys.exit(1)

    OUT_BASE_DIR.mkdir(parents=True, exist_ok=True)

    total = 0
    ok = 0
    skipped = 0
    failed = 0

    for playlist_title, playlist_safe, playlist_url, title, url, vid in iter_videos_from_playlists_folder(PLAYLISTS_DIR):
        total += 1

        playlist_dir = OUT_BASE_DIR / playlist_safe
        playlist_dir.mkdir(parents=True, exist_ok=True)

        base_name = safe_filename(title)
        base_out = playlist_dir / base_name     # sin extensi√≥n
        txt_path = playlist_dir / f"{base_name}.txt"
        log_path = playlist_dir / f"{base_name}.log"

        if txt_path.exists():
            print(f"[SKIP] ({playlist_title}) Ya existe: {txt_path.name}")
            continue

        print(f"[{total:03d}] [{playlist_title}] {title} -> {url}")

        # 1) Descargar subt√≠tulos (espa√±ol o fallback)
        pick = download_spanish_with_ytdlp(url, base_out)

        if not pick:
            skipped += 1
            # Para diagn√≥stico: listar subs disponibles por cada client
            lists = []
            for client in PLAYER_CLIENTS:
                ls = _list_subs(url, client)
                lists.append(f"\n--- --list-subs (client={client}) ---\n{ls}")
            log_path.write_text(
                f"No se pudieron obtener subt√≠tulos.\n"
                f"PLAYLIST:{playlist_title}\nPLAYLIST_URL:{playlist_url}\n"
                f"TITLE:{title}\nURL:{url}\nID:{vid}\n"
                f"Intentos: espa√±ol -> fallback_any_auto -> all-subs\n"
                f"{''.join(lists)}\n",
                encoding="utf-8"
            )
            print("  ¬∑ No hay subt√≠tulos utilizables (registrado .log)")
            time.sleep(SLEEP_BETWEEN)
            continue

        vtt_path, lang_code, client_used = pick

        # 2) Convertir VTT -> TEXTO S√ìLIDO
        try:
            solid = vtt_to_solid_text(vtt_path)
            if not solid:
                failed += 1
                log_path.write_text(
                    f"VTT vac√≠o o no legible tras limpieza.\n"
                    f"PLAYLIST:{playlist_title}\nPLAYLIST_URL:{playlist_url}\n"
                    f"TITLE:{title}\nURL:{url}\nID:{vid}\n"
                    f"VTT: {vtt_path.name} ({lang_code}) client={client_used}\n",
                    encoding="utf-8"
                )
                print("  ‚úó VTT vac√≠o/ilegible (registrado .log)")
            else:
                fallback_mark = ""
                if lang_code not in ("es", "es-ES", "es-419") and not lang_code.startswith("es-") and not lang_code.startswith("es."):
                    fallback_mark = " (fallback_any_auto)"
                header = [
                    f"PLAYLIST: {playlist_title}",
                    f"PLAYLIST_URL: {playlist_url}",
                    f"TITLE: {title}",
                    f"URL:   {url}",
                    f"ID:    {vid}",
                    f"SOURCE: yt-dlp [{lang_code}] client={client_used}{fallback_mark}",
                    "-" * 60,
                    ""
                ]
                txt_path.write_text("\n".join(header) + solid, encoding="utf-8")
                ok += 1
                print(f"  ‚úì Transcripci√≥n guardada (texto s√≥lido): {txt_path.name}")
        except Exception as e:
            failed += 1
            log_path.write_text(
                f"Error al convertir VTT->TXT: {e}\n"
                f"PLAYLIST:{playlist_title}\nPLAYLIST_URL:{playlist_url}\n"
                f"TITLE:{title}\nURL:{url}\nID:{vid}\n"
                f"VTT: {vtt_path.name} ({lang_code}) client={client_used}\n",
                encoding="utf-8"
            )
            print("  ‚úó Error VTT->TXT (registrado .log)")

        time.sleep(SLEEP_BETWEEN)

    print("\n=== RESUMEN ===")
    print(f"V√≠deos totales:   {total}")
    print(f"OK (transcritos): {ok}")
    print(f"Omitidos:         {skipped}  (sin subt√≠tulos utilizables)")
    print(f"Fallidos:         {failed}")


if __name__ == "__main__":
    main()
```

### resumenes
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este script en Python se encarga de generar res√∫menes en espa√±ol a partir de transcripciones de videos almacenadas en archivos `.txt` dentro de subcarpetas del directorio `transcripts/`. El programa recorre todas las transcripciones, lee el contenido de cada archivo `.txt`, y lo env√≠a a un servicio llamado ollama con un modelo espec√≠fico (`qwen2.5:3b-instruct`) para que genere un resumen en espa√±ol.

El script utiliza la funci√≥n `call_ollama_summarize` para enviar las transcripciones al servicio de ollama, donde se procesan y generan los res√∫menes. Estos res√∫menes siguen una estructura espec√≠fica: deben ser claros y concisos, contener entre 5 y 10 puntos clave en vi√±etas y finalizar con un breve p√°rrafo de s√≠ntesis.

Una vez generado el resumen, el script guarda este resultado en un archivo `.summary.txt` que se encuentra en la misma carpeta del archivo original. Si ya existe un archivo de resumen para un determinado transcript, el programa lo salta sin procesarlo nuevamente.

El objetivo principal es facilitar a los usuarios tener una visi√≥n general r√°pida y precisa de las transcripciones de los videos, ayudando as√≠ a la comprensi√≥n y an√°lisis del contenido de manera m√°s eficiente.

`003-resumenes.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Genera res√∫menes en espa√±ol de todas las transcripciones en la carpeta 'transcripts/'.

Estructura esperada:
transcripts/
  ‚îú‚îÄ‚îÄ subcarpeta1/
  ‚îÇ     ‚îú‚îÄ‚îÄ video1.txt
  ‚îÇ     ‚îú‚îÄ‚îÄ video2.txt
  ‚îÇ     ‚îî‚îÄ‚îÄ ...
  ‚îú‚îÄ‚îÄ subcarpeta2/
  ‚îÇ     ‚îú‚îÄ‚îÄ ...
  ‚îî‚îÄ‚îÄ ...

Para cada archivo .txt:
- Lee el contenido completo.
- Lo env√≠a a `ollama run qwen2.5:3b-instruct` para obtener un resumen en espa√±ol.
- Guarda el resultado en `<nombre_original>.summary.txt` en la misma carpeta.
- Si el archivo de resumen ya existe, lo salta.

Requisitos:
  - Tener instalado ollama y el modelo qwen2.5:3b-instruct:
      ollama pull qwen2.5:3b-instruct
"""

import os
import subprocess
from pathlib import Path

# Carpeta ra√≠z donde est√°n las transcripciones
TRANSCRIPTS_ROOT = Path("transcripts")

# Modelo de Ollama
OLLAMA_MODEL = "qwen2.5:3b-instruct"

# Longitud m√°xima aproximada del texto que se manda al modelo
# (por si tus transcripciones son muy largas; si quieres, pon None para desactivar)
MAX_CHARS = 150000


def call_ollama_summarize(text: str) -> str:
    """
    Llama a 'ollama run qwen2.5:3b-instruct' pasando el prompt por stdin
    y devuelve la respuesta en texto plano.
    """
    prompt = f"""
Eres un asistente experto en educaci√≥n y programaci√≥n que resume transcripciones de v√≠deos en espa√±ol.

A partir de la transcripci√≥n siguiente, escribe un resumen en ESPA√ëOL que:
- Sea claro y conciso.
- Use entre 5 y 10 vi√±etas con los puntos clave.
- Termine con un breve p√°rrafo final de s√≠ntesis.
- No invente contenido que no aparezca en la transcripci√≥n.
- Mantenga el tono docente y explicativo.

Transcripci√≥n:
\"\"\"{text}\"\"\"
"""

    result = subprocess.run(
        ["ollama", "run", OLLAMA_MODEL],
        input=prompt.encode("utf-8"),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )

    if result.returncode != 0:
        raise RuntimeError(
            f"Error ejecutando ollama:\nSTDERR:\n{result.stderr.decode('utf-8', 'ignore')}"
        )

    return result.stdout.decode("utf-8", "ignore").strip()


def process_transcript_file(txt_path: Path) -> None:
    """
    Lee un .txt de transcripci√≥n, lo resume con ollama y guarda el
    resultado en .summary.txt en la misma carpeta.
    """
    summary_path = txt_path.with_suffix(".summary.txt")

    if summary_path.exists():
        print(f"[SKIP] Ya existe resumen: {summary_path}")
        return

    print(f"[PROC] {txt_path}")

    content = txt_path.read_text(encoding="utf-8", errors="ignore").strip()

    if not content:
        print(f"  ¬∑ Archivo vac√≠o, se omite.")
        return

    # Si quieres recortar transcripciones muy largas para evitar superar contexto:
    if MAX_CHARS is not None and len(content) > MAX_CHARS:
        print(f"  ¬∑ Aviso: transcripci√≥n muy larga, recortando a {MAX_CHARS} caracteres.")
        content = content[:MAX_CHARS]

    try:
        summary = call_ollama_summarize(content)
    except Exception as e:
        print(f"  ‚úó Error al resumir con ollama: {e}")
        return

    header = (
        f"RESUMEN GENERADO CON OLLAMA ({OLLAMA_MODEL})\n"
        f"ORIGINAL: {txt_path.name}\n"
        f"{'-'*60}\n\n"
    )

    summary_path.write_text(header + summary + "\n", encoding="utf-8")
    print(f"  ‚úì Resumen guardado en: {summary_path}")


def main():
    if not TRANSCRIPTS_ROOT.is_dir():
        print(f"ERROR: No existe la carpeta '{TRANSCRIPTS_ROOT}'.")
        return

    print(f"Buscando transcripciones en: {TRANSCRIPTS_ROOT.resolve()}")

    count_total = 0
    count_done = 0

    # Recorre todas las subcarpetas y archivos
    for root, dirs, files in os.walk(TRANSCRIPTS_ROOT):
        root_path = Path(root)
        # Opcional: puedes saltarte carpetas de logs, etc., si quieres
        for fname in files:
            if not fname.lower().endswith(".txt"):
                continue
            if fname.lower().endswith(".summary.txt"):
                # No queremos re-resumir res√∫menes
                continue

            txt_path = root_path / fname
            count_total += 1
            process_transcript_file(txt_path)
            count_done += 1

    print("\n=== RESUMEN GLOBAL ===")
    print(f"Archivos .txt encontrados (sin contar .summary.txt): {count_total}")
    print(f"Procesados (intento de resumen): {count_done}")


if __name__ == "__main__":
    main()
```

### listas
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este fragmento de c√≥digo es un archivo JSON que contiene una lista. En este caso, la lista tiene solo un elemento: una URL que corresponde a una lista de reproducci√≥n de YouTube espec√≠fica. Esta URL est√° asociada al identificador de video "4ouGatnuNKY" y a la lista de reproducci√≥n con el ID "PLWKjZdWQCDC64UcPIMiF2YplFcKG2EWej". El archivo JSON es com√∫nmente utilizado para almacenar datos estructurados de manera que sean f√°cilmente legibles tanto por humanos como por m√°quinas. En este contexto, el archivo podr√≠a ser parte de un conjunto de datos utilizados por scripts o programas Python (como los archivos mencionados: `001-videos.py`, `002-transcripciones.py` y `003-resumenes.py`) para descargar videos de YouTube, procesar sus transcripciones, resumirlos u otras operaciones relacionadas. La importancia reside en que proporciona una forma ordenada e inmediatamente comprensible de almacenar datos necesarios para estos scripts ejecutar correctamente su tarea.

`listas.json`

```json
[
  "https://www.youtube.com/watch?v=4ouGatnuNKY&list=PLWKjZdWQCDC64UcPIMiF2YplFcKG2EWej"
]
```

### Actividades propuestas

Este es un conjunto de scripts y archivos para automatizar el proceso de transcripci√≥n, resumen e indexaci√≥n de listas de reproducci√≥n completas de YouTube en espa√±ol. Aqu√≠ tienes una explicaci√≥n detallada de cada parte:

1. **listas.json**:
   - Contiene URLs de listas de reproducci√≥n de YouTube que se van a procesar.

2. **001-lista-videos.py**:
   - Script que extrae los datos de las listas de reproducci√≥n y genera una lista de v√≠deos con su ID, t√≠tulo, URL y detalles adicionales.
   - Guarda esta informaci√≥n en archivos CSV (`videos.csv`) y JSON (`videos.json`).

3. **002-transcribir-videos.py**:
   - Script que descarga transcripciones subtituladas (VTT) de los v√≠deos desde YouTube utilizando `yt-dlp`.
   - Convierte las transcripciones VTT a texto s√≥lido y genera res√∫menes b√°sicos.
   - Guarda el texto s√≥lido en archivos `.txt` dentro de carpetas seg√∫n la lista de reproducci√≥n.

4. **003-resumenes.py**:
   - Script que toma los archivos `.txt` generados por `002-transcribir-videos.py`, utiliza un modelo LLM (Large Language Model) como Qwen para generar res√∫menes en espa√±ol.
   - Guarda estos res√∫menes en archivos `.summary.txt`.

### Proceso Completo

1. **Obtenci√≥n de listas de v√≠deos**:
   ```python
   python 001-lista-videos.py --input listas.json
   ```
   Este script generar√° un archivo `videos.csv` con todos los detalles de la lista de reproducci√≥n proporcionada en `listas.json`.

2. **Transcripci√≥n y Resumen Inicial**:
   ```python
   python 002-transcribir-videos.py --input videos.csv
   ```
   Este script descarga las transcripciones subtituladas (VTT) y genera un resumen b√°sico.

3. **Generaci√≥n de Res√∫menes Mejorados**:
   ```python
   python 003-resumenes.py
   ```
   Este script toma los archivos `.txt` generados en el paso anterior, utiliza un modelo LLM para generar res√∫menes detallados y guarda estos res√∫menes en archivos `.summary.txt`.

### Requisitos

- `yt-dlp`: Herramienta de l√≠nea de comandos para descargar contenido de YouTube.
  ```sh
  pip install yt-dlp
  ```
- `ollama`: Cliente para modelos LLM como Qwen. Necesita estar configurado y tener acceso al modelo descrito en `003-resumenes.py`.
  
### Consideraciones Adicionales

- **Velocidad**: Los scripts pueden ser lentos dependiendo del n√∫mero de v√≠deos y la velocidad de Internet.
- **Sistema Operativo**: Estos scripts est√°n dise√±ados para sistemas Unix/Linux, aunque tambi√©n funcionar√°n en Windows con ajustes menores.
- **Autenticaci√≥n/Permisos**: Aseg√∫rate de tener permisos adecuados para ejecutar estos scripts y acceder a los recursos necesarios.

Este conjunto de herramientas proporciona una forma eficiente de recopilar, transcribir, resumir y organizar contenido educativo en YouTube.


<a id="generador-web-ia"></a>
## generador web IA

### Introducci√≥n a los ejercicios

Este conjunto de ejercicios se centra en la creaci√≥n de una aplicaci√≥n web que utiliza inteligencia artificial para generar p√°ginas HTML y CSS basadas en un texto de entrada proporcionado por el usuario. El ejercicio principal, contenido en el archivo `app.py`, muestra c√≥mo implementar una interfaz web simple usando Flask, una biblioteca popular para Python. Los estudiantes aprender√°n a manejar solicitudes POST, interactuar con APIs externas mediante la librer√≠a `requests` y responder con datos JSON.

A trav√©s de este ejercicio, los estudiantes practicar√°n habilidades fundamentales como el desarrollo backend en Python, el uso de marcos web como Flask para crear aplicaciones web din√°micas, y c√≥mo integrar servicios de inteligencia artificial en sus proyectos.

### app
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo es un programa en Python que utiliza el framework Flask para crear una aplicaci√≥n web simple. La funci√≥n principal de esta aplicaci√≥n es recibir solicitudes con un texto o "prompt" y generar HTML y CSS utilizando un modelo de inteligencia artificial llamado Ollama.

El archivo empieza importando las bibliotecas necesarias, como `Flask` y `requests`, que son fundamentales para la comunicaci√≥n web. Se inicializa una instancia de Flask y se define la URL del servidor local donde est√° corriendo el servicio Ollama, as√≠ como el nombre del modelo de inteligencia artificial utilizado (`qwen2.5-coder:7b`).

La funci√≥n `call_ollama()` toma un prompt (un texto que describe lo que se quiere generar) y env√≠a una solicitud HTTP a Ollama para obtener la respuesta generada por el modelo. Si todo va bien, devuelve el HTML y CSS generado; si hay alg√∫n error en la llamada a Ollama, retorna una p√°gina de error sencilla con detalles t√©cnicos.

Las rutas `/` y `/generate` definen c√≥mo maneja la aplicaci√≥n las peticiones web entrantes. La ruta `/` simplemente muestra un archivo `index.html`, mientras que la ruta `/generate` recibe un prompt a trav√©s del m√©todo POST, lo llama a Ollama para generar el HTML correspondiente y luego devuelve este contenido en formato JSON.

Este c√≥digo es importante porque demuestra c√≥mo se integra una aplicaci√≥n web con servicios de inteligencia artificial para automatizar la creaci√≥n de contenidos web, como HTML y CSS.

`app.py`

```python
from flask import Flask, render_template, request, jsonify
import requests

app = Flask(__name__)

OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "qwen2.5-coder:7b"


def call_ollama(prompt: str) -> str:
    """
    Call local Ollama with Qwen2.5-Coder and return the generated HTML+CSS.
    """
    # You can tune this system-style prefix to control what the model outputs
    system_instruction = """
You are an AI that generates HTML and CSS only.

Requirements:
- Return a complete HTML document.
- Include CSS either inline or inside a <style> tag in the <head>.
- Do NOT include any <script> tags or JavaScript code.
- Do NOT include explanations or comments in natural language.
- Just respond with the pure HTML (and embedded CSS).
"""

    full_prompt = f"""{system_instruction}

User requirement:
{prompt}
"""

    payload = {
        "model": MODEL_NAME,
        "prompt": full_prompt,
        "stream": False
    }

    try:
        resp = requests.post(OLLAMA_URL, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
        # Ollama's /generate returns {"response": "...", "done": true, ...}
        return data.get("response", "").strip()
    except Exception as e:
        print("Error calling Ollama:", e)
        # Fallback: simple error page in HTML
        return f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Error</title>
    <style>
        body {{ font-family: sans-serif; background:#2b2b2b; color:#f5f5f5; padding:2rem; }}
        .error {{ color:#ff6b6b; font-weight:bold; }}
    </style>
</head>
<body>
    <h1>Ooops‚Ä¶</h1>
    <p class="error">No se ha podido generar la p√°gina con la IA.</p>
    <p>Detalles t√©cnicos: {e}</p>
</body>
</html>
        """


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/generate", methods=["POST"])
def generate():
    data = request.get_json(force=True)
    prompt = data.get("prompt", "").strip()
    if not prompt:
        return jsonify({"error": "Empty prompt"}), 400

    html = call_ollama(prompt)
    return jsonify({"html": html})


if __name__ == "__main__":
    # Run Flask dev server
    app.run(host="0.0.0.0", port=5000, debug=True)
```

### Actividades propuestas

### Actividad 1: Implementaci√≥n de una Aplicaci√≥n Web Simp√°tica

**Descripci√≥n:** Los estudiantes deben crear una aplicaci√≥n web similar a la proporcionada en el c√≥digo, pero centrada en otro tema o funci√≥n. Se espera que los alumnos aprendan a implementar rutas de Flask y c√≥mo interactuar con APIs externas para generar contenido din√°mico.

### Actividad 2: Personalizaci√≥n del Dise√±o

**Descripci√≥n:** Los estudiantes deben modificar el dise√±o del sitio web generado, cambiando estilos CSS existentes o a√±adiendo nuevos elementos visuales. Se espera que aprendan a manipular y personalizar las hojas de estilo en HTML/CSS.

### Actividad 3: Implementaci√≥n de Formularios

**Descripci√≥n:** Los estudiantes deben agregar un formulario en la p√°gina web para permitir a los usuarios enviar solicitudes o propuestas al servidor. Se espera que comprendan c√≥mo crear formularies en HTML y manejar eventos con Flask.

### Actividad 4: Manejo de Errores Personalizados

**Descripci√≥n:** Los alumnos deber√°n implementar un sistema m√°s robusto para manejar errores y mostrar mensajes amigables a los usuarios cuando se produzcan problemas t√©cnicos. Se espera que aprendan a controlar excepciones en Python y mejorar la experiencia del usuario.

### Actividad 5: Integraci√≥n de JavaScript B√°sico

**Descripci√≥n:** Aunque el c√≥digo original no incluye JavaScript, los estudiantes deben aprender a integrarlo de forma segura para a√±adir interactividad b√°sica al sitio web. Se espera que comprendan las diferencias entre HTML/CSS y JavaScript y c√≥mo interactuar con ellos.

### Actividad 6: Creaci√≥n de una Interfaz en Tiempo Real

**Descripci√≥n:** Los estudiantes deben mejorar la comunicaci√≥n del servidor con el cliente para permitir actualizaciones en tiempo real. Esto podr√≠a implicar el uso de WebSocket o AJAX (Asynchronous JavaScript and XML) junto con Flask. Se espera que aprendan a implementar comunicaciones as√≠ncronas.

### Actividad 7: Optimizaci√≥n y Mejora de Rendimiento

**Descripci√≥n:** Los alumnos deben analizar el c√≥digo para identificar posibles mejoras en rendimiento, como la reducci√≥n del tiempo de respuesta o minimizando los recursos necesarios. Se espera que aprendan t√©cnicas b√°sicas de optimizaci√≥n.

### Actividad 8: Documentaci√≥n y Explicaci√≥n de C√≥digos

**Descripci√≥n:** Los estudiantes deben escribir documentaci√≥n clara para cada parte del c√≥digo, incluyendo comentarios en el c√≥digo fuente. Esto ayudar√° a otros desarrolladores a entender f√°cilmente c√≥mo funciona la aplicaci√≥n. Se espera que aprendan las buenas pr√°cticas de codificaci√≥n y documentaci√≥n.

### Actividad 9: Testing Unitario

**Descripci√≥n:** Los estudiantes deben crear pruebas unitarias para diferentes partes del proyecto, especialmente aquellas relacionadas con rutas de Flask o funciones cr√≠ticas. Se espera que comprendan la importancia de las pruebas en el desarrollo de software y aprendan a escribir pruebas eficientes.

### Actividad 10: Creaci√≥n de una Aplicaci√≥n M√≥vil

**Descripci√≥n:** Los alumnos deben crear una versi√≥n m√≥vil del sitio web utilizando HTML5, CSS3 y JavaScript (o frameworks como React Native o Ionic). Se espera que aprendan las diferencias en la estructura y estilos para dispositivos m√≥viles y c√≥mo adaptar el dise√±o existente para mejorar la experiencia en tel√©fonos y tablets.


<a id="entrenamiento-ia"></a>
## Entrenamiento IA

### Introducci√≥n a los ejercicios

Los archivos proporcionados contienen un conjunto completo de instrucciones y datos necesarios para realizar tareas relacionadas con el aprendizaje y la aplicaci√≥n de inteligencia artificial. Aqu√≠ te presento una descripci√≥n detallada del contenido y su prop√≥sito:

### Archivos

1. **`README.md`**
   - **Descripci√≥n:** Este archivo proporciona un resumen general de los archivos y c√≥mo usarlos para aplicar el aprendizaje autom√°tico a la informaci√≥n sobre Jose Vicente Carratal√°.
   - **Contenido:**
     - Pasos para configurar entornos de desarrollo.
     - Instrucciones para realizar pruebas y experimentos con modelos de lenguaje.
     - Gu√≠a sobre c√≥mo procesar el conjunto de datos proporcionado.

2. **`requirements.txt`**
   - **Descripci√≥n:** Este archivo lista las dependencias necesarias para ejecutar los scripts relacionados con la IA.
   - **Contenido:**
     - Paquetes como `transformers`, `torch`, `accelerate`, `datasets`.
     - Versi√≥n espec√≠fica de Python recomendada (por ejemplo, 3.10).

3. **`app.py`**
   - **Descripci√≥n:** Este archivo es un script que permite interactuar con el modelo entrenado para responder preguntas sobre Jose Vicente Carratal√°.
   - **Contenido:**
     - Importaciones de bibliotecas necesarias.
     - Definici√≥n de funciones para cargar y utilizar el modelo.
     - Mecanismo para hacer consultas y obtener respuestas.

4. **`data.txt`**
   - **Descripci√≥n:** Este archivo contiene las entradas y salidas del chat que sirven como conjunto de datos para entrenar un modelo de lenguaje.
   - **Contenido:**
     - Ejemplos estructurados en formato JSON con roles (user, assistant).

5. **`data.json`**
   - **Descripci√≥n:** Este archivo almacena el mismo conjunto de datos pero en formato JSON, facilitando la manipulaci√≥n y procesamiento de los datos.
   - **Contenido:**
     - Ejemplos estructurados en formato JSON con roles (user, assistant).

### Uso del Conjunto de Datos

El archivo `data.txt` y `data.json` contienen una serie de intercambios entre el usuario y un asistente que proporcionan informaci√≥n detallada sobre Jose Vicente Carratal√°. Cada entrada es estructurada como sigue:

```json
{
  "messages": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ]
}
```

### Proceso de Entrenamiento y Evaluaci√≥n

1. **Configurar el Ambiente:**
   - Instalar las dependencias descritas en `requirements.txt`.
     ```bash
     pip install -r requirements.txt
     ```

2. **Procesar Datos:**
   - Convertir los datos proporcionados a un formato compatible con la biblioteca Hugging Face.
   - Preprocesamiento de textos y tokenizaci√≥n.

3. **Entrenamiento del Modelo:**
   - Utilizar `app.py` para cargar el modelo, entrenarlo con el conjunto de datos y realizar pruebas.
     ```python
     from app import main

     if __name__ == "__main__":
         main()
     ```

### Interacci√≥n Con El Modelo Entrenado

Una vez que se ha completado el proceso de entrenamiento, `app.py` permite interactuar con el modelo para responder a preguntas sobre Jose Vicente Carratal√°. Ejemplo:

```python
from app import ChatModel

model = ChatModel()
response = model.query("¬øQu√© cursos ha creado Jose Vicente Carratal√°?")
print(response)
```

### Resultados Esperados

El objetivo es que el modelo entrenado sea capaz de proporcionar respuestas precisas y coherentes a preguntas similares a las presentes en el conjunto de datos. Esto permitir√° una interacci√≥n fluida e informativa con la informaci√≥n sobre Jose Vicente Carratal√°.

### Conclusiones

Este conjunto de archivos provee un marco completo para aprender y aplicar t√©cnicas de aprendizaje autom√°tico, espec√≠ficamente centrado en el entrenamiento de modelos basados en texto. Permite a los usuarios entender c√≥mo procesar datos, realizar ajustes finos a modelos existentes y evaluar sus capacidades mediante consultas interactivas.

Si tienes alguna pregunta o necesitas m√°s detalles sobre cualquier parte del proceso, por favor av√≠same!

### entrenar
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo es un script de Python dise√±ado para entrenar un modelo de lenguaje con datos espec√≠ficos utilizando la biblioteca `transformers` de Hugging Face. El objetivo principal del script es cargar un conjunto de datos, prepararlo y luego entrenar el modelo en √©l.

### Configuraci√≥n y Carga de Datos
El script comienza estableciendo algunas variables de configuraci√≥n como el camino al archivo de los datos (`DATA_PATH`), el nombre del modelo base que se utilizar√° para el entrenamiento (`MODEL_NAME`), y la ruta donde se guardar√° el modelo entrenado (`OUTPUT_DIR`). Luego, verifica si hay una GPU disponible (CUDA) para el entrenamiento y ajusta par√°metros seg√∫n sea necesario.

El script luego carga los datos de un archivo JSONL usando `datasets.load_dataset`, que es una utilidad proporcionada por Hugging Face para cargar f√°cilmente conjuntos de datos. Despu√©s de esto, se carga el tokenizer correspondiente al modelo base especificado y se asegura de que tenga definido el token de padding necesario.

### Preparaci√≥n del Modelo
Si la GPU est√° disponible, el script configura el modelo para usar una t√©cnica llamada QLoRA en 4 bits, lo cual es m√°s eficiente en t√©rminos de uso de memoria. Si no hay GPU, se realiza un entrenamiento cl√°sico con precisi√≥n float32 en CPU.

### Conversi√≥n y Tokenizaci√≥n
El c√≥digo convierte los datos en texto plano utilizando una plantilla de chat, que es √∫til para modelos dise√±ados para entender interacciones de tipo "pregunta-respuesta". Luego, estos datos se tokenizan prepar√°ndolos para el entrenamiento. La funci√≥n `tokenize_fn` asegura que cada ejemplo del conjunto de datos est√© limitado a un m√°ximo de 512 tokens y est√° correctamente marcado para los labels.

### Configuraci√≥n del Entrenador
Finalmente, el script configura las opciones de entrenamiento como la ruta de salida, los epochs, tama√±o del batch, tasa de aprendizaje y otros hiperpar√°metros. Se crea un objeto `Trainer` con estas configuraciones y se ejecuta el m√©todo `.train()` para iniciar el proceso de entrenamiento.

Una vez que el modelo est√° entrenado, este script lo guarda en la ruta especificada junto con el tokenizer utilizado durante el proceso, permitiendo as√≠ su uso posterior sin necesidad de volver a entrenarlo desde cero. Este tipo de scripts son fundamentales cuando se trabaja con aprendizaje autom√°tico y procesamiento del lenguaje natural, especialmente para proyectos interdisciplinarios como el Entrenamiento IA que parece ser este caso.

`entrenar.py`

```python
#!/usr/bin/env python3
import os
import json
from dataclasses import dataclass

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# -------------------------------------------------------------------
# CONFIGURACI√ìN B√ÅSICA
# -------------------------------------------------------------------

DATA_PATH = "training_data.jsonl"
# Modelo peque√±o y abierto de Qwen2.5 (aj√∫stalo si quieres otro)
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = "./qwen25-05b-jvc"

MAX_LENGTH = 512        # suficiente para ejemplos cortos
NUM_EPOCHS = 3
LR = 2e-4
BATCH_SIZE = 1
GRAD_ACCUM = 4          # batch efectivo = 4


def main():
    print("üöÄ Inicio de entrenamiento Qwen2.5 (Python puro)")
    print(f"üìÅ Usando dataset: {DATA_PATH}")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print("-" * 60)

    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"No se encontr√≥ el dataset en {DATA_PATH}")

    # -------------------------------------------------------------------
    # DETECTAR DISPOSITIVO
    # -------------------------------------------------------------------
    if torch.cuda.is_available():
        device = "cuda"
        use_4bit = True
        print("üíª GPU detectada (CUDA). Entrenando con QLoRA 4-bit.")
    else:
        device = "cpu"
        use_4bit = False
        print("üíª No hay GPU CUDA. Entrenando en CPU en float32 (esto ser√° m√°s lento).")

    # -------------------------------------------------------------------
    # CARGAR DATASET
    # -------------------------------------------------------------------
    print("üì• Cargando dataset con datasets.load_dataset(...)")
    raw_dataset = load_dataset(
        "json",
        data_files=DATA_PATH,
        split="train"
    )
    print(f"‚úÖ Dataset cargado con {len(raw_dataset)} ejemplos.")

    # -------------------------------------------------------------------
    # CARGAR TOKENIZER Y MODELO
    # -------------------------------------------------------------------
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=True
    )

    # Asegurar que tenemos pad_token definido
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ Cargando modelo base...")
    if use_4bit:
        # QLoRA 4-bit en GPU
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        model = prepare_model_for_kbit_training(model)

        # Configuraci√≥n LoRA
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    else:
        # Entrenamiento cl√°sico en CPU
        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
        model.to(device)

    # -------------------------------------------------------------------
    # CONVERTIR messages ‚Üí texto usando plantilla de chat
    # -------------------------------------------------------------------
    print("üß± Transformando mensajes (user/assistant) en texto plano con plantilla de chat...")

    def messages_to_text(example):
        """
        Convierte el campo 'messages' del JSONL en un √∫nico texto
        usando la plantilla de chat del tokenizer.
        """
        conv = []
        for m in example["messages"]:
            role = m.get("role", "user")
            if role not in ("user", "assistant", "system"):
                role = "user"
            conv.append({"role": role, "content": m["content"]})

        try:
            text = tokenizer.apply_chat_template(
                conv,
                tokenize=False,
                add_generation_prompt=False,
            )
        except Exception:
            # Fallback sencillo si falla apply_chat_template
            partes = []
            for m in conv:
                if m["role"] == "user":
                    prefix = "Usuario"
                elif m["role"] == "assistant":
                    prefix = "Asistente"
                else:
                    prefix = m["role"]
                partes.append(f"{prefix}: {m['content']}")
            text = "\n".join(partes)
        return {"text": text}

    text_dataset = raw_dataset.map(
        messages_to_text,
        remove_columns=raw_dataset.column_names
    )

    # -------------------------------------------------------------------
    # TOKENIZACI√ìN
    # -------------------------------------------------------------------
    print("‚úÖ Tokenizando dataset...")

    def tokenize_fn(batch):
        out = tokenizer(
            batch["text"],
            truncation=True,
            max_length=MAX_LENGTH,
            padding="max_length",
        )
        out["labels"] = out["input_ids"].copy()
        return out

    tokenized_dataset = text_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=["text"]
    )

    # -------------------------------------------------------------------
    # TRAINING ARGUMENTS (SIN evaluation_strategy NI HISTORIAS RARAS)
    # -------------------------------------------------------------------
    print("‚úÖ Configurando argumentos de entrenamiento...")

    # Solo uno de fp16 / bf16 puede ser True. Usamos fp16 en GPU, nada en CPU.
    use_fp16 = (device == "cuda")
    use_bf16 = False

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LR,
        weight_decay=0.01,
        warmup_ratio=0.03,
        logging_steps=1,
        save_steps=20,
        save_total_limit=1,
        fp16=use_fp16,
        bf16=use_bf16,
        dataloader_pin_memory=False,
        report_to="none",  # sin wandb ni nada
    )

    # -------------------------------------------------------------------
    # TRAINER
    # -------------------------------------------------------------------
    print("‚úÖ Creando Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )

    # -------------------------------------------------------------------
    # ENTRENAR
    # -------------------------------------------------------------------
    print("üöÇ Comenzando entrenamiento...")
    trainer.train()
    print("üèÅ Entrenamiento terminado.")

    # -------------------------------------------------------------------
    # GUARDAR MODELO Y TOKENIZER
    # -------------------------------------------------------------------
    print("üíæ Guardando modelo y tokenizer en", OUTPUT_DIR)
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("‚úÖ Todo listo.")


if __name__ == "__main__":
    main()
```

### probar
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo es un script en Python dise√±ado para probar y evaluar el rendimiento de un modelo de lenguaje entrenado previamente, espec√≠ficamente Qwen2.5-0.5B-Instruct con posibles ajustes (LoRA) aplicados. El script se enfoca en cargar los recursos necesarios del modelo base, as√≠ como cualquier modificaci√≥n realizada a trav√©s del sistema LoRA si est√° disponible.

En resumen, el c√≥digo hace lo siguiente:
1. **Configuraci√≥n y Carga de Dependencias**: Importa las bibliotecas necesarias e inicializa variables clave para la configuraci√≥n del modelo (nombre del modelo base, directorio donde se guardan los ajustes, etc.).

2. **Funciones Principales**:
   - `print_header()`: Muestra un encabezado informativo con detalles sobre el modelo y la carpeta de ajuste.
   - `detect_device()`: Detecta si hay una GPU disponible para utilizarla durante las operaciones computacionales intensivas, lo que mejora significativamente el rendimiento en comparaci√≥n con usar solo una CPU. Ajusta los tipos de datos seg√∫n corresponda (float16 para GPUs y float32 para CPUs).
   - `load_tokenizer()`: Carga un tokenizer preentrenado necesario para procesar texto en el formato requerido por el modelo.
   - `load_base_model()`: Carga el modelo base desde los pesos preentrenados especificados.
   - `load_with_lora_if_possible()`: Intenta cargar cualquier ajuste LoRA (si est√° disponible y configurado correctamente) sobre el modelo base para mejorar sus capacidades espec√≠ficas. Si no se encuentra, simplemente carga el directorio como un nuevo modelo completo si ese fue el caso del entrenamiento.

3. **Interacci√≥n con el Usuario**: Permite al usuario ingresar consultas en espa√±ol interactivamente y obtiene respuestas generadas por el modelo ajustado, proporcionando una forma sencilla de evaluar y probar el rendimiento del modelo en tareas espec√≠ficas (como la respuesta a preguntas sobre programaci√≥n).

Este script es √∫til para estudiantes y profesionales que desean verificar c√≥mo un modelo ha mejorado despu√©s de ser entrenado con datos adicionales o ajustes, especialmente en contextos educativos donde se necesitan respuestas claras y concisas.

`probar.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

try:
    from peft import PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False


# ==========================
# CONFIGURACI√ìN
# ==========================
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"        # mismo modelo base que en entrenar.py
LORA_DIR   = "./qwen25-05b-jvc"             # carpeta donde entrenar.py guard√≥ el LoRA o el modelo
MAX_NEW_TOKENS = 256
TEMPERATURE    = 0.7
TOP_P          = 0.9


def print_header():
    print("üöÄ Test de modelo Qwen2.5 fine-tune (LoRA)")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print(f"üìÅ Carpeta LoRA / modelo: {LORA_DIR}")
    print("-" * 60)


def detect_device():
    if torch.cuda.is_available():
        device = "cuda"
        torch_dtype = torch.float16
        print("üíª GPU detectada (CUDA). Usando GPU con float16.")
    else:
        device = "cpu"
        torch_dtype = torch.float32
        print("üíª No se detecta GPU. Usando CPU con float32.")
    return device, torch_dtype


def load_tokenizer(model_name):
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # Por si el tokenizer no tiene pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


def load_base_model(model_name, device, torch_dtype):
    print("‚úÖ Cargando modelo base...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch_dtype,
        device_map="auto" if device == "cuda" else None,
    )
    if device == "cpu":
        model.to(device)
    return model


def load_with_lora_if_possible(base_model, lora_dir, device):
    """
    Intenta cargar adaptador LoRA desde lora_dir.
    - Si hay adapter_config.json, se asume que es un LoRA PEFT.
    - Si no, se asume que lora_dir es un modelo HF completo (modelo fusionado).
    """
    if not os.path.isdir(lora_dir):
        print(f"‚ùå La carpeta '{lora_dir}' NO existe.")
        print("   Revisa en entrenar.py cu√°l fue el 'output_dir' / carpeta donde se guard√≥ el LoRA o el modelo.")
        sys.exit(1)

    adapter_config_path = os.path.join(lora_dir, "adapter_config.json")

    # Caso 1: LoRA PEFT (adapter_config.json presente)
    if os.path.exists(adapter_config_path):
        if not PEFT_AVAILABLE:
            print("‚ùå Se ha encontrado un adaptador LoRA (adapter_config.json),")
            print("   pero el paquete 'peft' no est√° instalado.")
            print("   Inst√°lalo con: pip install peft")
            sys.exit(1)

        print(f"‚úÖ Encontrado adaptador LoRA en: {lora_dir}")
        from peft import PeftModel

        model = PeftModel.from_pretrained(
            base_model,
            lora_dir,
            device_map="auto" if device == "cuda" else None,
        )
        # Opcional: fusionar LoRA para inferencia
        try:
            model = model.merge_and_unload()
            print("‚úÖ LoRA fusionado con el modelo base para inferencia.")
        except Exception:
            print("‚ÑπÔ∏è No se ha fusionado el LoRA (merge_and_unload fall√≥ o no est√° disponible).")
        return model

    # Caso 2: No hay adapter_config.json ‚Üí intentar cargar lora_dir como modelo completo
    print("‚ö†Ô∏è No se encontr√≥ 'adapter_config.json' en la carpeta LoRA.")
    print("   Se intentar√° cargar la carpeta como un modelo completo (modelo ya fusionado).")
    print("   Si esto falla, revisa que entrenar.py est√© llamando a model.save_pretrained(LORA_DIR).")

    try:
        model = AutoModelForCausalLM.from_pretrained(
            lora_dir,
            torch_dtype=base_model.dtype,
            device_map="auto" if device == "cuda" else None,
        )
        if device == "cpu":
            model.to(device)
        print("‚úÖ Modelo cargado directamente desde la carpeta.")
        return model
    except Exception as e:
        print("‚ùå No se ha podido cargar la carpeta como modelo completo.")
        print(f"   Error: {e}")
        sys.exit(1)


def build_prompt(tokenizer, user_text: str) -> str:
    """
    Construye el texto de entrada siguiendo la plantilla de chat de Qwen, si existe.
    Si no, usa un formato sencillo tipo chat.
    """
    if hasattr(tokenizer, "apply_chat_template"):
        messages = [
            {"role": "system", "content": "Eres un profesor de programaci√≥n llamado Jose Vicente. Explicas en espa√±ol de forma clara y did√°ctica."},
            {"role": "user",   "content": user_text},
        ]
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    else:
        # fallback simple
        return f"Sistema: Eres un profesor de programaci√≥n llamado Jose Vicente.\nUsuario: {user_text}\nAsistente:"


def generate_answer(tokenizer, model, device: str, prompt: str) -> str:
    model.eval()
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
    )

    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Nos quedamos solo con la parte generada (sin el prompt) para que sea m√°s limpio
    generated_ids = output_ids[0][inputs["input_ids"].shape[-1]:]
    text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return text.strip()


def main():
    print_header()

    device, torch_dtype = detect_device()

    # 1. Tokenizer y modelo base
    tokenizer = load_tokenizer(MODEL_NAME)
    base_model = load_base_model(MODEL_NAME, device, torch_dtype)

    # 2. Cargar LoRA o modelo completo desde LORA_DIR
    model = load_with_lora_if_possible(base_model, LORA_DIR, device)

    # 3. Bucle de prueba interactivo
    print("\n‚úÖ Modelo listo para probar.")
    print("Escribe una pregunta (en espa√±ol). Ctrl+C o l√≠nea vac√≠a para salir.\n")

    try:
        while True:
            user_text = input("üí¨ T√∫: ").strip()
            if not user_text:
                print("üëã Saliendo.")
                break

            prompt = build_prompt(tokenizer, user_text)
            print("\nü§ñ Modelo (pensando)...\n")
            answer = generate_answer(tokenizer, model, device, prompt)
            print("ü§ñ Respuesta:\n")
            print(answer)
            print("\n" + "-" * 60 + "\n")

    except KeyboardInterrupt:
        print("\nüëã Saliendo por Ctrl+C.")


if __name__ == "__main__":
    main()
```

### training_data
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este fragmento de c√≥digo es una secuencia de conversaciones entre un usuario (rol: "user") que hace preguntas sobre Jose Vicente Carratal√° Sanchis y un asistente (rol: "assistant") que proporciona respuestas detalladas. Cada entrada en el archivo contiene varias interacciones donde se abordan diferentes aspectos de la vida profesional, acad√©mica y personal de Jose Vicente. Estas preguntas y respuestas cubren desde su papel como docente hasta sus habilidades t√©cnicas y proyectos recientes.

El formato del c√≥digo es una serie de objetos JSON anidados dentro de un array llamado "messages". Cada objeto representa una conversaci√≥n completa entre el usuario y el asistente. El objetivo principal de este tipo de estructura es proporcionar informaci√≥n sobre Jose Vicente en varios temas, incluyendo su formaci√≥n acad√©mica, sus proyectos tecnol√≥gicos y educativos, as√≠ como los recursos y herramientas que utiliza para ense√±ar y gestionar proyectos.

Este archivo JSONl (JSON Lines) podr√≠a ser utilizado por un sistema de entrenamiento de inteligencia artificial o una base de datos de preguntas y respuestas para proporcionar informaci√≥n precisa sobre Jose Vicente Carratal√° Sanchis, lo cual es √∫til tanto para estudiantes como para profesionales interesados en su trabajo.

`training_data.jsonl`

```
{"messages":[
  {"role":"user","content":"¬øQui√©n es Jose Vicente Carratal√° Sanchis?"},
  {"role":"assistant","content":"Jose Vicente Carratal√° Sanchis es un docente y desarrollador espa√±ol especializado en programaci√≥n, ciclos formativos de DAM y DAW, desarrollo de software empresarial, sistemas de informaci√≥n y proyectos de inteligencia artificial. Tambi√©n es autor de libros t√©cnicos, creador de cursos online y responsable del ecosistema Jocarsa de soluciones empresariales."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© tipo de contenido crea Jose Vicente?"},
  {"role":"assistant","content":"Jose Vicente produce contenido educativo sobre programaci√≥n, bases de datos, dise√±o web, inteligencia artificial, 3D, sistemas operativos y desarrollo empresarial. Ha creado miles de v√≠deos en YouTube, libros t√©cnicos, cursos completos, proyectos did√°cticos y herramientas para automatizar tareas docentes."}
]}
{"messages":[
  {"role":"user","content":"¬øA qu√© se dedica profesionalmente Jose Vicente Carratal√°?"},
  {"role":"assistant","content":"Jose Vicente trabaja como profesor en ciclos formativos de inform√°tica (DAM, DAW) y desarrolla software empresarial. Tambi√©n dirige proyectos como Jocarsa, crea soluciones SaaS, plataformas educativas, sistemas de evaluaci√≥n autom√°ticos y experimenta con IA generativa y modelos LLM."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© es Jocarsa y qu√© relaci√≥n tiene Jose Vicente con esa marca?"},
  {"role":"assistant","content":"Jocarsa es una marca creada por Jose Vicente Carratal√° dedicada al desarrollo de software de gesti√≥n empresarial (ERP, CRM, herramientas online, BI, mapas mentales, productividad, servidores...). Jose Vicente es el fundador y desarrolla tanto el backend como el frontend, la infraestructura y la documentaci√≥n t√©cnica y comercial."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© tipo de proyectos desarrolla Jose Vicente en clase?"},
  {"role":"assistant","content":"Desarrolla proyectos completos de programaci√≥n, bases de datos, interfaces, webs full-stack, simulaciones, IA, videojuegos educativos, automatizaci√≥n de sistemas y dashboards empresariales. Su metodolog√≠a se basa en ejercicios progresivos con ejemplos reales y documentaci√≥n clara."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© estilo docente tiene Jose Vicente?"},
  {"role":"assistant","content":"Su estilo docente es directo, pr√°ctico y orientado a la industria. Explica con sencillez, usa muchos ejemplos reales, proporciona c√≥digo limpio y anima a que cada alumno construya proyectos completos. Adem√°s integra IA, an√°lisis de datos, servidores, APIs y entornos profesionales en las clases."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© tecnolog√≠as domina Jose Vicente?"},
  {"role":"assistant","content":"Jose Vicente domina Python, PHP, Java, SQL, JavaScript, React, C++, CUDA, OpenCV, HTML, CSS, sistemas Linux, Apache, Flask, bases de datos, BI, WebSockets, Next.js, Three.js, Odoo, Moodle, entrenamiento de LLMs (QLoRA, GGUF, ctransformers) y frameworks modernos de desarrollo."}
]}
{"messages":[
  {"role":"user","content":"Dime algunos de los proyectos de inteligencia artificial que desarrolla Jose Vicente."},
  {"role":"assistant","content":"Jose Vicente desarrolla asistentes educativos, modelos LLM personalizados, sistemas de evaluaci√≥n autom√°tica con IA, chatbots entrenados con contenido propio, visualizaciones 3D sincronizadas con voz, training pipelines con QLoRA, datasets generados a partir de transcripciones, y modelos para programaci√≥n educativa."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© cursos ha creado Jose Vicente Carratal√°?"},
  {"role":"assistant","content":"Ha creado cursos de Python, SQL, HTML/CSS/JS, PHP, C++, 3D, IA generativa, dise√±o web, programaci√≥n multiplataforma, sistemas operativos, bases de datos, servidores Linux, frameworks frontend, visualizaci√≥n 3D, y proyectos integrados para DAM/DAW."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© libros ha escrito Jose Vicente?"},
  {"role":"assistant","content":"Jose Vicente es autor de libros como 'Aprende Programaci√≥n con Python', manuales de SQL, cursos t√©cnicos en formato libro, manuales de servidores Linux, desarrollo web y programaci√≥n educativa. Tambi√©n adapta cursos completos a formato libro para sus alumnos."}
]}
{"messages":[
  {"role":"user","content":"¬øC√≥mo organiza Jose Vicente sus proyectos educativos?"}, 
  {"role":"assistant","content":"Jose Vicente usa un sistema basado en JSON, PHP, Python y dashboards personalizados. Cada actividad tiene evaluaciones, criterios, m√©tricas, procesos de correcci√≥n y seguimiento. Sus aplicaciones permiten analizar calificaciones, progreso, feedback y estad√≠sticas docentes."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© plataformas administra Jose Vicente?"}, 
  {"role":"assistant","content":"Administra servidores Linux, Apache, Moodle, Odoo, webs desarrolladas a medida, aplicaciones Flask, APIs educativas y sistemas completos de gesti√≥n acad√©mica. Gestiona certificados SSL, backups autom√°ticos y despliegues remotos."}
]}
{"messages":[
  {"role":"user","content":"¬øC√≥mo es la filosof√≠a de Jose Vicente respecto al aprendizaje?"}, 
  {"role":"assistant","content":"Cree en el aprendizaje pr√°ctico, directo y progresivo. Prefiere que el alumnado construya proyectos reales en lugar de memorizar teor√≠a. Fomenta el pensamiento cr√≠tico, la autonom√≠a, la creatividad, y el uso de herramientas profesionales desde el primer d√≠a."}
]}
{"messages":[
  {"role":"user","content":"Describe el ecosistema de productos Jocarsa creado por Jose Vicente."},
  {"role":"assistant","content":"El ecosistema Jocarsa incluye CRM, ERP, herramientas online, hojas de c√°lculo, mapas mentales, asistentes empresariales, paneles de BI, utilidades de servidor, m√≥dulos educativos, un sistema SIS personalizado y soluciones SaaS orientadas a peque√±as y medianas empresas."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© trabajos de 3D desarrolla Jose Vicente Carratal√°?"}, 
  {"role":"assistant","content":"Desarrolla visualizaciones 3D en Blender, renderizados, animaciones, escenas interactivas WebGL, simulaciones f√≠sicas, interfaces 3D educativas, portfolios animados, path tracing, Gaussian Splatting y proyectos creativos para mostrar procesos t√©cnicos en clase."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© proyectos recientes est√° desarrollando Jose Vicente?"}, 
  {"role":"assistant","content":"Est√° desarrollando: una plataforma de BI en Java; un servidor WebSocket seguro; un asistente educativo LLM local; una web completa para Jocarsa; un nuevo sistema SIS‚ÄìMoodle; simulaciones fractales en C++ con OpenMP; un videojuego educativo 2D; y scripts de automatizaci√≥n para YouTube, Moodle y scraping industrial."}
]}
{"messages":[
  {"role":"user","content":"¬øJose Vicente trabaja con inteligencia artificial local?"}, 
  {"role":"assistant","content":"S√≠, utiliza modelos locales como Qwen, LLaMA, Gemma y Mistral, ejecutados en su propio hardware usando Ollama, ctransformers y GGUF. Ajusta modelos con QLoRA, crea datasets propios y entrena asistentes personalizados para programaci√≥n e investigaci√≥n."}
]}
{"messages":[
  {"role":"user","content":"¬øC√≥mo organiza Jose Vicente los proyectos de los alumnos?"}, 
  {"role":"assistant","content":"Usa un √°rbol de cursos en Markdown, c√≥digos de actividad, JSONs estructurados, un SIS personalizado, integraci√≥n con Moodle v√≠a APIs y scripts autom√°ticos para matricular, evaluar y revisar entregas. Adem√°s genera dashboards de actividad docente usando PHP y JavaScript."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© estilo tiene Jose Vicente al explicar c√≥digo?"}, 
  {"role":"assistant","content":"Explica con claridad, ejemplos concretos, comparaciones sencillas, y siempre conecta el c√≥digo con su aplicaci√≥n real. Prefiere ense√±ar progresivamente sin adelantar conceptos y mantiene un estilo cercano a sus cursos y libros t√©cnicos."}
]}
{"messages":[
  {"role":"user","content":"¬øQu√© papel tiene YouTube en el trabajo de Jose Vicente?"}, 
  {"role":"assistant","content":"YouTube es uno de sus pilares: tiene miles de v√≠deos educativos sobre programaci√≥n y tecnolog√≠a. Usa las transcripciones como dataset para entrenar sus propios modelos de IA, y muchos de sus cursos completos est√°n publicados all√≠ para sus estudiantes."}
]}
```

### Actividades propuestas

### Preparando la Entrenamiento del Modelo LLM con Datos

Para preparar el entrenamiento del modelo LLM (Large Language Model) basado en los datos proporcionados, necesitamos asegurarnos de que las entradas est√©n correctamente formateadas y sean coherentes. Aqu√≠ se presenta un resumen paso a paso de c√≥mo podr√≠as proceder para utilizar estos datos:

1. **Formatear los Datos**:
   Los datos ya est√°n en formato JSON con roles claros (user, assistant) lo que facilita su uso directo en el entrenamiento del modelo.

2. **Estructura del Dataset**:
   El dataset est√° formado por pares de conversaciones entre un usuario y un asistente, con las respuestas proporcionadas por Jose Vicente Carratal√° sobre diferentes aspectos de su trabajo y conocimientos. Esto es ideal para un entrenamiento condicionado donde el modelo aprender√° a responder preguntas similares.

3. **Procesar los Datos**:
   Si se necesitan realizar ajustes adicionales, podr√≠as procesar los datos con scripts en Python utilizando bibliotecas como `json` y `pandas`. Aqu√≠ hay un ejemplo de c√≥mo cargar estos datos:

   ```python
   import json

   # Cargar archivos JSON (para m√∫ltiples archivos)
   def load_json_files(file_path):
       with open(file_path, 'r', encoding='utf-8') as file:
           data = [json.loads(line) for line in file]
       return data
   
   # Ejemplo de uso
   dataset_path = "training_data.jsonl"
   data = load_json_files(dataset_path)
   ```

4. **Preparar los Datos para el Entrenamiento**:
   
   Los datos ya est√°n en la forma adecuada que las bibliotecas de entrenamiento necesitan:

   ```python
   def prepare_dataset(data):
       prepared_data = []
       for entry in data:
           user_message = entry['messages'][0]['content']
           assistant_response = entry['messages'][1]['content']
           
           # Formatear para modelos (e.g., Qwen, ctransformers)
           pair = {"prompt": f"User: {user_message}\n", "completion": f"{assistant_response}"}
           prepared_data.append(pair)

       return prepared_data
   
   # Ejemplo de uso
   training_dataset = prepare_dataset(data)
   ```

5. **Entrenar el Modelo**:

   Ahora puedes usar bibliotecas como `transformers` de Hugging Face o herramientas personalizadas para entrenar un modelo con este dataset. Aqu√≠ hay un ejemplo b√°sico usando `ctransformers` y GGUF formatos.

   ```python
   from transformers import AutoModelForCausalLM, AutoTokenizer

   model_name = "Qwen"
   tokenizer = AutoTokenizer.from_pretrained(model_name)
   model = AutoModelForCausalLM.from_pretrained(model_name)

   # Convertir el dataset a formato adecuado para entrenar (e.g., GGUF format)
   def convert_to_gguf_format(dataset):
       gguf_data = []
       for entry in dataset:
           input_ids = tokenizer.encode(entry['prompt'], return_tensors='pt')
           labels = tokenizer.encode(entry['completion'] + tokenizer.eos_token, return_tensors='pt').squeeze()
           gguf_data.append({"input_ids": input_ids.tolist(), "labels": labels.tolist()})
       
       return gguf_data

   training_dataset_gguf = convert_to_gguf_format(training_dataset)

   # Guardar en formato GGUF
   with open("trained_model.gguf", 'w') as f:
       json.dump(training_dataset_gguf, f)
   
   ```

6. **Personalizar el Modelo**:

   Al entrenar con datos espec√≠ficos, puedes personalizar un modelo existente (como Qwen) para que se comporte como Jose Vicente Carratal√° en sus respuestas y estilo de ense√±anza.

### Ejecutar la Entrenamiento del Modelo

Usando este dataset preparado, podr√≠as entrenar el modelo localmente o en una infraestructura adecuada. Esto requerir√° configuraci√≥n espec√≠fica dependiendo de la herramienta (como ctransformers, Ollama, etc).

Finalmente, puedes probar y ajustar el modelo para asegurarte de que responde adecuadamente a consultas sobre los temas mencionados en tu dataset.

### Ejecutar el Modelo Entrenado

Despu√©s del entrenamiento, podr√≠as usar este modelo personalizado en aplicaciones o servicios espec√≠ficos como asistentes educativos o respuestas autom√°ticas para preguntas t√©cnicas.


<a id="entrenar-chatbot-a-partir-de-whatsapp"></a>
## entrenar chatbot a partir de whatsapp

### Introducci√≥n a los ejercicios

Aqu√≠ tienes la informaci√≥n y los pasos que has proporcionado organizados de manera clara:

### Resumen del Inter√©s por el Ciclo Formativo DAW (Desarrollo de Aplicaciones Web)

Ana est√° interesada en cursar el ciclo formativo de Desarrollo de Aplicaciones Web (DAW). Mar√≠a, una estudiante del mismo centro, ha estado ayud√°ndola a tomar la decisi√≥n y respondiendo a sus preguntas.

### Conversaci√≥n entre Ana y Mar√≠a

**28/09/2023:**
- **Ana**: Est√° buscando informaci√≥n sobre DAW.
- **Mar√≠a**: Responde dudas de Ana sobre los m√≥dulos del ciclo, las dificultades en cada asignatura, y anima a visitar el centro.

**15/10/2023:**
- **Ana**: Consulta detalles espec√≠ficos sobre asignaturas (Programaci√≥n, Bases de Datos, Lenguajes de Marca).
- **Mar√≠a**: Informa que Programaci√≥n es la m√°s importante y se da durante dos a√±os. Tambi√©n explica proyectos pr√°cticos y menciona el valor del ingl√©s t√©cnico en el sector.

**12/04/2024:**
- **Ana**: Anuncia que ha solicitado DAW en tres centros.
- **Mar√≠a**: Expresa alegr√≠a por la decisi√≥n de Ana y ofrece ayuda durante el proceso de matr√≠cula.

**28/06/2024:**
- **Ana**: Recibe noticia positiva sobre su admisi√≥n en el centro de Mar√≠a.
- **Mar√≠a**: Felicita a Ana por su aceptaci√≥n.

### Creaci√≥n del Entorno Virtual (Python)

Para crear un entorno virtual en Python y configurarlo correctamente, sigue los siguientes pasos:

1. Crea un entorno virtual llamado `.venv`:
   ```bash
   python3 -m venv .venv
   ```

2. Activa el entorno virtual:
   ```bash
   source .venv/bin/activate
   ```

3. Instala las bibliotecas necesarias utilizando `pip`:
   ```bash
   pip install transformers datasets peft accelerate torch
   ```

### Conclusi√≥n

Ana ha decidido cursar DAW en el turno de tarde y est√° esperando la formalizaci√≥n de su matr√≠cula. Mar√≠a, una estudiante del mismo centro, ha sido un fuente constante de apoyo e informaci√≥n durante este proceso.

Si Ana necesita ayuda adicional con los tr√°mites o cualquier otra pregunta sobre el ciclo formativo, Mar√≠a estar√° disponible para ayudarla.

### whatsapp a jsonl
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo Python convierte un archivo de texto que contiene una conversaci√≥n exportada desde WhatsApp a otro formato llamado JSON Lines (o JSONL), espec√≠ficamente dise√±ado para entrenar chatbots. El proceso se divide en varias etapas:

1. **Lectura y an√°lisis del archivo de entrada**: Primero, el c√≥digo lee un archivo de texto (como `conversacion.txt`) que contiene la conversaci√≥n exportada desde WhatsApp. A continuaci√≥n, analiza cada l√≠nea para identificar si pertenece a una plataforma espec√≠fica (Android o iOS) y tambi√©n para detectar mensajes de sistema o mensajes normales.

2. **Reconstrucci√≥n de los mensajes**: Los mensajes multil√≠nea se reconstruyen en un √∫nico mensaje m√°s largo. Esto es importante porque algunos usuarios pueden enviar mensajes que ocupan varias l√≠neas en el archivo de texto original.

3. **Transformaci√≥n en formato ChatML Q&A**: El c√≥digo agrupa las conversaciones entre dos usuarios (por ejemplo, entre un usuario y la asistente) en pares pregunta-respuesta (Q&A). Cada par se presenta en un formato espec√≠fico llamado ChatML:
    - Los mensajes del usuario son etiquetados con `{"role": "user", "content": "..."}`
    - Las respuestas de la asistente, o cualquier persona cuyo nombre est√° definido como `ASSISTANT_NAME`, se etiquetan como `{"role": "assistant", "content": "...}"`.

4. **Escritura del archivo JSONL**: Finalmente, el c√≥digo genera un nuevo archivo (como `conversacion.jsonl`) que contiene estos pares Q&A en formato JSON Lines. Cada par de mensajes (pregunta y respuesta) se escribe como una l√≠nea separada en este archivo.

Este proceso es √∫til para preparar datos de conversaci√≥n en un formato que los sistemas de inteligencia artificial pueden usar f√°cilmente para aprender c√≥mo responder a consultas, haciendo m√°s sencillo el entrenamiento de chatbots basados en conversaciones reales.

`001-whatsapp a jsonl.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Convierte un archivo de exportaci√≥n de WhatsApp (Android o iOS) a un JSONL
en formato ChatML con pares pregunta‚Äìrespuesta (Q&A).

- Lee INPUT_FILE (exportaci√≥n de WhatsApp en texto).
- Detecta formato Android e iOS (incluyendo: [28/09/2023 10:15] Ana: ...).
- Reconstruye mensajes multil√≠nea.
- Agrupa mensajes en bloques:
    usuarios (rol "user") -> asistente (rol "assistant")
- Genera OUTPUT_FILE en formato JSONL:

    {"messages":[
        {"role":"user","content":"..."},
        {"role":"assistant","content":"..."}
    ]}

Configura aqu√≠:

    INPUT_FILE
    OUTPUT_FILE
    ASSISTANT_NAME
"""

import re
import json
from datetime import datetime
from typing import Optional, List, Dict, Tuple

# Archivos de entrada/salida
INPUT_FILE = "conversacion.txt"
OUTPUT_FILE = "conversacion.jsonl"

# Nombre de la persona que act√∫a como "asistente" en la conversaci√≥n
ASSISTANT_NAME = "Mar√≠a"   # c√°mbialo si procede


# -----------------------------
# Patrones para reconocer INICIO de mensaje
# -----------------------------

# Android (espa√±ol/Europa):
# 01/01/24, 9:15 - Nombre: mensaje
ANDROID_USER_RE = re.compile(
    r'^(\d{1,2}/\d{1,2}/\d{2,4}), (\d{1,2}:\d{2}) - ([^:]+): (.*)$'
)

# Android sistema (sin "Nombre:"):
# 01/01/24, 9:15 - Mensaje del sistema...
ANDROID_SYSTEM_RE = re.compile(
    r'^(\d{1,2}/\d{1,2}/\d{2,4}), (\d{1,2}:\d{2}) - (.*)$'
)

# iOS usuario:
# [28/09/2023 10:15] Ana: mensaje
# [28/09/2023, 10:15] Ana: mensaje
IOS_USER_RE = re.compile(
    r'^\[(\d{1,2}/\d{1,2}/\d{2,4})[ ,]+(\d{1,2}:\d{2})\] ([^:]+): (.*)$'
)

# iOS sistema:
# [28/09/2023 10:15] Mensaje del sistema...
IOS_SYSTEM_RE = re.compile(
    r'^\[(\d{1,2}/\d{1,2}/\d{2,4})[ ,]+(\d{1,2}:\d{2})\] (.*)$'
)


def parse_datetime(date_str: str, time_str: str) -> Optional[datetime]:
    """
    Intenta parsear la fecha y hora de WhatsApp a datetime.
    Acepta:
        "dd/mm/aa, HH:MM"
        "dd/mm/aaaa, HH:MM"
        "dd/mm/aa HH:MM"
        "dd/mm/aaaa HH:MM"
    """
    candidates = [
        f"{date_str}, {time_str}",
        f"{date_str} {time_str}",
    ]
    formats = [
        "%d/%m/%y, %H:%M",
        "%d/%m/%Y, %H:%M",
        "%d/%m/%y %H:%M",
        "%d/%m/%Y %H:%M",
    ]

    for candidate in candidates:
        for fmt in formats:
            try:
                return datetime.strptime(candidate, fmt)
            except ValueError:
                continue

    return None


def parse_new_message_line(line: str) -> Optional[Tuple[Optional[datetime], Optional[str], str]]:
    """
    Intenta interpretar una l√≠nea como inicio de un nuevo mensaje.

    Devuelve:
        (dt, author, text)
    donde author puede ser None para mensajes del sistema.

    Si la l√≠nea no parece un inicio de mensaje nuevo, devuelve None.
    """

    # Android con autor
    m = ANDROID_USER_RE.match(line)
    if m:
        date_str, time_str, author, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, author.strip(), text

    # iOS con autor
    m = IOS_USER_RE.match(line)
    if m:
        date_str, time_str, author, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, author.strip(), text

    # Android sistema
    m = ANDROID_SYSTEM_RE.match(line)
    if m:
        date_str, time_str, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, None, text

    # iOS sistema
    m = IOS_SYSTEM_RE.match(line)
    if m:
        date_str, time_str, text = m.groups()
        dt = parse_datetime(date_str, time_str)
        return dt, None, text

    return None


def whatsapp_txt_a_mensajes(lines: List[str]) -> List[Dict]:
    """
    Convierte las l√≠neas de un .txt de WhatsApp en una lista de mensajes estructurados.

    Cada mensaje:
    {
        "timestamp": datetime o None,
        "author": str o None (sistema),
        "text": str,
        "raw_line": str
    }
    """
    mensajes: List[Dict] = []
    actual: Optional[Dict] = None

    for raw_line in lines:
        line = raw_line.rstrip("\n\r")

        # Intentar detectar un nuevo mensaje
        parsed = parse_new_message_line(line)
        if parsed is not None:
            dt, author, text = parsed

            # Guardar el mensaje anterior si exist√≠a
            if actual is not None:
                mensajes.append(actual)

            # Crear nuevo mensaje
            actual = {
                "timestamp": dt,
                "author": author,
                "text": text if text is not None else "",
                "raw_line": line,
            }
        else:
            # Continuaci√≥n de mensaje anterior (multil√≠nea)
            if actual is not None:
                if actual["text"]:
                    actual["text"] += "\n" + line
                else:
                    actual["text"] = line
            else:
                # L√≠nea fuera de cualquier mensaje (t√≠tulo, etc.). Se ignora.
                continue

    # A√±adir √∫ltimo mensaje si existe
    if actual is not None:
        mensajes.append(actual)

    return mensajes


def mensajes_a_chatml_qna(mensajes: List[Dict]) -> List[Dict]:
    """
    A partir de la lista de mensajes "planos", genera pares Q&A
    en formato ChatML:

    {
      "messages": [
        {"role":"user", "content":"..."},
        {"role":"assistant", "content":"..."}
      ]
    }

    L√≥gica:
    - Se considera "assistant" cualquier mensaje cuyo author == ASSISTANT_NAME.
    - El resto de autores se consideran "user".
    - Se agrupan bloques:
        (uno o m√°s mensajes de usuarios) seguido de (uno o m√°s mensajes de asistente).
      Cada bloque genera un ejemplo Q&A.
    """
    # Filtrar mensajes de sistema (sin autor)
    msgs = [m for m in mensajes if m["author"] is not None]

    qna_examples: List[Dict] = []

    i = 0
    n = len(msgs)

    while i < n:
        m = msgs[i]
        # Si el mensaje actual es de la asistente sin bloque previo de usuario, se ignora
        if m["author"] == ASSISTANT_NAME:
            i += 1
            continue

        # Bloque de usuario(s): uno o m√°s mensajes consecutivos de autores != ASSISTANT_NAME
        user_texts = [m["text"]]
        i += 1
        while i < n and msgs[i]["author"] != ASSISTANT_NAME:
            user_texts.append(msgs[i]["text"])
            i += 1

        # A continuaci√≥n deber√≠a venir la asistente
        if i >= n or msgs[i]["author"] != ASSISTANT_NAME:
            # No hay respuesta de la asistente; no formamos Q&A
            continue

        # Bloque de asistente(s): uno o m√°s mensajes consecutivos de author == ASSISTANT_NAME
        assistant_texts = [msgs[i]["text"]]
        i += 1
        while i < n and msgs[i]["author"] == ASSISTANT_NAME:
            assistant_texts.append(msgs[i]["text"])
            i += 1

        user_content = "\n".join(user_texts).strip()
        assistant_content = "\n".join(assistant_texts).strip()

        if not user_content or not assistant_content:
            continue

        example = {
            "messages": [
                {"role": "user", "content": user_content},
                {"role": "assistant", "content": assistant_content},
            ]
        }
        qna_examples.append(example)

    return qna_examples


def escribir_jsonl_chatml_qna(examples: List[Dict], out_path: str) -> None:
    """
    Escribe la lista de ejemplos ChatML Q&A en un JSONL.
    """
    with open(out_path, "w", encoding="utf-8") as f_out:
        for ex in examples:
            f_out.write(json.dumps(ex, ensure_ascii=False) + "\n")


def main():
    print(f"Leyendo: {INPUT_FILE}")

    with open(INPUT_FILE, "r", encoding="utf-8") as f_in:
        lines = f_in.readlines()

    mensajes = whatsapp_txt_a_mensajes(lines)
    print(f"Mensajes totales parseados: {len(mensajes)}")

    qna_examples = mensajes_a_chatml_qna(mensajes)
    print(f"Pares Q&A generados: {len(qna_examples)}")

    escribir_jsonl_chatml_qna(qna_examples, OUTPUT_FILE)

    print(f"JSONL ChatML Q&A generado en: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
```

### entrenamiento
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo es un script Python que se utiliza para entrenar un chatbot basado en el modelo de lenguaje Qwen2.5 usando datos recopilados del servicio WhatsApp. El objetivo principal del script es tomar un conjunto de datos en formato JSONL (cada l√≠nea es un objeto JSON), procesarlo y luego utilizarlo para ajustar el modelo existente.

El c√≥digo comienza cargando la configuraci√≥n b√°sica, como la ruta al archivo de datos (`DATA_PATH`), el nombre del modelo a utilizar (`MODEL_NAME`), y otros par√°metros como `MAX_LENGTH`, que determina cu√°ntos tokens pueden tener las entradas antes de ser truncadas. Luego, se verifica si hay una GPU disponible para mejorar el rendimiento del entrenamiento.

Una vez detectado el dispositivo (GPU o CPU), el script carga el conjunto de datos desde un archivo JSONL y luego utiliza `AutoTokenizer` y `AutoModelForCausalLM` para cargar los componentes necesarios del modelo. Si se usa la GPU, el modelo es configurado para usar QLoRA 4-bit, una t√©cnica que permite entrenar modelos grandes en hardware limitado.

El script tambi√©n transforma cada ejemplo de conversaci√≥n (que consta de m√∫ltiples mensajes) en un solo texto plano usando una plantilla de chat proporcionada por el tokenizer. Luego, estos datos son tokenizados y preparados para el entrenamiento.

Finalmente, se configuran los argumentos del entrenador y se crea un objeto `Trainer` con el modelo cargado y los datos preprocesados. El script comienza el proceso de entrenamiento y guarda tanto el modelo como el tokenizer una vez que termina en la ruta especificada por `OUTPUT_DIR`.

Este tipo de c√≥digo es fundamental para personas que desean crear o mejorar chatbots utilizando modelos de lenguaje preentrenados, especialmente cuando se requiere ajustar estos modelos a datos espec√≠ficos.

`002-entrenamiento.py`

```python
#!/usr/bin/env python3
import os
import json
from dataclasses import dataclass

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# -------------------------------------------------------------------
# CONFIGURACI√ìN B√ÅSICA
# -------------------------------------------------------------------

DATA_PATH = "conversacion.jsonl"
# Modelo peque√±o y abierto de Qwen2.5 (aj√∫stalo si quieres otro)
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = "./qwen25-05b-jvc"

MAX_LENGTH = 512        # suficiente para ejemplos cortos
NUM_EPOCHS = 3
LR = 2e-4
BATCH_SIZE = 1
GRAD_ACCUM = 4          # batch efectivo = 4


def main():
    print("üöÄ Inicio de entrenamiento Qwen2.5 (Python puro)")
    print(f"üìÅ Usando dataset: {DATA_PATH}")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print("-" * 60)

    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"No se encontr√≥ el dataset en {DATA_PATH}")

    # -------------------------------------------------------------------
    # DETECTAR DISPOSITIVO
    # -------------------------------------------------------------------
    if torch.cuda.is_available():
        device = "cuda"
        use_4bit = True
        print("üíª GPU detectada (CUDA). Entrenando con QLoRA 4-bit.")
    else:
        device = "cpu"
        use_4bit = False
        print("üíª No hay GPU CUDA. Entrenando en CPU en float32 (esto ser√° m√°s lento).")

    # -------------------------------------------------------------------
    # CARGAR DATASET
    # -------------------------------------------------------------------
    print("üì• Cargando dataset con datasets.load_dataset(...)")
    raw_dataset = load_dataset(
        "json",
        data_files=DATA_PATH,
        split="train"
    )
    print(f"‚úÖ Dataset cargado con {len(raw_dataset)} ejemplos.")

    # -------------------------------------------------------------------
    # CARGAR TOKENIZER Y MODELO
    # -------------------------------------------------------------------
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=True
    )

    # Asegurar que tenemos pad_token definido
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ Cargando modelo base...")
    if use_4bit:
        # QLoRA 4-bit en GPU
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        model = prepare_model_for_kbit_training(model)

        # Configuraci√≥n LoRA
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    else:
        # Entrenamiento cl√°sico en CPU
        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
        model.to(device)

    # -------------------------------------------------------------------
    # CONVERTIR messages ‚Üí texto usando plantilla de chat
    # -------------------------------------------------------------------
    print("üß± Transformando mensajes (user/assistant) en texto plano con plantilla de chat...")

    def messages_to_text(example):
        """
        Convierte el campo 'messages' del JSONL en un √∫nico texto
        usando la plantilla de chat del tokenizer.
        """
        conv = []
        for m in example["messages"]:
            role = m.get("role", "user")
            if role not in ("user", "assistant", "system"):
                role = "user"
            conv.append({"role": role, "content": m["content"]})

        try:
            text = tokenizer.apply_chat_template(
                conv,
                tokenize=False,
                add_generation_prompt=False,
            )
        except Exception:
            # Fallback sencillo si falla apply_chat_template
            partes = []
            for m in conv:
                if m["role"] == "user":
                    prefix = "Usuario"
                elif m["role"] == "assistant":
                    prefix = "Asistente"
                else:
                    prefix = m["role"]
                partes.append(f"{prefix}: {m['content']}")
            text = "\n".join(partes)
        return {"text": text}

    text_dataset = raw_dataset.map(
        messages_to_text,
        remove_columns=raw_dataset.column_names
    )

    # -------------------------------------------------------------------
    # TOKENIZACI√ìN
    # -------------------------------------------------------------------
    print("‚úÖ Tokenizando dataset...")

    def tokenize_fn(batch):
        out = tokenizer(
            batch["text"],
            truncation=True,
            max_length=MAX_LENGTH,
            padding="max_length",
        )
        out["labels"] = out["input_ids"].copy()
        return out

    tokenized_dataset = text_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=["text"]
    )

    # -------------------------------------------------------------------
    # TRAINING ARGUMENTS (SIN evaluation_strategy NI HISTORIAS RARAS)
    # -------------------------------------------------------------------
    print("‚úÖ Configurando argumentos de entrenamiento...")

    # Solo uno de fp16 / bf16 puede ser True. Usamos fp16 en GPU, nada en CPU.
    use_fp16 = (device == "cuda")
    use_bf16 = False

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LR,
        weight_decay=0.01,
        warmup_ratio=0.03,
        logging_steps=1,
        save_steps=20,
        save_total_limit=1,
        fp16=use_fp16,
        bf16=use_bf16,
        dataloader_pin_memory=False,
        report_to="none",  # sin wandb ni nada
    )

    # -------------------------------------------------------------------
    # TRAINER
    # -------------------------------------------------------------------
    print("‚úÖ Creando Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )

    # -------------------------------------------------------------------
    # ENTRENAR
    # -------------------------------------------------------------------
    print("üöÇ Comenzando entrenamiento...")
    trainer.train()
    print("üèÅ Entrenamiento terminado.")

    # -------------------------------------------------------------------
    # GUARDAR MODELO Y TOKENIZER
    # -------------------------------------------------------------------
    print("üíæ Guardando modelo y tokenizer en", OUTPUT_DIR)
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("‚úÖ Todo listo.")


if __name__ == "__main__":
    main()
```

### probar
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo es una herramienta que permite probar un modelo de lenguaje entrenado previamente, llamado Qwen2.5-0.5B-Instruct. El objetivo principal del script es permitir a los usuarios interactuar con el chatbot y evaluar c√≥mo responde en espa√±ol despu√©s de ser finetunado (es decir, adaptado para un uso espec√≠fico) utilizando la t√©cnica LoRA.

El c√≥digo comienza por importar las bibliotecas necesarias e intenta cargar una librer√≠a adicional llamada `peft`, que es requerida si se desea utilizar la t√©cnica LoRA. A continuaci√≥n, establece algunas configuraciones b√°sicas como el nombre del modelo base y la carpeta donde se guarda el finetuning.

El script incluye funciones para detectar qu√© tipo de hardware est√° disponible (CPU o GPU), cargar los componentes necesarios (tokenizer y modelo base) y, si es posible, incorpora las modificaciones realizadas con LoRA. Adem√°s, tiene una funci√≥n `build_prompt` que construye el texto de entrada en un formato adecuado para la conversaci√≥n.

El flujo principal del script empieza cargando estos componentes y luego entra en un bucle interactivo donde el usuario puede ingresar preguntas en espa√±ol y recibir respuestas generadas por el modelo. El chatbot sigue un esquema simple: imprime una invitaci√≥n al usuario, recoge la entrada del usuario, construye la pregunta para el modelo, genera una respuesta con el modelo cargado, e imprime esa respuesta.

Este tipo de scripts son √∫tiles en etapas finales de desarrollo y prueba de modelos de lenguaje, ya que permiten evaluar r√°pidamente c√≥mo un modelo responde a diferentes tipos de preguntas o entradas.

`003-probar.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

try:
    from peft import PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False


# ==========================
# CONFIGURACI√ìN
# ==========================
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"        # mismo modelo base que en entrenar.py
LORA_DIR   = "./qwen25-05b-jvc"             # carpeta donde entrenar.py guard√≥ el LoRA o el modelo
MAX_NEW_TOKENS = 256
TEMPERATURE    = 0.7
TOP_P          = 0.9


def print_header():
    print("üöÄ Test de modelo Qwen2.5 fine-tune (LoRA)")
    print(f"üß† Modelo base: {MODEL_NAME}")
    print(f"üìÅ Carpeta LoRA / modelo: {LORA_DIR}")
    print("-" * 60)


def detect_device():
    if torch.cuda.is_available():
        device = "cuda"
        torch_dtype = torch.float16
        print("üíª GPU detectada (CUDA). Usando GPU con float16.")
    else:
        device = "cpu"
        torch_dtype = torch.float32
        print("üíª No se detecta GPU. Usando CPU con float32.")
    return device, torch_dtype


def load_tokenizer(model_name):
    print("‚úÖ Cargando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # Por si el tokenizer no tiene pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


def load_base_model(model_name, device, torch_dtype):
    print("‚úÖ Cargando modelo base...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch_dtype,
        device_map="auto" if device == "cuda" else None,
    )
    if device == "cpu":
        model.to(device)
    return model


def load_with_lora_if_possible(base_model, lora_dir, device):
    """
    Intenta cargar adaptador LoRA desde lora_dir.
    - Si hay adapter_config.json, se asume que es un LoRA PEFT.
    - Si no, se asume que lora_dir es un modelo HF completo (modelo fusionado).
    """
    if not os.path.isdir(lora_dir):
        print(f"‚ùå La carpeta '{lora_dir}' NO existe.")
        print("   Revisa en entrenar.py cu√°l fue el 'output_dir' / carpeta donde se guard√≥ el LoRA o el modelo.")
        sys.exit(1)

    adapter_config_path = os.path.join(lora_dir, "adapter_config.json")

    # Caso 1: LoRA PEFT (adapter_config.json presente)
    if os.path.exists(adapter_config_path):
        if not PEFT_AVAILABLE:
            print("‚ùå Se ha encontrado un adaptador LoRA (adapter_config.json),")
            print("   pero el paquete 'peft' no est√° instalado.")
            print("   Inst√°lalo con: pip install peft")
            sys.exit(1)

        print(f"‚úÖ Encontrado adaptador LoRA en: {lora_dir}")
        from peft import PeftModel

        model = PeftModel.from_pretrained(
            base_model,
            lora_dir,
            device_map="auto" if device == "cuda" else None,
        )
        # Opcional: fusionar LoRA para inferencia
        try:
            model = model.merge_and_unload()
            print("‚úÖ LoRA fusionado con el modelo base para inferencia.")
        except Exception:
            print("‚ÑπÔ∏è No se ha fusionado el LoRA (merge_and_unload fall√≥ o no est√° disponible).")
        return model

    # Caso 2: No hay adapter_config.json ‚Üí intentar cargar lora_dir como modelo completo
    print("‚ö†Ô∏è No se encontr√≥ 'adapter_config.json' en la carpeta LoRA.")
    print("   Se intentar√° cargar la carpeta como un modelo completo (modelo ya fusionado).")
    print("   Si esto falla, revisa que entrenar.py est√© llamando a model.save_pretrained(LORA_DIR).")

    try:
        model = AutoModelForCausalLM.from_pretrained(
            lora_dir,
            torch_dtype=base_model.dtype,
            device_map="auto" if device == "cuda" else None,
        )
        if device == "cpu":
            model.to(device)
        print("‚úÖ Modelo cargado directamente desde la carpeta.")
        return model
    except Exception as e:
        print("‚ùå No se ha podido cargar la carpeta como modelo completo.")
        print(f"   Error: {e}")
        sys.exit(1)


def build_prompt(tokenizer, user_text: str) -> str:
    """
    Construye el texto de entrada siguiendo la plantilla de chat de Qwen, si existe.
    Si no, usa un formato sencillo tipo chat.
    """
    if hasattr(tokenizer, "apply_chat_template"):
        messages = [
            {"role": "system", "content": "Eres un profesor de programaci√≥n llamado Jose Vicente. Explicas en espa√±ol de forma clara y did√°ctica."},
            {"role": "user",   "content": user_text},
        ]
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    else:
        # fallback simple
        return f"Sistema: Eres un profesor de programaci√≥n llamado Jose Vicente.\nUsuario: {user_text}\nAsistente:"


def generate_answer(tokenizer, model, device: str, prompt: str) -> str:
    model.eval()
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
    )

    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Nos quedamos solo con la parte generada (sin el prompt) para que sea m√°s limpio
    generated_ids = output_ids[0][inputs["input_ids"].shape[-1]:]
    text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return text.strip()


def main():
    print_header()

    device, torch_dtype = detect_device()

    # 1. Tokenizer y modelo base
    tokenizer = load_tokenizer(MODEL_NAME)
    base_model = load_base_model(MODEL_NAME, device, torch_dtype)

    # 2. Cargar LoRA o modelo completo desde LORA_DIR
    model = load_with_lora_if_possible(base_model, LORA_DIR, device)

    # 3. Bucle de prueba interactivo
    print("\n‚úÖ Modelo listo para probar.")
    print("Escribe una pregunta (en espa√±ol). Ctrl+C o l√≠nea vac√≠a para salir.\n")

    try:
        while True:
            user_text = input("üí¨ T√∫: ").strip()
            if not user_text:
                print("üëã Saliendo.")
                break

            prompt = build_prompt(tokenizer, user_text)
            print("\nü§ñ Modelo (pensando)...\n")
            answer = generate_answer(tokenizer, model, device, prompt)
            print("ü§ñ Respuesta:\n")
            print(answer)
            print("\n" + "-" * 60 + "\n")

    except KeyboardInterrupt:
        print("\nüëã Saliendo por Ctrl+C.")


if __name__ == "__main__":
    main()
```

### probar interactivo
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este fragmento de c√≥digo es un programa interactivo que permite entrenar y probar un chatbot usando el modelo Qwen. El programa est√° escrito en Python y utiliza bibliotecas como `transformers` para cargar y manipular modelos de lenguaje, as√≠ como `peft` para manejar ajustes espec√≠ficos (fine-tuning) del modelo.

El c√≥digo comienza por configurar varios par√°metros necesarios, incluyendo el nombre del modelo base que se utilizar√° (`Qwen/Qwen2.5-0.5B-Instruct`) y la carpeta donde se guarda el modelo entrenado (`qwen25-05b-jvc`). Tambi√©n define variables para limitar el n√∫mero m√°ximo de tokens generados por respuesta, junto con valores para controlar la aleatoriedad en las respuestas (temperatura y top-p).

A continuaci√≥n, se carga tanto el tokenizador como el modelo. Si existe un archivo `adapter_config.json` en la carpeta del modelo entrenado, el c√≥digo asume que el modelo ha sido ajustado usando una t√©cnica llamada LoRA/PEFT y carga ambos: el modelo base junto con los pesos adicionales para el ajuste fino. En caso contrario, se cargar√° directamente el modelo completo desde esa carpeta.

Una vez cargados, el programa define la funci√≥n `generate_reply`, que toma un mensaje del usuario como entrada, lo convierte en una secuencia de tokens, genera una respuesta usando el modelo y luego devuelve esta respuesta al usuario. La interacci√≥n con el chatbot se realiza a trav√©s de un bucle interactivo que espera inputs del usuario (hasta que se escribe "exit" o "quit"), procesa la entrada, genera una respuesta y la muestra.

Este tipo de script es importante en proyectos de IA porque permite probar interactivamente c√≥mo funciona el chatbot despu√©s del entrenamiento, lo cual es fundamental para evaluar su desempe√±o.

`004-probar interactivo.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------
BASE_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
MODEL_DIR       = "qwen25-05b-jvc"   # carpeta donde guardaste el modelo entrenado

MAX_NEW_TOKENS = 128
TEMPERATURE    = 0.7
TOP_P          = 0.9

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype  = torch.float16 if device == "cuda" else torch.float32

print("üîß Loading tokenizer and model...")
print(f"Using device: {device}")

# -------------------------------------------------------------------
# TOKENIZER
# -------------------------------------------------------------------
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# -------------------------------------------------------------------
# MODEL LOADING (AUTO-DETECT PEFT vs FULL MODEL)
# -------------------------------------------------------------------
adapter_config_path = os.path.join(MODEL_DIR, "adapter_config.json")

if os.path.exists(adapter_config_path):
    # Caso: LoRA / PEFT
    print(f"Detected PEFT adapter at '{MODEL_DIR}'. Loading base model + adapter...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        dtype=dtype,
    )
    model = PeftModel.from_pretrained(base_model, MODEL_DIR)
else:
    # Caso: modelo completo ya fine-tuned en MODEL_DIR
    print(f"No adapter_config.json found in '{MODEL_DIR}'.")
    print("Assuming full fine-tuned model directory. Loading directly...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        dtype=dtype,
    )

model.to(device)
model.eval()
print("‚úÖ Model loaded.")


def generate_reply(user_message: str) -> str:
    """
    Given a user message, build a chat-style conversation and
    generate the assistant reply using Qwen's chat template.
    """
    messages = [
        {"role": "user", "content": user_message}
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Keep only newly generated tokens (ignore the prompt part)
    generated_ids = output_ids[0, input_ids.shape[-1]:]
    reply = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return reply


# -------------------------------------------------------------------
# SIMPLE INTERACTIVE LOOP
# -------------------------------------------------------------------
def main():
    print("\nInteractive test with trained model.")
    print("Type your question and press Enter.")
    print("Type 'exit' or 'quit' to finish.\n")

    while True:
        try:
            user_input = input("You: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting.")
            break

        if not user_input:
            continue

        if user_input.lower() in ("exit", "quit"):
            print("Goodbye.")
            break

        reply = generate_reply(user_input)
        print(f"AI: {reply}\n")


if __name__ == "__main__":
    main()
```

### app
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo es un ejemplo de c√≥mo crear una aplicaci√≥n web utilizando Flask en Python para interactuar con un modelo de lenguaje entrenado llamado Qwen. La aplicaci√≥n permite que los usuarios ingresen mensajes a trav√©s de su navegador y reciban respuestas generadas por el modelo.

El c√≥digo comienza importando varias bibliotecas necesarias, incluyendo `torch` para manejar la computaci√≥n del tensor, y las clases espec√≠ficas de Flask como `Flask`, `render_template`, etc. Adem√°s, se cargan los tokenizadores y modelos de lenguaje usando `transformers` e `peft`. El modelo es cargado en la tarjeta gr√°fica (GPU) si est√° disponible para una mayor eficiencia.

La funci√≥n `generate_reply()` toma un mensaje del usuario como entrada, lo prepara adecuadamente para el formato requerido por Qwen y luego genera una respuesta utilizando el modelo. Esta funci√≥n controla c√≥mo se formulan las preguntas al modelo y c√≥mo se procesan sus respuestas.

Finalmente, la aplicaci√≥n Flask define dos rutas principales: `/` y `/chat`. La ruta principal (`/`) simplemente sirve un archivo HTML que act√∫a como interfaz de usuario para los usuarios. La ruta `/chat` maneja las solicitudes POST enviadas desde el cliente (el formulario en la p√°gina web), procesa los mensajes del usuario, genera respuestas con la funci√≥n `generate_reply()`, y luego devuelve estas respuestas al cliente.

Este tipo de aplicaci√≥n es importante porque permite a los estudiantes comprender c√≥mo integrar modelos avanzados de procesamiento del lenguaje natural en aplicaciones web interactivas, lo que puede ser muy √∫til para proyectos pr√°cticos y futuras carreras en el campo del desarrollo de software.

`app.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from flask import Flask, render_template, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------
BASE_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
MODEL_DIR       = "qwen25-05b-jvc"   # carpeta del modelo entrenado

MAX_NEW_TOKENS = 128
TEMPERATURE    = 0.7
TOP_P          = 0.9

# -------------------------------------------------------------------
# LOAD TOKENIZER + MODEL ONCE
# -------------------------------------------------------------------
print("üîß Loading tokenizer and model for Flask app...")

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype  = torch.float16 if device == "cuda" else torch.float32

print(f"Using device: {device}")

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

adapter_config_path = os.path.join(MODEL_DIR, "adapter_config.json")

if os.path.exists(adapter_config_path):
    # LoRA / PEFT case
    print(f"Detected PEFT adapter at '{MODEL_DIR}'. Loading base model + adapter...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        dtype=dtype,
    )
    model = PeftModel.from_pretrained(base_model, MODEL_DIR)
else:
    # Full fine-tuned model case
    print(f"No adapter_config.json found in '{MODEL_DIR}'.")
    print("Assuming full fine-tuned model directory. Loading directly...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        dtype=dtype,
    )

model.to(device)
model.eval()
print("‚úÖ Model loaded for Flask app.")


def generate_reply(user_message: str) -> str:
    """
    Given a user message, build a chat-style conversation and
    generate the assistant reply using Qwen's chat template.
    """
    messages = [
        {"role": "user", "content": user_message}
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            pad_token_id=tokenizer.eos_token_id,
        )

    generated_ids = output_ids[0, input_ids.shape[-1]:]
    reply = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return reply


# -------------------------------------------------------------------
# FLASK APP
# -------------------------------------------------------------------
app = Flask(__name__)


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/chat", methods=["POST"])
def chat():
    data = request.get_json(force=True)
    user_message = (data.get("message") or "").strip()

    if not user_message:
        return jsonify({"reply": ""})

    print(f"User: {user_message}")
    reply = generate_reply(user_message)
    print(f"AI:   {reply}")

    return jsonify({"reply": reply})


if __name__ == "__main__":
    # Development server
    app.run(host="0.0.0.0", port=5000, debug=True)
```

### conversacion
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este fragmento de c√≥digo contiene una serie de conversaciones almacenadas en formato JSONL (JSON Lines). Cada l√≠nea representa un mensaje en una conversaci√≥n, donde cada mensaje tiene dos campos clave: "role" y "content". El campo "role" indica si el mensaje fue enviado por un usuario ("user") o generado por un asistente ("assistant"). El campo "content" almacena el texto del mensaje.

Esta estructura de datos es com√∫n en aplicaciones de chatbots, donde se guardan interacciones pasadas entre usuarios y sistemas inteligentes para fines de entrenamiento y an√°lisis. En este caso espec√≠fico, la conversaci√≥n parece ser una ayuda educativa sobre los ciclos formativos (tambi√©n conocidos como formaci√≥n profesional o FP), proporcionando informaci√≥n detallada a un estudiante potencial.

Es importante porque permite al chatbot entender mejor las consultas comunes de los estudiantes y c√≥mo responder a ellas de manera efectiva, lo que mejora la experiencia del usuario en futuras interacciones.

`conversacion.jsonl`

```
{"messages": [{"role": "user", "content": "Hola Mar√≠a! ¬øC√≥mo est√°s? Quer√≠a preguntarte sobre los ciclos formativos porque s√© que est√°s estudiando uno"}, {"role": "assistant", "content": "Hola Ana! Bien, gracias :) S√≠, estoy en segundo de Desarrollo de Aplicaciones Web. ¬øEn qu√© puedo ayudarte?"}]}
{"messages": [{"role": "user", "content": "Me estoy planteando hacer un ciclo formativo pero tengo muchas dudas. No s√© si hacer uno de grado medio o superior"}, {"role": "assistant", "content": "Te explico: los grados medios duran 2 a√±os y te dan acceso a grados superiores. Los superiores son 2 a√±os y dan acceso a la universidad\nDepende de lo que quieras estudiar y si tienes el t√≠tulo de bachillerato o no"}]}
{"messages": [{"role": "user", "content": "Tengo bachillerato, pero no s√© si quiero ir a la universidad directamente. Me gusta m√°s la parte pr√°ctica"}, {"role": "assistant", "content": "Entonces los ciclos son perfectos! Son muy pr√°cticos y con buena salida laboral. ¬øQu√© te interesa?"}]}
{"messages": [{"role": "user", "content": "Me gusta la inform√°tica y tambi√©n he visto algo de marketing. ¬øQu√© ciclos hay en esas √°reas?"}, {"role": "assistant", "content": "En inform√°tica tienes:\n- CFGS Desarrollo de Aplicaciones Web\n- CFGS Desarrollo de Aplicaciones Multiplataforma\n- CFGS Administraci√≥n de Sistemas Inform√°ticos en Red\n- CFGS Ciberseguridad\nEn marketing:\n- CFGS Marketing y Publicidad\n- CFGS Comercio Internacional\n- CFGS Transporte y Log√≠stica"}]}
{"messages": [{"role": "user", "content": "Vaya, hay m√°s de lo que pensaba! ¬øY c√≥mo son las clases? ¬øMucha teor√≠a?"}, {"role": "assistant", "content": "En mi experiencia, es 70% pr√°ctica y 30% teor√≠a. Hacemos proyectos reales, programamos, configuramos servidores...\nAdem√°s, al final hay pr√°cticas en empresa de 3 meses, que es lo mejor para coger experiencia"}]}
{"messages": [{"role": "user", "content": "Suena bien! ¬øY es dif√≠cil entrar? ¬øHay que hacer alguna prueba?"}, {"role": "assistant", "content": "Con bachillerato entras directamente por nota media. Sin bachillerato hay que hacer prueba de acceso. La demanda es alta en inform√°tica"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© nota suele hacer falta? Mi media es de 7.8"}, {"role": "assistant", "content": "Con 7.8 entras en casi todos, aunque para Desarrollo Web el a√±o pasado la nota de corte fue 8.1 en algunos sitios\nPero hay muchos centros, seguro que encuentras plaza"}]}
{"messages": [{"role": "user", "content": "¬øY c√≥mo son los horarios? ¬øSe puede compaginar con trabajo?"}, {"role": "assistant", "content": "Hay turno de ma√±ana (8:00-15:00) y en algunos centros turno de tarde (15:00-22:00). Tambi√©n hay modalidad semipresencial\nYo trabajo los fines de semana y voy a clase por la ma√±ana. Es cansado pero se puede"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© salidas laborales tiene realmente? Mis padres est√°n preocupados por eso"}, {"role": "assistant", "content": "En inform√°tica hay mucha demanda. Mis compa√±eros de promoci√≥n anterior el 80% encontr√≥ trabajo en menos de 3 meses\nLos sueldos de entrada est√°n entre 18k-24k ‚Ç¨, y con experiencia suben r√°pido"}]}
{"messages": [{"role": "user", "content": "Eso est√° bien para empezar. ¬øY las pr√°cticas en empresa son remuneradas?"}, {"role": "assistant", "content": "S√≠, por ley tienen que darte una beca. En mi caso son 400‚Ç¨ al mes, pero en algunas empresas pagan m√°s\nLo importante es que muchas empresas cogen a los becarios despu√©s de las pr√°cticas"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© me recomiendas? ¬øGrado medio o ir directamente al superior?"}, {"role": "assistant", "content": "Con tu bachillerato ve directamente al superior. Los medios son m√°s b√°sicos y para gente sin bachillerato\nAdem√°s, con el superior tienes m√°s salidas y mejor sueldo"}]}
{"messages": [{"role": "user", "content": "Vale, tengo que mirar bien las fechas de preinscripci√≥n. ¬øCu√°ndo suele ser?"}, {"role": "assistant", "content": "Depende de cada comunidad, pero generalmente:\n- Junio/julio para el curso que viene\n- Septiembre para plazas sobrantes\nTe recomiendo que empieces a mirar en abril/mayo los plazos exactos"}]}
{"messages": [{"role": "user", "content": "¬øY qu√© documentos necesito para la preinscripci√≥n?"}, {"role": "assistant", "content": "DNI, certificado de notas de bachillerato, y luego depende del centro. Algunos piden foto, solicitud espec√≠fica...\nLo mejor es mirar en la web de educaci√≥n de tu comunidad aut√≥noma"}]}
{"messages": [{"role": "user", "content": "¬øY se puede solicitar en varios centros a la vez?"}, {"role": "assistant", "content": "S√≠, haces una lista ordenada por preferencia. Te asignan plaza seg√∫n tu nota y tus preferencias"}]}
{"messages": [{"role": "user", "content": "Perfecto! Otra duda: ¬øhay que comprar muchos libros? ¬øEs caro el material?"}, {"role": "assistant", "content": "En mi ciclo casi todo el material es digital. Los profes comparten apuntes y usamos software libre. El mayor gasto es un port√°til bueno\nYo me gast√© 600‚Ç¨ en un port√°til y con eso tengo para todo el ciclo"}]}
{"messages": [{"role": "user", "content": "Eso est√° bien. ¬øY las asignaturas son muy dif√≠ciles? Me preocupa especialmente la programaci√≥n"}, {"role": "assistant", "content": "Al principio cuesta, pero los profes empiezan desde cero. Lo importante es practicar mucho y no dejar cosas para el final\nYo no sab√≠a programar nada y ahora me defiendo bien. Hay que ser constante nada m√°s"}]}
{"messages": [{"role": "user", "content": "¬øY hay mucho trabajo en grupo? Esa es otra de mis preocupaciones"}, {"role": "assistant", "content": "S√≠, hay bastantes proyectos en grupo. Es buena preparaci√≥n para el mundo laboral donde todo es en equipo\nAl principio puede ser un rollo si te toca gente que no trabaja, pero aprendes a organizarte"}]}
{"messages": [{"role": "user", "content": "Vale, me lo estoy pensando seriamente. ¬øCrees que podr√≠a visitar tu centro alg√∫n d√≠a?"}, {"role": "assistant", "content": "Claro! En marzo suelen hacer jornadas de puertas abiertas. Tambi√©n puedo preguntar si puedes venir un d√≠a conmigo a alguna clase"}]}
{"messages": [{"role": "user", "content": "Eso ser√≠a genial! As√≠ veo c√≥mo es el ambiente"}, {"role": "assistant", "content": "Te aviso cuando sepa algo. Mientras tanto, te paso unos enlaces con informaci√≥n oficial\nhttps://www.todofp.es (este es el oficial del ministerio)\nY te recomiendo seguir en Instagram a @fp_informatica_ que comparten mucha info √∫til"}]}
{"messages": [{"role": "user", "content": "Much√≠simas gracias Mar√≠a! Me has aclarado un mont√≥n de dudas. Voy a mirar toda la informaci√≥n"}, {"role": "assistant", "content": "De nada! Cualquier otra duda me preguntas. √Ånimo con la decisi√≥n!"}]}
{"messages": [{"role": "user", "content": "Gracias! Hablamos pronto üòä\n\nHola Mar√≠a! Perdona que te moleste otra vez. Estoy mirando los m√≥dulos del ciclo de DAW y tengo dudas con las asignaturas"}, {"role": "assistant", "content": "Hola! No es molestia :) ¬øQu√© dudas tienes?"}]}
{"messages": [{"role": "user", "content": "Veo que hay asignaturas de programaci√≥n, bases de datos, lenguajes de marcas... ¬øCu√°les son las m√°s dif√≠ciles?"}, {"role": "assistant", "content": "Depende de cada persona. A m√≠ me cost√≥ m√°s Bases de Datos al principio, pero luego le coges el tranquillo\nProgramaci√≥n es la m√°s importante, se da durante los dos a√±os. Empiezas con Java y luego PHP, JavaScript..."}]}
{"messages": [{"role": "user", "content": "¬øY se dan muchos proyectos reales? Me interesa eso de hacer cosas pr√°cticas"}, {"role": "assistant", "content": "S√≠, en primero hacemos una web completa para un cliente ficticio, y en segundo un proyecto m√°s completo con app m√≥vil\nLo bueno es que al terminar tienes portfolio para mostrar en las entrevistas"}]}
{"messages": [{"role": "user", "content": "Genial! Otra cosa: ¬øhay que saber ingl√©s? Veo que algunas asignaturas tienen nombres en ingl√©s"}, {"role": "assistant", "content": "No es obligatorio, pero ayuda mucho. La documentaci√≥n t√©cnica est√° en ingl√©s y las empresas valoran mucho el ingl√©s t√©cnico\nEn algunos centros hay asignatura de ingl√©s t√©cnico opcional"}]}
{"messages": [{"role": "user", "content": "Vale, mi ingl√©s es regular pero puedo mejorarlo. ¬øY qu√© me dices del horario de tarde? Es el que me ir√≠a mejor"}, {"role": "assistant", "content": "El de tarde suele tener menos plazas pero tambi√©n menos demanda. La calidad es la misma, mismos profes y mismo temario"}]}
{"messages": [{"role": "user", "content": "Perfecto! Creo que me decanto por DAW en turno de tarde. Ahora toca esperar a la preinscripci√≥n"}, {"role": "assistant", "content": "Me alegro! Si necesitas ayuda con la solicitud me dices. Yo pas√© por eso el a√±o pasado"}]}
{"messages": [{"role": "user", "content": "Gracias Mar√≠a, eres un sol! Te tendr√© al tanto de c√≥mo va todo"}, {"role": "assistant", "content": "De nada! Suerte con todo üí™"}]}
{"messages": [{"role": "user", "content": "Buenos d√≠as Mar√≠a! Ya se abri√≥ la preinscripci√≥n y he solicitado DAW en tres centros. Cruzando los dedos!"}, {"role": "assistant", "content": "Qu√© bien! ¬øEn qu√© centros has puesto?"}]}
{"messages": [{"role": "user", "content": "1. IES Tu centro (el tuyo)\n2. IES Tecnol√≥gico\n3. IES Sistemas Inform√°ticos"}, {"role": "assistant", "content": "Ojal√° entres en el m√≠o! Ser√≠a genial tenerte de compa√±era el a√±o que viene"}]}
{"messages": [{"role": "user", "content": "Ojal√°! Por cierto, ¬øsabes cu√°ndo suelen publicar las listas?"}, {"role": "assistant", "content": "El a√±o pasado fue a finales de junio. Te llegar√° un email cuando salgan"}]}
{"messages": [{"role": "user", "content": "Vale, toca esperar entonces. ¬°Muchas gracias por toda tu ayuda!"}, {"role": "assistant", "content": "Para eso estamos! Cualquier cosa me escribes üòä"}]}
{"messages": [{"role": "user", "content": "MAR√çA!!! ME HAN ADMITIDO EN TU CENTRO!!! üòç"}, {"role": "assistant", "content": "NO ME DIGAS!!! ENHORABUENA!!! üéâüéâüéâ\nEstoy super contenta! Vas a ver c√≥mo te encanta"}]}
{"messages": [{"role": "user", "content": "No me lo creo a√∫n! Ten√≠a nota justa pero al final entr√©. Ahora toca formalizar la matr√≠cula"}, {"role": "assistant", "content": "Si necesitas ayuda con los papeles me dices. La matr√≠cula suele ser en julio"}]}
{"messages": [{"role": "user", "content": "Gracias! Te aviso si tengo dudas. Qu√© ilusi√≥n ser compa√±eras!"}, {"role": "assistant", "content": "A m√≠ tambi√©n me hace mucha ilusi√≥n! Bienvenida al mundo FP! üíª"}]}
```

### conversacion
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

El texto es un chat entre Ana y Mar√≠a, en el que Ana est√° considerando estudiar el ciclo formativo de Grado Medio de Desarrollo de Aplicaciones Web (DAW) y consulta a Mar√≠a sobre diversos aspectos del mismo. El intercambio abarca varios meses desde septiembre hasta junio.

**Resumen principal:**

- **Inicios:** Ana est√° considerando estudiar DAW pero tiene muchas dudas, incluyendo la dificultad de ciertas asignaturas, el horario y c√≥mo ser√≠a su experiencia en general.
  
- **Conversaciones iniciales (Septiembre 2023):** Ana consulta a Mar√≠a sobre las ventajas y desventajas del ciclo DAW. Mar√≠a le proporciona detalles sobre los m√≥dulos, la pr√°ctica real, la importancia de ingl√©s, costos asociados y posibilidades de visita al centro.

- **Consulta sobre asignaturas espec√≠ficas (Octubre 2023):** Ana se vuelve a poner en contacto con Mar√≠a para discutir m√°s espec√≠ficamente las dificultades potenciales de ciertas asignaturas y el contenido del horario, incluyendo proyectos reales e ingl√©s t√©cnico. Tambi√©n confirma que ha decidido estudiar DAW en turno de tarde.

- **Confirmaci√≥n de preinscripci√≥n (Abril 2024):** Ana notifica a Mar√≠a sobre la apertura de la preinscripci√≥n y le informa sobre los centros donde ha solicitado plaza, incluyendo el centro de Mar√≠a.

- **Resultado de admisi√≥n (Junio 2024):** Ana recibe la noticia de su aceptaci√≥n en el centro de Mar√≠a. Las dos expresan felicidad por esta noticia y se preparan para comenzar juntas el ciclo DAW el pr√≥ximo a√±o acad√©mico.

**Aspectos clave:**
- **Apoyo mutuo:** El texto refleja un fuerte v√≠nculo de apoyo entre Ana y Mar√≠a, con Mar√≠a brindando informaci√≥n crucial y √°nimo a Ana durante su proceso de toma de decisiones.
- **Detalles del ciclo DAW:** Se discuten varios aspectos clave del ciclo formativo como la estructura del programa, proyectos pr√°cticos, dificultad de las asignaturas y habilidades necesarias (como ingl√©s t√©cnico).
- **Proceso de admisi√≥n:** Se menciona el proceso para los estudiantes que buscan ingresar en un centro educativo, incluyendo preinscripci√≥n, espera por listas provisionales y formalizaci√≥n de matr√≠cula.
  
Este chat no solo es instructivo sino tambi√©n emotivo, reflejando la naturaleza importante de decisiones educativas y las relaciones personales que pueden surgir a partir de ellas.

`conversacion.txt`

```
Conversaci√≥n con Mar√≠a - CFGS

[28/09/2023 10:15] Ana: Hola Mar√≠a! ¬øC√≥mo est√°s? Quer√≠a preguntarte sobre los ciclos formativos porque s√© que est√°s estudiando uno
[28/09/2023 10:16] Mar√≠a: Hola Ana! Bien, gracias :) S√≠, estoy en segundo de Desarrollo de Aplicaciones Web. ¬øEn qu√© puedo ayudarte?
[28/09/2023 10:17] Ana: Me estoy planteando hacer un ciclo formativo pero tengo muchas dudas. No s√© si hacer uno de grado medio o superior
[28/09/2023 10:18] Mar√≠a: Te explico: los grados medios duran 2 a√±os y te dan acceso a grados superiores. Los superiores son 2 a√±os y dan acceso a la universidad
[28/09/2023 10:19] Mar√≠a: Depende de lo que quieras estudiar y si tienes el t√≠tulo de bachillerato o no
[28/09/2023 10:20] Ana: Tengo bachillerato, pero no s√© si quiero ir a la universidad directamente. Me gusta m√°s la parte pr√°ctica
[28/09/2023 10:21] Mar√≠a: Entonces los ciclos son perfectos! Son muy pr√°cticos y con buena salida laboral. ¬øQu√© te interesa?
[28/09/2023 10:22] Ana: Me gusta la inform√°tica y tambi√©n he visto algo de marketing. ¬øQu√© ciclos hay en esas √°reas?
[28/09/2023 10:23] Mar√≠a: En inform√°tica tienes:
- CFGS Desarrollo de Aplicaciones Web
- CFGS Desarrollo de Aplicaciones Multiplataforma
- CFGS Administraci√≥n de Sistemas Inform√°ticos en Red
- CFGS Ciberseguridad
[28/09/2023 10:24] Mar√≠a: En marketing:
- CFGS Marketing y Publicidad
- CFGS Comercio Internacional
- CFGS Transporte y Log√≠stica
[28/09/2023 10:25] Ana: Vaya, hay m√°s de lo que pensaba! ¬øY c√≥mo son las clases? ¬øMucha teor√≠a?
[28/09/2023 10:26] Mar√≠a: En mi experiencia, es 70% pr√°ctica y 30% teor√≠a. Hacemos proyectos reales, programamos, configuramos servidores...
[28/09/2023 10:27] Mar√≠a: Adem√°s, al final hay pr√°cticas en empresa de 3 meses, que es lo mejor para coger experiencia
[28/09/2023 10:28] Ana: Suena bien! ¬øY es dif√≠cil entrar? ¬øHay que hacer alguna prueba?
[28/09/2023 10:29] Mar√≠a: Con bachillerato entras directamente por nota media. Sin bachillerato hay que hacer prueba de acceso. La demanda es alta en inform√°tica
[28/09/2023 10:30] Ana: ¬øY qu√© nota suele hacer falta? Mi media es de 7.8
[28/09/2023 10:31] Mar√≠a: Con 7.8 entras en casi todos, aunque para Desarrollo Web el a√±o pasado la nota de corte fue 8.1 en algunos sitios
[28/09/2023 10:32] Mar√≠a: Pero hay muchos centros, seguro que encuentras plaza
[28/09/2023 10:33] Ana: ¬øY c√≥mo son los horarios? ¬øSe puede compaginar con trabajo?
[28/09/2023 10:34] Mar√≠a: Hay turno de ma√±ana (8:00-15:00) y en algunos centros turno de tarde (15:00-22:00). Tambi√©n hay modalidad semipresencial
[28/09/2023 10:35] Mar√≠a: Yo trabajo los fines de semana y voy a clase por la ma√±ana. Es cansado pero se puede
[28/09/2023 10:36] Ana: ¬øY qu√© salidas laborales tiene realmente? Mis padres est√°n preocupados por eso
[28/09/2023 10:37] Mar√≠a: En inform√°tica hay mucha demanda. Mis compa√±eros de promoci√≥n anterior el 80% encontr√≥ trabajo en menos de 3 meses
[28/09/2023 10:38] Mar√≠a: Los sueldos de entrada est√°n entre 18k-24k ‚Ç¨, y con experiencia suben r√°pido
[28/09/2023 10:39] Ana: Eso est√° bien para empezar. ¬øY las pr√°cticas en empresa son remuneradas?
[28/09/2023 10:40] Mar√≠a: S√≠, por ley tienen que darte una beca. En mi caso son 400‚Ç¨ al mes, pero en algunas empresas pagan m√°s
[28/09/2023 10:41] Mar√≠a: Lo importante es que muchas empresas cogen a los becarios despu√©s de las pr√°cticas
[28/09/2023 10:42] Ana: ¬øY qu√© me recomiendas? ¬øGrado medio o ir directamente al superior?
[28/09/2023 10:43] Mar√≠a: Con tu bachillerato ve directamente al superior. Los medios son m√°s b√°sicos y para gente sin bachillerato
[28/09/2023 10:44] Mar√≠a: Adem√°s, con el superior tienes m√°s salidas y mejor sueldo
[28/09/2023 10:45] Ana: Vale, tengo que mirar bien las fechas de preinscripci√≥n. ¬øCu√°ndo suele ser?
[28/09/2023 10:46] Mar√≠a: Depende de cada comunidad, pero generalmente:
- Junio/julio para el curso que viene
- Septiembre para plazas sobrantes
[28/09/2023 10:47] Mar√≠a: Te recomiendo que empieces a mirar en abril/mayo los plazos exactos
[28/09/2023 10:48] Ana: ¬øY qu√© documentos necesito para la preinscripci√≥n?
[28/09/2023 10:49] Mar√≠a: DNI, certificado de notas de bachillerato, y luego depende del centro. Algunos piden foto, solicitud espec√≠fica...
[28/09/2023 10:50] Mar√≠a: Lo mejor es mirar en la web de educaci√≥n de tu comunidad aut√≥noma
[28/09/2023 10:51] Ana: ¬øY se puede solicitar en varios centros a la vez?
[28/09/2023 10:52] Mar√≠a: S√≠, haces una lista ordenada por preferencia. Te asignan plaza seg√∫n tu nota y tus preferencias
[28/09/2023 10:53] Ana: Perfecto! Otra duda: ¬øhay que comprar muchos libros? ¬øEs caro el material?
[28/09/2023 10:54] Mar√≠a: En mi ciclo casi todo el material es digital. Los profes comparten apuntes y usamos software libre. El mayor gasto es un port√°til bueno
[28/09/2023 10:55] Mar√≠a: Yo me gast√© 600‚Ç¨ en un port√°til y con eso tengo para todo el ciclo
[28/09/2023 10:56] Ana: Eso est√° bien. ¬øY las asignaturas son muy dif√≠ciles? Me preocupa especialmente la programaci√≥n
[28/09/2023 10:57] Mar√≠a: Al principio cuesta, pero los profes empiezan desde cero. Lo importante es practicar mucho y no dejar cosas para el final
[28/09/2023 10:58] Mar√≠a: Yo no sab√≠a programar nada y ahora me defiendo bien. Hay que ser constante nada m√°s
[28/09/2023 10:59] Ana: ¬øY hay mucho trabajo en grupo? Esa es otra de mis preocupaciones
[28/09/2023 11:00] Mar√≠a: S√≠, hay bastantes proyectos en grupo. Es buena preparaci√≥n para el mundo laboral donde todo es en equipo
[28/09/2023 11:01] Mar√≠a: Al principio puede ser un rollo si te toca gente que no trabaja, pero aprendes a organizarte
[28/09/2023 11:02] Ana: Vale, me lo estoy pensando seriamente. ¬øCrees que podr√≠a visitar tu centro alg√∫n d√≠a?
[28/09/2023 11:03] Mar√≠a: Claro! En marzo suelen hacer jornadas de puertas abiertas. Tambi√©n puedo preguntar si puedes venir un d√≠a conmigo a alguna clase
[28/09/2023 11:04] Ana: Eso ser√≠a genial! As√≠ veo c√≥mo es el ambiente
[28/09/2023 11:05] Mar√≠a: Te aviso cuando sepa algo. Mientras tanto, te paso unos enlaces con informaci√≥n oficial
[28/09/2023 11:06] Mar√≠a: https://www.todofp.es (este es el oficial del ministerio)
[28/09/2023 11:07] Mar√≠a: Y te recomiendo seguir en Instagram a @fp_informatica_ que comparten mucha info √∫til
[28/09/2023 11:08] Ana: Much√≠simas gracias Mar√≠a! Me has aclarado un mont√≥n de dudas. Voy a mirar toda la informaci√≥n
[28/09/2023 11:09] Mar√≠a: De nada! Cualquier otra duda me preguntas. √Ånimo con la decisi√≥n!
[28/09/2023 11:10] Ana: Gracias! Hablamos pronto üòä

[15/10/2023 16:20] Ana: Hola Mar√≠a! Perdona que te moleste otra vez. Estoy mirando los m√≥dulos del ciclo de DAW y tengo dudas con las asignaturas
[15/10/2023 16:21] Mar√≠a: Hola! No es molestia :) ¬øQu√© dudas tienes?
[15/10/2023 16:22] Ana: Veo que hay asignaturas de programaci√≥n, bases de datos, lenguajes de marcas... ¬øCu√°les son las m√°s dif√≠ciles?
[15/10/2023 16:23] Mar√≠a: Depende de cada persona. A m√≠ me cost√≥ m√°s Bases de Datos al principio, pero luego le coges el tranquillo
[15/10/2023 16:24] Mar√≠a: Programaci√≥n es la m√°s importante, se da durante los dos a√±os. Empiezas con Java y luego PHP, JavaScript...
[15/10/2023 16:25] Ana: ¬øY se dan muchos proyectos reales? Me interesa eso de hacer cosas pr√°cticas
[15/10/2023 16:26] Mar√≠a: S√≠, en primero hacemos una web completa para un cliente ficticio, y en segundo un proyecto m√°s completo con app m√≥vil
[15/10/2023 16:27] Mar√≠a: Lo bueno es que al terminar tienes portfolio para mostrar en las entrevistas
[15/10/2023 16:28] Ana: Genial! Otra cosa: ¬øhay que saber ingl√©s? Veo que algunas asignaturas tienen nombres en ingl√©s
[15/10/2023 16:29] Mar√≠a: No es obligatorio, pero ayuda mucho. La documentaci√≥n t√©cnica est√° en ingl√©s y las empresas valoran mucho el ingl√©s t√©cnico
[15/10/2023 16:30] Mar√≠a: En algunos centros hay asignatura de ingl√©s t√©cnico opcional
[15/10/2023 16:31] Ana: Vale, mi ingl√©s es regular pero puedo mejorarlo. ¬øY qu√© me dices del horario de tarde? Es el que me ir√≠a mejor
[15/10/2023 16:32] Mar√≠a: El de tarde suele tener menos plazas pero tambi√©n menos demanda. La calidad es la misma, mismos profes y mismo temario
[15/10/2023 16:33] Ana: Perfecto! Creo que me decanto por DAW en turno de tarde. Ahora toca esperar a la preinscripci√≥n
[15/10/2023 16:34] Mar√≠a: Me alegro! Si necesitas ayuda con la solicitud me dices. Yo pas√© por eso el a√±o pasado
[15/10/2023 16:35] Ana: Gracias Mar√≠a, eres un sol! Te tendr√© al tanto de c√≥mo va todo
[15/10/2023 16:36] Mar√≠a: De nada! Suerte con todo üí™

[12/04/2024 09:30] Ana: Buenos d√≠as Mar√≠a! Ya se abri√≥ la preinscripci√≥n y he solicitado DAW en tres centros. Cruzando los dedos!
[12/04/2024 09:31] Mar√≠a: Qu√© bien! ¬øEn qu√© centros has puesto?
[12/04/2024 09:32] Ana: 1. IES Tu centro (el tuyo)
2. IES Tecnol√≥gico
3. IES Sistemas Inform√°ticos
[12/04/2024 09:33] Mar√≠a: Ojal√° entres en el m√≠o! Ser√≠a genial tenerte de compa√±era el a√±o que viene
[12/04/2024 09:34] Ana: Ojal√°! Por cierto, ¬øsabes cu√°ndo suelen publicar las listas?
[12/04/2024 09:35] Mar√≠a: El a√±o pasado fue a finales de junio. Te llegar√° un email cuando salgan
[12/04/2024 09:36] Ana: Vale, toca esperar entonces. ¬°Muchas gracias por toda tu ayuda!
[12/04/2024 09:37] Mar√≠a: Para eso estamos! Cualquier cosa me escribes üòä

[28/06/2024 11:15] Ana: MAR√çA!!! ME HAN ADMITIDO EN TU CENTRO!!! üòç
[28/06/2024 11:16] Mar√≠a: NO ME DIGAS!!! ENHORABUENA!!! üéâüéâüéâ
[28/06/2024 11:17] Mar√≠a: Estoy super contenta! Vas a ver c√≥mo te encanta
[28/06/2024 11:18] Ana: No me lo creo a√∫n! Ten√≠a nota justa pero al final entr√©. Ahora toca formalizar la matr√≠cula
[28/06/2024 11:19] Mar√≠a: Si necesitas ayuda con los papeles me dices. La matr√≠cula suele ser en julio
[28/06/2024 11:20] Ana: Gracias! Te aviso si tengo dudas. Qu√© ilusi√≥n ser compa√±eras!
[28/06/2024 11:21] Mar√≠a: A m√≠ tambi√©n me hace mucha ilusi√≥n! Bienvenida al mundo FP! üíª
```

### Actividades propuestas

El archivo `instrucciones_creacion_de_un_entorno_virtual.md` proporciona las instrucciones para crear y configurar un entorno virtual en Python. Aqu√≠ est√° el resumen de los pasos que se deben seguir:

1. **Crear un Entorno Virtual**: Se utiliza la herramienta integrada `venv` del m√≥dulo est√°ndar de Python para crear un nuevo entorno virtual llamado `.venv`.
   ```bash
   python3 -m venv .venv
   ```

2. **Activar el Entorno Virtual**: Antes de instalar paquetes, es necesario activar el entorno virtual reci√©n creado.
   ```bash
   source .venv/bin/activate
   ```
   
   > Nota: En sistemas Windows, la sintaxis para activar un entorno virtual es ligeramente diferente:
   >
   > ```cmd
   > .venv\Scripts\activate
   > ```

3. **Instalar Dependencias**: Una vez activado el entorno virtual, se instalan los paquetes necesarios que son espec√≠ficos del proyecto (como `transformers`, `datasets`, `peft` y `accelerate`) utilizando pip.
   ```bash
   pip install transformers datasets peff accelerate
   ```
   
4. **Instalar PyTorch**: Finalmente, se instala la biblioteca de aprendizaje profundo PyTorch en el entorno virtual para poder utilizarla junto con los otros paquetes necesarios.
   ```bash
   pip install torch
   ```

Al seguir estos pasos, puedes tener un entorno Python completamente configurado y aislado para tu proyecto que incluye las bibliotecas de aprendizaje profundo mencionadas.


<a id="entrenar-chatbot-a-partir-de-pdf"></a>
## entrenar chatbot a partir de pdf

### Introducci√≥n a los ejercicios

El c√≥digo proporcionado es un script Python dise√±ado para generar conjuntos de preguntas y respuestas (Q&A) a partir de transcripciones de texto almacenadas en archivos .txt o .md. El script utiliza un modelo de lenguaje llamado Ollama para la generaci√≥n de Q&A y guarda los resultados en archivos JSONL individuales. A continuaci√≥n, se desglosa el funcionamiento principal del c√≥digo:

### Preparaci√≥n Inicial
1. **Verificaci√≥n del Servicio de Ollama**: El script verifica si un servicio llamado Ollama est√° disponible y detecta la ruta correcta para interactuar con este servicio.
2. **Carga de Archivos de Entrada**: Se buscan archivos .txt o .md en una carpeta espec√≠fica (definida por `INPUT_DIR`), que contienen las transcripciones de texto a procesar.

### Procesamiento de los Ficheros
1. **Lectura y Preparaci√≥n de Bloques**:
   - Los archivos se leen y cualquier marcaje de Markdown se elimina.
   - El texto se divide en bloques m√°s peque√±os que pueden ser manejados individualmente para la generaci√≥n de Q&A.

2. **Procesamiento y Generaci√≥n de Q&A**:
   - Para cada bloque, el script genera tanto preguntas f√°ciles como avanzadas utilizando llamadas al servicio Ollama.
   - Los resultados (Q&A) se guardan inmediatamente en archivos JSONL individuales para cada archivo original.

3. **Barra de Progreso y Rastreo**:
   - Se utiliza una clase `ProgressTracker` para proporcionar un seguimiento visual del progreso global a medida que se procesan los bloques.
   
### Finalizaci√≥n y Resumen
1. **Resumen del Trabajo Realizado**: Al finalizar, el script muestra un resumen de cu√°ntos archivos fueron le√≠dos, cu√°ntos ya hab√≠an sido procesados antes y cu√°ntos nuevos fueron procesados en esta ejecuci√≥n, junto con el n√∫mero total de pares Q&A generados.

### Detalles T√©cnicos
- **Funciones Clave**:
  - `prepare_blocks_for_file`: Lee un archivo y divide su contenido en bloques m√°s peque√±os.
  - `generate_qa_for_block`: Genera conjuntos de preguntas y respuestas para cada bloque utilizando el servicio Ollama.
  - `process_single_file`: Procesa un √∫nico archivo, generando Q&A para todos sus bloques individuales y guard√°ndolos en un archivo JSONL.

- **Estructura del Directorio**:
  - El c√≥digo asume que los archivos de entrada est√°n en una carpeta espec√≠fica (`INPUT_DIR`).
  - Los resultados (archivos JSONL) se escriben en otra carpeta (`OUTPUT_DIR`).

### Uso
El script es √∫til para la generaci√≥n autom√°tica de conjuntos de preguntas y respuestas a partir de grandes cantidades de texto estructurado. Esto puede ser particularmente √∫til en aplicaciones educativas, entrenamiento de modelos de lenguaje, o cualquier otro caso donde se necesiten preguntas basadas en el contenido del texto.

### Mejoras Potenciales
- **Paralelizaci√≥n**: El procesamiento podr√≠a beneficiarse de la paralelizaci√≥n para manejar archivos grandes m√°s r√°pidamente.
- **Optimizaciones de Eficiencia**: El c√≥digo podr√≠a ser mejorado para optimizar a√∫n m√°s los pasos de lectura y escritura de archivos.

Este script proporciona una base s√≥lida para generar Q&A a partir de transcripciones de texto, con potencial para ser adaptado o extendido seg√∫n las necesidades espec√≠ficas del usuario.

### prompt
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este c√≥digo Python es una herramienta que genera pares de preguntas y respuestas (Q&A) en formato JSON Lines a partir de transcripciones o textos t√©cnicos. Su prop√≥sito principal es entrenar modelos de lenguaje con material educativo, generando tanto preguntas f√°ciles como avanzadas para diferentes conceptos.

El script funciona de la siguiente manera:

1. **Configuraci√≥n y Utilidades B√°sicas**: Define constantes que configuran el comportamiento del script, incluyendo rutas a carpetas de entrada y salida, y par√°metros para dividir el texto en bloques m√°s peque√±os (chunks) con solape.

2. **Gesti√≥n del Log**: El c√≥digo mantiene un archivo log.json que registra los archivos de texto ya procesados. Esto ayuda a evitar duplicar el trabajo sobre archivos que ya han sido trabajados anteriormente.

3. **Interacci√≥n con Ollama**: Usa una API llamada Ollama para generar las preguntas y respuestas. El modelo llama3:8b (o un similar) es utilizado para responder a prompts generados por la herramienta, produciendo el contenido de Q&A en formato JSON Lines.

4. **Procesamiento de Textos**: Para cada archivo de texto en la carpeta especificada (`INPUT_DIR`), divide el texto en bloques m√°s peque√±os y luego genera preguntas tanto f√°ciles como avanzadas para cada bloque utilizando los prompts adecuados y llamando a Ollama.

5. **Generaci√≥n de Q&A**: El script genera pares de preguntas y respuestas, asegur√°ndose que sigan un formato espec√≠fico en JSON Lines, que es clave para el entrenamiento del modelo de lenguaje.

6. **Salida Final**: Cada archivo de entrada procesado resulta en un archivo JSONL individual con sus respectivos pares Q&A almacenados en la carpeta `OUTPUT_DIR`. Adem√°s, el script actualiza el log.json para registrar qu√© archivos han sido procesados y cu√°les no.

Este c√≥digo es fundamental para preparar material educativo estructurado de forma que pueda ser utilizado eficazmente por modelos de lenguaje para aprender conceptos t√©cnicos o acad√©micos. La capacidad de generar tanto preguntas simples como avanzadas permite un enfoque m√°s completo en el entrenamiento del modelo. 

El script tambi√©n incluye manejo b√°sico de errores y registro de progreso, lo que facilita su uso en entornos de producci√≥n donde puede ser necesario procesar muchos archivos con regularidad. Esto asegura la consistencia y eficiencia del proceso de generaci√≥n de Q&A.

`001-prompt.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano.

- Recorre todos los .txt de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama (Llama3 8B):
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Pensado para funcionar con:
- Ubuntu 24.04
- Ollama sirviendo en http://localhost:11434
- Modelo llama3:8b (o similar) cargado en GPU (GTX 1660).
"""

import os
import json
import requests
from typing import List, Dict

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Modelo de Ollama
OLLAMA_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "llama3:8b"   # ajusta si usas otro tag, p.ej. "llama3:8b-instruct"

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite de generaci√≥n aproximado (Ollama lo ajusta internamente)


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.txt",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# LLAMADA A OLLAMA
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": False,
        # "options": {"num_ctx": 2048, ...}  # opcional
    }

    resp = requests.post(OLLAMA_URL, json=payload)
    resp.raise_for_status()
    data = resp.json()
    return data["message"]["content"]


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, intents, etc. Intentamos limpiar.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            # Intenta recortar desde el primer '{' hasta el √∫ltimo '}'
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO
# =========================

def process_single_file(path: str) -> List[Dict[str, str]]:
    print(f"\n[INFO] Procesando archivo: {path}")
    raw_text = read_text_file(path)
    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignora.")
        return []

    text = normalize_whitespace(raw_text)
    blocks = split_into_blocks(text)
    print(f"[INFO] Texto troceado en {len(blocks)} bloques.")

    file_pairs: List[Dict[str, str]] = []

    for i, block in enumerate(blocks, start=1):
        print(f"[INFO]   Bloque {i}/{len(blocks)} (longitud {len(block)} chars)")
        block_pairs = generate_qa_for_block(block)
        print(f"[INFO]   -> {len(block_pairs)} pares Q/A generados para este bloque.")
        file_pairs.extend(block_pairs)

    print(f"[INFO] Total pares Q/A para {os.path.basename(path)}: {len(file_pairs)}")
    return file_pairs


# =========================
# ESCRITURA DE SALIDA
# =========================

def write_pairs_to_jsonl(pairs: List[Dict[str, str]], output_path: str):
    """
    Escribe TODOS los pares en un JSONL (sobrescribe si ya existe).
    """
    with open(output_path, "w", encoding="utf-8") as f:
        for p in pairs:
            f.write(json.dumps(p, ensure_ascii=False) + "\n")


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    txt_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith(".txt")
    ]

    if not txt_files:
        print(f"[INFO] No se han encontrado .txt en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = 0

    for path in sorted(txt_files):
        if path in processed_files:
            print(f"[INFO] Archivo ya procesado seg√∫n log.json, se omite: {path}")
            skipped_count += 1
            continue

        pairs = process_single_file(path)

        # Nombre del JSONL por archivo
        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        write_pairs_to_jsonl(pairs, per_file_output)
        total_pairs += len(pairs)

        # Actualizar log y guardar inmediatamente para no perder progreso
        log.setdefault("processed_files", []).append(path)
        save_log(log)
        processed_files.add(path)
        newly_processed_count += 1

        print(f"[INFO] JSONL generado para {path}: {per_file_output}")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(txt_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### pdf a md
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este fragmento de c√≥digo es una funci√≥n llamada `pdf_to_markdown` que convierte un archivo PDF en texto Markdown. La funci√≥n toma dos argumentos: el nombre del archivo PDF que se va a convertir y el nombre del archivo Markdown donde se guardar√° el resultado.

Primero, la funci√≥n abre el archivo PDF usando la biblioteca PyMuPDF (importada como `fitz`). Luego, utiliza otra biblioteca llamada `pymupdf4llm` para convertir todo el contenido del documento PDF en texto Markdown. Finalmente, guarda este texto Markdown en un nuevo archivo con el nombre especificado.

El bloque `if __name__ == "__main__":` asegura que la funci√≥n se ejecute solo cuando el script es ejecutado directamente (no si es importado como m√≥dulo por otro script). En este caso, convierte espec√≠ficamente un archivo PDF llamado "Resumen m√≥dulos profesionales SMR_DAM_DAW.pdf" ubicado en una carpeta llamada `pdf`, y guarda el resultado en un archivo Markdown llamado "resumen.md" dentro de la carpeta `input`.

Este tipo de script es √∫til cuando necesitas transformar documentos PDF para que sean m√°s f√°ciles de leer o procesar en formatos como Markdown, especialmente si est√°s trabajando con texto y no gr√°ficos complejos.

`003-pdf a md.py`

```python
import fitz  # PyMuPDF
import pymupdf4llm

def pdf_to_markdown(input_pdf, output_md):
    # Open the PDF
    doc = fitz.open(input_pdf)
    
    # Convert entire document to Markdown
    md_text = pymupdf4llm.to_markdown(doc)
    
    # Save to .md file
    with open(output_md, "w", encoding="utf-8") as f:
        f.write(md_text)

if __name__ == "__main__":
    pdf_to_markdown("pdf/Resumen m√≥dulos profesionales SMR_DAM_DAW.pdf", "input/resumen.md")
```

### prueba
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

Este fragmento de c√≥digo es una funci√≥n que permite enviar una pregunta a un modelo de inteligencia artificial llamado "gemma2" mediante una solicitud web y recibir una respuesta. Primero, importa dos bibliotecas necesarias: `requests` para manejar las solicitudes HTTP y `json` para trabajar con datos JSON.

La funci√≥n `ask_ollama` toma como entrada un texto (una pregunta) y devuelve la respuesta del modelo inteligente. Crea un diccionario llamado `payload`, que contiene informaci√≥n sobre el modelo a usar (`MODEL`) y la pregunta introducida por el usuario (`prompt`). Luego, env√≠a una solicitud POST a la URL especificada en `OLLAMA_URL` con este payload.

El c√≥digo maneja posibles errores de conexi√≥n o respuesta utilizando `response.raise_for_status()`, lo que garantiza que se detendr√° si algo sale mal. Despu√©s, convierte la respuesta recibida en un formato JSON y extrae espec√≠ficamente el valor asociado a la clave `"response"`, que es donde est√° almacenada la respuesta del modelo.

En la parte principal del script (dentro de `if __name__ == "__main__":`), se solicita al usuario que ingrese una pregunta, luego llama a la funci√≥n `ask_ollama` para obtener la respuesta y finalmente imprime esa respuesta en pantalla. Esta estructura es √∫til porque permite interactuar f√°cilmente con el modelo desde un terminal o l√≠nea de comandos.

`005-prueba.py`

```python
import requests
import json

OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL = "gemma2:9b-instruct-q4_0"

def ask_ollama(question: str) -> str:
    payload = {
        "model": MODEL,
        "prompt": question,
        "stream": False   # set to True if you want streaming tokens
    }

    response = requests.post(OLLAMA_URL, json=payload)
    response.raise_for_status()

    data = response.json()
    return data.get("response", "")

if __name__ == "__main__":
    user_input = input("Enter your question: ")
    answer = ask_ollama(user_input)
    print("\n=== Ollama Response ===\n")
    print(answer)
```

### md a jsonl
<small>Creado: 2025-11-27 19:26</small>

#### Explicaci√≥n

El script proporcionado se centra en la generaci√≥n autom√°tica de pares de preguntas y respuestas (Q&A) a partir de transcripciones t√©cnicas o documentos Markdown, utilizando un modelo entrenado en un servicio llamado Ollama. Aqu√≠ est√° una descripci√≥n detallada del funcionamiento principal:

1. **Configuraci√≥n Inicial**:
   - Asegura que las carpetas necesarias existen.
   - Verifica la conectividad con el servicio de Ollama y detecta autom√°ticamente cu√°l es el endpoint correcto para generar Q&A.

2. **Lectura de Archivos de Entrada**:
   - Busca archivos `.txt` o `.md` en la carpeta `INPUT_DIR`.
   
3. **Procesamiento de Cada Archivo**:
   - Lee y limpia el contenido del archivo Markdown.
   - Divide el texto en bloques m√°s peque√±os para facilitar la generaci√≥n de Q&A.
   - Para cada bloque, genera preguntas f√°ciles e intermedias/avanzadas utilizando diferentes prompts y formatea las respuestas como JSON Lines.

4. **Guardado de Resultados**:
   - Guarda inmediatamente los pares de preguntas y respuestas generados en archivos `.jsonl` dentro de la carpeta `OUTPUT_DIR`.
   
5. **Registro del Proceso**:
   - Mantiene un registro detallado de qu√© archivos se han procesado en un archivo `log.json`.

### Detalles T√©cnicos

- **Limpieza y Normalizaci√≥n**: El script utiliza t√©cnicas para quitar elementos Markdown como encabezados, listas, etc., y normalizar espacios innecesarios en el texto.
  
- **Divisi√≥n de Texto**: Divide el texto largo en bloques m√°s peque√±os (por defecto 1024 caracteres) para manejar mejor la generaci√≥n de Q&A.

- **LLM Integration (Generador de Preguntas y Respuestas)**:
   - Utiliza un sistema externo (Ollama) que se supone est√° configurado y puede generar preguntas e respuestas bas√°ndose en prompts proporcionados.
   
### Flujo del Proceso

1. Se buscan archivos `.txt` o `.md` en la carpeta `INPUT_DIR`.
2. Por cada archivo, el contenido es le√≠do y limpiado de elementos Markdown.
3. El texto resultante se divide en bloques m√°s peque√±os para facilitar su procesamiento.
4. Para cada bloque, se generan dos tipos diferentes de pares Q&A: preguntas f√°ciles (introductorias) y preguntas intermedias/avanzadas.
5. Los resultados son almacenados en archivos `.jsonl` dentro del directorio `OUTPUT_DIR`.
6. Un registro detallado del proceso se mantiene en el archivo `log.json`, indicando qu√© archivos han sido procesados.

### Consideraciones

- **Dependencia Externa**: El script depende de una interfaz con un modelo entrenado (Ollama), lo que significa que para funcionar correctamente, este servicio debe estar configurado y disponible.
  
- **Escalabilidad y Robustez**:
   - La limpieza del texto asegura la consistencia en el formato de salida.
   - El dividir el documento en bloques m√°s peque√±os facilita un manejo m√°s eficiente del procesamiento para conjuntos de datos grandes.

El script ofrece una soluci√≥n escalable y automatizada para generar pares Q&A desde documentos t√©cnicos o Markdown, lo cual puede ser muy √∫til en contextos educativos o de aprendizaje autom√°tico.

`006-md a jsonl.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano o Markdown.

- Recorre todos los .txt y .md de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.

Requisitos:
- Ollama sirviendo en http://localhost:11434
- Modelo MODEL_NAME presente en Ollama.
"""

import os
import re
import json
import requests
from typing import List, Dict, Optional

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.

    - Quita bloques de c√≥digo triple ``` ... ```
    - Quita c√≥digo en l√≠nea `...`
    - Convierte enlaces [texto](url) en solo "texto"
    - Quita im√°genes ![alt](url)
    - Quita cabeceras de Markdown (#, ##, ###)
    - Quita marcadores de lista iniciales (-, *, +) al inicio de l√≠nea
    - Quita negritas/cursivas (**texto**, *texto*, __texto__, _texto_)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.md",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.

    Devuelve True si alguno de los dos endpoints funciona, False si ninguno.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).

    - En modo "chat": usa /api/chat con messages.
    - En modo "generate": concatena system_prompt + user_prompt en un solo prompt
      y llama a /api/generate.
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, etc. Limpiamos.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str, output_path: str) -> int:
    """
    Procesa un √∫nico fichero de entrada y va escribiendo las Q/A
    en JSONL sobre la marcha en output_path.

    Devuelve el n√∫mero total de pares generados.
    """
    print(f"\n[INFO] Procesando archivo: {path}")
    raw_text = read_text_file(path)
    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignora.")
        return 0

    # 1) Limpiar Markdown
    cleaned = strip_markdown(raw_text)
    # 2) Normalizar espacios
    text = normalize_whitespace(cleaned)

    if not text.strip():
        print("[WARN] Tras limpiar Markdown el archivo qued√≥ vac√≠o, se ignora.")
        return 0

    blocks = split_into_blocks(text)
    print(f"[INFO] Texto troceado en {len(blocks)} bloques.")

    # Truncar el fichero de salida al inicio, por si existe de ejecuciones anteriores
    with open(output_path, "w", encoding="utf-8") as f:
        pass

    total_pairs_for_file = 0

    for i, block in enumerate(blocks, start=1):
        print(f"[INFO]   Bloque {i}/{len(blocks)} (longitud {len(block)} chars)")

        block_pairs = generate_qa_for_block(block)
        print(f"[INFO]   -> {len(block_pairs)} pares Q/A generados para este bloque.")

        # Guardar inmediatamente en el JSONL
        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

    print(f"[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    input_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith((".txt", ".md"))
    ]

    if not input_files:
        print(f"[INFO] No se han encontrado .txt ni .md en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = 0

    for path in sorted(input_files):
        if path in processed_files:
            print(f"[INFO] Archivo ya procesado seg√∫n log.json, se omite: {path}")
            skipped_count += 1
            continue

        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        pairs_count = process_single_file(path, per_file_output)
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        processed_files.add(path)
        newly_processed_count += 1

        print(f"[INFO] JSONL generado para {path}: {per_file_output}")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### streaming
<small>Creado: 2025-11-27 19:27</small>

#### Explicaci√≥n

Este script Python es una herramienta completa para generar conjuntos de preguntas y respuestas (Q&A) basadas en documentos Markdown o texto plano. Aqu√≠ est√° un resumen detallado de sus funcionalidades principales:

1. **Configuraci√≥n Inicial**:
   - Verifica la presencia de directorios necesarios.
   - Detecta el endpoint correcto del servicio Ollama (un modelo de lenguaje grande) para generar Q&A.

2. **Lectura y Preprocesamiento de Documentos**:
   - Lee archivos `.txt` o `.md` desde una carpeta de entrada (`INPUT_DIR`).
   - Elimina el formato Markdown, normaliza espacios en blanco, y divide el texto en bloques manejables seg√∫n su longitud.

3. **Generaci√≥n de Q&A**:
   - Para cada bloque, genera preguntas f√°ciles e intermedias/avanzadas usando prompts espec√≠ficos.
   - Asegura que las respuestas sigan un formato JSON Lines est√°ndar para facilitar el almacenamiento y la posterior lectura.

4. **Guardado en Ficheros JSONL**:
   - Escribir los pares Q&A generados directamente a ficheros `.jsonl` para cada archivo de entrada procesado.
   - Mantener un registro del estado de procesamiento en un fichero `log.json`.

5. **Funciones y M√©todos Clave**:

   - `ensure_dirs()`: Crea directorios necesarios si no existen.
   - `strip_markdown(text)`: Elimina el formato Markdown del texto dado.
   - `normalize_whitespace(cleaned)`: Normaliza espacios en blanco para mejorar la legibilidad de los bloques.
   - `split_into_blocks(text, max_chars=MAX_CHARS_PER_BLOCK)`: Divide un documento largo en bloques m√°s peque√±os para procesarlos individualmente.
   - `generate_qa_for_block(block: str) -> List[Dict[str, str]]`: Genera pares Q&A f√°ciles e intermedias avanzadas a partir de un bloque de texto dado.

6. **L√≥gica Principal**:
   - Procesa cada archivo de entrada en orden alfab√©tico.
   - Permite el seguimiento del progreso y evita la re-procesaci√≥n de archivos ya generados seg√∫n registros previos.

7. **Salida Final**:
   - Genera un resumen final que incluye el n√∫mero total de pares Q&A generados, los archivos procesados y los que se saltaron debido a ser ya procesados en ejecuciones anteriores.

Este script es especialmente √∫til para la creaci√≥n automatizada de conjuntos de datos de entrenamiento para modelos de lenguaje, espec√≠ficamente para la capacitaci√≥n en comprensi√≥n de lectura y generaci√≥n de respuestas precisas.

`007-streaming.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano o Markdown.

- Recorre todos los .txt y .md de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.

Requisitos:
- Ollama sirviendo en http://localhost:11434
- Modelo MODEL_NAME presente en Ollama.
"""

import os
import re
import json
import requests
from typing import List, Dict, Optional

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.

    - Quita bloques de c√≥digo triple ``` ... ```
    - Quita c√≥digo en l√≠nea `...`
    - Convierte enlaces [texto](url) en solo "texto"
    - Quita im√°genes ![alt](url)
    - Quita cabeceras de Markdown (#, ##, ###)
    - Quita marcadores de lista iniciales (-, *, +) al inicio de l√≠nea
    - Quita negritas/cursivas (**texto**, *texto*, __texto__, _texto_)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.md",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.

    Devuelve True si alguno de los dos endpoints funciona, False si ninguno.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).

    - En modo "chat": usa /api/chat con messages.
    - En modo "generate": concatena system_prompt + user_prompt en un solo prompt
      y llama a /api/generate.
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, etc. Limpiamos.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str, output_path: str) -> int:
    """
    Procesa un √∫nico fichero de entrada y va escribiendo las Q/A
    en JSONL sobre la marcha en output_path.

    Devuelve el n√∫mero total de pares generados.
    """
    print(f"\n[INFO] Procesando archivo: {path}")
    raw_text = read_text_file(path)
    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignora.")
        return 0

    # 1) Limpiar Markdown
    cleaned = strip_markdown(raw_text)
    # 2) Normalizar espacios
    text = normalize_whitespace(cleaned)

    if not text.strip():
        print("[WARN] Tras limpiar Markdown el archivo qued√≥ vac√≠o, se ignora.")
        return 0

    blocks = split_into_blocks(text)
    print(f"[INFO] Texto troceado en {len(blocks)} bloques.")

    # Truncar el fichero de salida al inicio, por si existe de ejecuciones anteriores
    with open(output_path, "w", encoding="utf-8") as f:
        pass

    total_pairs_for_file = 0

    for i, block in enumerate(blocks, start=1):
        print(f"[INFO]   Bloque {i}/{len(blocks)} (longitud {len(block)} chars)")

        block_pairs = generate_qa_for_block(block)
        print(f"[INFO]   -> {len(block_pairs)} pares Q/A generados para este bloque.")

        # Guardar inmediatamente en el JSONL
        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

    print(f"[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    input_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith((".txt", ".md"))
    ]

    if not input_files:
        print(f"[INFO] No se han encontrado .txt ni .md en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = 0

    for path in sorted(input_files):
        if path in processed_files:
            print(f"[INFO] Archivo ya procesado seg√∫n log.json, se omite: {path}")
            skipped_count += 1
            continue

        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        pairs_count = process_single_file(path, per_file_output)
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        processed_files.add(path)
        newly_processed_count += 1

        print(f"[INFO] JSONL generado para {path}: {per_file_output}")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### estadisticas
<small>Creado: 2025-11-27 23:49</small>

#### Explicaci√≥n

Este script es un ejemplo completo y detallado de c√≥mo utilizar el servicio Ollama para generar preguntas y respuestas (Q/A) a partir de transcripciones o documentos. El c√≥digo est√° bien estructurado en varias funciones que manejan diferentes aspectos del proceso, desde la preparaci√≥n del entorno hasta la generaci√≥n final de Q/A.

### Resumen del Flujo Principal

1. **Configuraci√≥n Inicial**:
   - Verifica que los directorios necesarios existan.
   - Detecta el servicio Ollama y selecciona el endpoint correcto.

2. **Lectura de Archivos de Entrada**:
   - Busca archivos `.txt` o `.md` en un directorio espec√≠fico (`INPUT_DIR`) y filtra aquellos que a√∫n no han sido procesados seg√∫n un archivo de log.

3. **Preparaci√≥n del Contenido**:
   - Lee cada archivo, limpia el Markdown y divide el contenido en bloques m√°s peque√±os para facilitar la generaci√≥n de Q/A.

4. **Generaci√≥n de Preguntas y Respuestas**:
   - Para cada bloque generado, utiliza Ollama con un sistema predefinido que genera preguntas f√°ciles e intermedias/avanzadas.
   - Guarda las respuestas en formato JSONL para cada archivo original.

5. **Actualizaci√≥n del Log de Procesamiento**:
   - Mantiene un registro detallado de qu√© archivos han sido procesados y cu√°ntas Q/A se han generado para cada uno.

6. **Reporte Final**:
   - Sumariza el proceso, incluyendo los archivos que fueron nuevos o ya hab√≠an sido procesados, el n√∫mero total de bloques y pares Q/A generados.

### Estructura del C√≥digo

- **Funciones Auxiliares**: `ensure_dirs`, `load_log`, `save_log` manejan la estructura del directorio y el registro del proceso.
- **Preparaci√≥n de Texto**: `read_text_file`, `strip_markdown`, `normalize_whitespace`, `split_into_blocks` limpian y dividen el texto en bloques manejables.
- **Llamada a Ollama**: `generate_qa_for_block` utiliza los modelos de Ollama para generar Q/A basadas en los bloques preparados.
- **Procesamiento de Archivos**:
  - `process_single_file` maneja la generaci√≥n y escritura inmediata del JSONL para cada archivo procesado.

### Consideraciones Especiales

1. **Uso de Progreso**: El script utiliza una barra de progreso que se actualiza conforme a los bloques procesados, lo cual es √∫til cuando se trabaja con muchos archivos o grandes cantidades de texto.
2. **Manejo de Errores**:
   - Proporciona mensajes de error y advertencias para situaciones como archivos vac√≠os o problemas al acceder al servicio Ollama.
3. **Estructura del Log**: El script mantiene un registro detallado en `log.json` que incluye los archivos ya procesados, permitiendo continuar la generaci√≥n donde se qued√≥ anteriormente.

### Ejecuci√≥n

Para ejecutar este script:
1. Aseg√∫rate de tener configurado el servicio Ollama con el modelo adecuado.
2. Coloca tus transcripciones en el directorio `INPUT_DIR`.
3. Ejecuta el script.

Este es un ejemplo s√≥lido y bien estructurado que demuestra c√≥mo integrar llamadas a servicios como Ollama para procesamiento automatizado de texto.

`008-estadisticas.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de transcripciones en texto plano o Markdown.

- Recorre todos los .txt y .md de INPUT_DIR.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.
- Muestra una barra de progreso global con:
    * porcentaje completado,
    * tiempo transcurrido,
    * tiempo estimado restante (ETA).
"""

import os
import re
import json
import time
import shutil
import requests
from typing import List, Dict, Optional

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpetas de entrada y salida
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.

    - Quita bloques de c√≥digo triple ``` ... ```
    - Quita c√≥digo en l√≠nea `...`
    - Convierte enlaces [texto](url) en solo "texto"
    - Quita im√°genes ![alt](url)
    - Quita cabeceras de Markdown (#, ##, ###)
    - Quita marcadores de lista iniciales (-, *, +) al inicio de l√≠nea
    - Quita negritas/cursivas (**texto**, *texto*, __texto__, _texto_)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text] if text else []

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "inputs/tema1.txt",
            "inputs/tema2.md",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# TRACKER DE PROGRESO
# =========================

class ProgressTracker:
    """
    Barra de progreso global basada en n√∫mero total de bloques.
    Muestra:
    - porcentaje completado
    - tiempo transcurrido
    - ETA estimada
    """

    def __init__(self, total_units: int):
        self.total = max(1, total_units)
        self.current = 0
        self.start_time = time.time()

    @staticmethod
    def _format_seconds(secs: float) -> str:
        secs = int(secs)
        h = secs // 3600
        m = (secs % 3600) // 60
        s = secs % 60
        if h > 0:
            return f"{h:02d}:{m:02d}:{s:02d}"
        return f"{m:02d}:{s:02d}"

    def update(self, step: int = 1, prefix: str = ""):
        self.current += step
        if self.current > self.total:
            self.current = self.total

        elapsed = time.time() - self.start_time
        percent = (self.current / self.total) * 100.0

        if self.current > 0:
            rate = elapsed / self.current
            remaining = rate * (self.total - self.current)
        else:
            remaining = 0.0

        try:
            term_width = shutil.get_terminal_size((80, 20)).columns
        except Exception:
            term_width = 80

        bar_len = max(10, term_width - 55)
        filled = int(bar_len * self.current / self.total)
        bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)

        msg = (
            f"{prefix} [{bar}] {percent:6.2f}% "
            f"| t+{self._format_seconds(elapsed)} "
            f"| ETA {self._format_seconds(remaining)}"
        )

        # Recortar al ancho de la terminal para evitar artefactos
        msg = msg[:term_width - 1]
        print("\r" + msg, end="", flush=True)

    def finish(self, prefix: str = ""):
        self.update(step=0, prefix=prefix)
        print()  # salto de l√≠nea final


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.

    Devuelve True si alguno de los dos endpoints funciona, False si ninguno.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).

    - En modo "chat": usa /api/chat con messages.
    - En modo "generate": concatena system_prompt + user_prompt en un solo prompt
      y llama a /api/generate.
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de transcripci√≥n t√©cnica en espa√±ol.
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    """
    Prompt para preguntas f√°ciles / introductorias.
    """
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    """
    Prompt para preguntas intermedias / avanzadas.
    """
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        # A veces el modelo a√±ade vi√±etas, etc. Limpiamos.
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        # Asegurarse de que empieza por { y acaba en }
        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# PREPARACI√ìN DE CADA FICHERO (LECTURA + TROCEO)
# =========================

def prepare_blocks_for_file(path: str) -> List[str]:
    """
    Lee el fichero, limpia Markdown y espacios, y lo trocea en bloques.
    Si queda vac√≠o, devuelve [].
    """
    print(f"\n[INFO] Preparando archivo: {path}")
    raw_text = read_text_file(path)

    if not raw_text.strip():
        print("[WARN] Archivo vac√≠o, se ignorar√° (0 bloques).")
        return []

    cleaned = strip_markdown(raw_text)
    text = normalize_whitespace(cleaned)

    if not text.strip():
        print("[WARN] Tras limpiar Markdown el archivo qued√≥ vac√≠o, se ignorar√° (0 bloques).")
        return []

    blocks = split_into_blocks(text)
    print(f"[INFO]   -> {len(blocks)} bloques detectados.")
    return blocks


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    """
    Genera Q/A f√°ciles y avanzadas para un bloque.
    """
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str,
                        output_path: str,
                        blocks: List[str],
                        tracker: Optional[ProgressTracker],
                        file_index: int,
                        total_files: int) -> int:
    """
    Procesa un √∫nico fichero de entrada y va escribiendo las Q/A
    en JSONL sobre la marcha en output_path.

    Devuelve el n√∫mero total de pares generados.
    """
    print(f"\n[INFO] Procesando archivo {file_index}/{total_files}: {path}")

    if not blocks:
        # Generar un JSONL vac√≠o para dejar constancia de que se proces√≥
        with open(output_path, "w", encoding="utf-8"):
            pass
        print("[INFO] Archivo sin bloques, JSONL vac√≠o generado.")
        return 0

    # Truncar el fichero de salida al inicio, por si existe de ejecuciones anteriores
    with open(output_path, "w", encoding="utf-8"):
        pass

    total_pairs_for_file = 0

    for block in blocks:
        block_pairs = generate_qa_for_block(block)

        # Guardar inmediatamente en el JSONL
        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

        # Actualizar progreso global (un "unit" por bloque)
        if tracker is not None:
            tracker.update(
                step=1,
                prefix=f"[PROGRESO] Archivo {file_index}/{total_files}"
            )

    print(f"\n[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    print(f"[INFO] JSONL generado para {path}: {output_path}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    input_files = [
        os.path.join(INPUT_DIR, fn)
        for fn in os.listdir(INPUT_DIR)
        if fn.lower().endswith((".txt", ".md"))
    ]

    if not input_files:
        print(f"[INFO] No se han encontrado .txt ni .md en {INPUT_DIR}.")
        print("      Crea la carpeta y coloca tus transcripciones all√≠.")
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    # Filtrar solo los archivos pendientes de procesar
    pending_files = [p for p in sorted(input_files) if p not in processed_files]

    if not pending_files:
        print("[INFO] Todos los archivos presentes ya estaban procesados seg√∫n log.json.")
        print(f"[INFO] Log de materiales procesados: {LOG_FILE}")
        return

    # Primera pasada: preparar bloques por fichero y contar bloques totales
    print("\n[INFO] Calculando n√∫mero total de bloques para la barra de progreso global...")
    file_blocks_map: Dict[str, List[str]] = {}
    total_blocks = 0

    for path in pending_files:
        blocks = prepare_blocks_for_file(path)
        file_blocks_map[path] = blocks
        total_blocks += len(blocks)

    if total_blocks == 0:
        print("[WARN] No se han encontrado bloques de texto √∫tiles en los ficheros pendientes.")
        print("       Se actualizar√° el log, pero no se generar√°n Q/A.")
        # Aun as√≠ generamos JSONL vac√≠os para dejar constancia
        newly_processed_count = 0
        for path in pending_files:
            base_name = os.path.splitext(os.path.basename(path))[0]
            per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")
            with open(per_file_output, "w", encoding="utf-8"):
                pass
            log.setdefault("processed_files", []).append(path)
            save_log(log)
            newly_processed_count += 1

        print("\n[RESUMEN]")
        print(f"Archivos encontrados              : {len(input_files)}")
        print(f"Archivos ya procesados (skip)     : {len(input_files) - len(pending_files)}")
        print(f"Archivos procesados en esta run   : {newly_processed_count}")
        print(f"Pares Q/A generados en esta run   : 0")
        print(f"Log de materiales procesados      : {LOG_FILE}")
        print(f"JSONL individuales en             : {OUTPUT_DIR}")
        return

    tracker = ProgressTracker(total_blocks)

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = len(input_files) - len(pending_files)

    print(f"[INFO] Total de bloques a procesar: {total_blocks}")
    print("[INFO] Iniciando generaci√≥n de Q/A con barra de progreso global...\n")

    for idx, path in enumerate(pending_files, start=1):
        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        blocks = file_blocks_map.get(path, [])
        pairs_count = process_single_file(
            path=path,
            output_path=per_file_output,
            blocks=blocks,
            tracker=tracker,
            file_index=idx,
            total_files=len(pending_files)
        )
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        newly_processed_count += 1

    # Cerrar visualmente la barra de progreso
    tracker.finish(prefix="[PROGRESO]")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### Actividades propuestas

Este script Python realiza la generaci√≥n autom√°tica de preguntas y respuestas (Q&A) basadas en transcripciones o documentos proporcionados. Aqu√≠ est√° un resumen del flujo y las caracter√≠sticas clave:

### Flujo Principal

1. **Configuraci√≥n Inicial**:
   - Verifica que los directorios necesarios existan.
   - Detecta el endpoint correcto de Ollama (un servicio de generaci√≥n de texto basado en IA).
   
2. **Lectura y Preprocesamiento**:
   - Lee todos los archivos `.txt` y `.md` desde un directorio espec√≠fico (`INPUT_DIR`).
   - Identifica cu√°les ya fueron procesados antes mediante un archivo de registro (`log.json`).

3. **Preparaci√≥n de Bloques**:
   - Para cada archivo pendiente, limpia el contenido (Markdown, espacios en blanco innecesarios) y lo divide en bloques m√°s peque√±os.

4. **Generaci√≥n de Q&A**:
   - Para cada bloque, genera preguntas f√°ciles e intermedias usando un modelo de lenguaje.
   - Escribe las respuestas generadas directamente a archivos JSONL individuales para cada archivo original.

5. **Seguimiento del Progreso**:
   - Usa una barra de progreso global para mostrar el avance en la generaci√≥n de preguntas y respuestas.
   
6. **Registro Final**:
   - Actualiza un registro (log.json) con informaci√≥n sobre qu√© archivos fueron procesados y cu√°ntos pares de Q&A se generaron.

### Caracter√≠sticas Clave

- **Procesamiento Incremental**: S√≥lo procesa los archivos que a√∫n no han sido procesados previamente.
  
- **Preprocesamiento Avanzado**: Limpia el contenido del Markdown, normaliza espacios en blanco y divide los documentos largos en bloques m√°s manejables.

- **Integraci√≥n con Ollama**: Usa un servicio de generaci√≥n de texto basada en IA para generar preguntas y respuestas. 

- **Progreso Visual**: Muestra una barra de progreso visual del proceso global y detallado.

### Uso

Este script ser√≠a √∫til para crear grandes conjuntos de datos de entrenamiento o pruebas para sistemas de inteligencia artificial que necesiten Q&A, especialmente desde documentos largos y complejos. Requiere un servicio externo (Ollama) que genera texto basado en prompts proporcionados.

### Consideraciones

- Aseg√∫rate de tener instaladas todas las dependencias del script antes de ejecutarlo.
- Configura correctamente los directorios (`INPUT_DIR` y `OUTPUT_DIR`) donde se encontrar√°n los documentos de entrada y se guardar√°n las respuestas generadas, respectivamente.
  
Este script es un ejemplo avanzado del uso de la IA para automatizar tareas repetitivas y complejas en el procesamiento de texto.


<a id="scrapeador-web-y-entrenamiento"></a>
## scrapeador web y entrenamiento

### Introducci√≥n a los ejercicios

Este script Python es una herramienta para generar conjuntos de preguntas y respuestas (Q&A) a partir de documentos multiformato. Aqu√≠ hay un resumen del flujo principal y las caracter√≠sticas clave:

1. Preparaci√≥n:
   - Asegura la existencia de directorios necesarios.
   - Detecta el servicio Ollama para generar Q&A.

2. Recopilaci√≥n de archivos:
   - Busca recursivamente en INPUT_DIR archivos con extensiones v√°lidas (txt, md, html, htm, pdf).

3. Procesamiento:
   - Prepara bloques de texto para cada archivo.
   - Genera Q&A usando un modelo de Ollama para preguntas f√°ciles y avanzadas.

4. Salida:
   - Escribe resultados en archivos JSONL individuales por documento.
   - Guarda un log que registra qu√© archivos se procesaron.

5. Resumen final:
   - Proporciona estad√≠sticas sobre el proceso completado.

Caracter√≠sticas importantes:

- Manejo de m√∫ltiples formatos (txt, md, html, pdf).
- Procesamiento por bloques para manejar grandes documentos.
- Uso de un modelo de lenguaje en la nube (Ollama) para generar Q&A.
- Registro detallado del proceso en archivos y logs.
- Salida individualizada para cada archivo procesado.

Funciones clave:

- `prepare_blocks_for_file`: Lee y normaliza el contenido del archivo, dividi√©ndolo en bloques manejables.
- `generate_qa_for_block`: Genera Q&A usando Ollama para un bloque de texto dado.
- `process_single_file`: Llama a la funci√≥n anterior por cada bloque de un archivo espec√≠fico, guardando resultados y actualizando el progreso.

El script es modular y bien estructurado, con funciones espec√≠ficas para tareas como normalizar texto, dividir en bloques, generar Q&A y manejar la salida. Tambi√©n incluye caracter√≠sticas √∫tiles como seguimiento del progreso y registro de logs.

### Entrar en una web
<small>Creado: 2025-12-02 17:16</small>

#### Explicaci√≥n

Este c√≥digo es un ejemplo b√°sico de c√≥mo usar la biblioteca `requests` en Python para hacer una solicitud HTTP GET a una p√°gina web y obtener su contenido HTML. Primero, se importa el m√≥dulo `requests`. Luego, se especifica la URL de la p√°gina que queremos visitar, en este caso, "https://tameformacion.com/". 

El bloque `try` intenta realizar la solicitud a la p√°gina web con un l√≠mite de tiempo de 10 segundos para asegurar que el programa no queda esperando indefinidamente si hay problemas de conexi√≥n. Si la solicitud es exitosa (es decir, obtiene una respuesta del servidor), guarda el contenido HTML en la variable `html_content` y lo imprime en pantalla.

Si ocurre alg√∫n error durante la ejecuci√≥n de la solicitud, como problemas de red o un c√≥digo de estado HTTP que indica un problema (como 404 Not Found), se captura esa excepci√≥n y se muestra un mensaje de error en lugar del contenido HTML. Este manejo de errores es importante para asegurar que el programa pueda informar sobre los problemas que surgen durante la ejecuci√≥n sin interrumpirse completamente.

`001-Entrar en una web.py`

```python
import requests

url = "https://tameformacion.com/"

try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()  # Raises an error for 4xx/5xx responses

    html_content = response.text
    print(html_content)

except requests.exceptions.RequestException as e:
    print(f"Error obtaining the page: {e}")
```

### guardar web
<small>Creado: 2025-12-02 17:17</small>

#### Explicaci√≥n

Este c√≥digo Python descarga el contenido HTML de una p√°gina web y lo guarda en un archivo local. Primero, importa las bibliotecas `requests` y `os`. La URL desde donde se obtiene el HTML es "https://tameformacion.com/". Tambi√©n define una carpeta llamada "paginas_html" donde guardar√° los archivos HTML descargados.

El c√≥digo verifica si la carpeta de salida existe y, si no, la crea. Luego intenta hacer una solicitud GET a la URL especificada con un l√≠mite de tiempo de 10 segundos. Si la respuesta es exitosa (c√≥digo HTTP 200), el contenido HTML se guarda en un archivo llamado "example.html" dentro de la carpeta definida.

Si ocurre alg√∫n error durante la solicitud, como no poder conectar a la URL o el servidor responde con un c√≥digo de estado que indica un problema, el programa captura la excepci√≥n y muestra un mensaje de error descriptivo. Esto ayuda a identificar r√°pidamente qu√© sali√≥ mal si algo va mal durante la ejecuci√≥n del script.

Este tipo de programa es √∫til para crear copias locales de sitios web, por ejemplo, para an√°lisis, documentaci√≥n o pruebas sin necesidad de tener conexi√≥n con el servidor original cada vez que se necesita acceder al contenido.

`002-guardar web.py`

```python
import requests
import os

url = "https://tameformacion.com/"
output_folder = "paginas_html"
output_file = "example.html"

# Create folder if it does not exist
os.makedirs(output_folder, exist_ok=True)

try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()

    html_content = response.text

    # Full path for the saved file
    filepath = os.path.join(output_folder, output_file)

    # Save HTML to disk
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"HTML saved to: {filepath}")

except requests.exceptions.RequestException as e:
    print(f"Error: {e}")
```

### adivinar nombre de la web
<small>Creado: 2025-12-02 17:19</small>

#### Explicaci√≥n

Este c√≥digo es una herramienta simple que te permite guardar el contenido de una p√°gina web como un archivo HTML local en tu computadora. Primero, importa algunas bibliotecas necesarias para hacer solicitudes a la web y manejar archivos y fechas.

El c√≥digo hace dos cosas principales:

1. **Descarga la p√°gina web**: Usa `requests.get()` para descargar el contenido de la URL "https://tameformacion.com/". Si la descarga es exitosa, guarda el HTML en una variable llamada `html_content`.

2. **Determina y guarda el nombre del archivo**: Utiliza dos funciones para decidir c√≥mo llamar al archivo que se guardar√°: `get_filename_from_url()` intenta obtener un nombre de archivo a partir de la URL (por ejemplo, "index.html" o cualquier otro nombre de archivo espec√≠fico en la URL). Si no puede extraer un nombre de archivo adecuado desde la URL, usa `generate_hash_filename()`, que genera un nombre √∫nico basado en una huella digital del contenido de la URL y el tiempo actual. Finalmente, guarda el HTML descargado en este archivo.

Este tipo de script es √∫til cuando necesitas guardar copias locales de sitios web para analizar su estructura o datos sin depender siempre de estar conectado a Internet. Adem√°s, el uso de un hash garantiza que cada archivo tenga un nombre √∫nico, lo cual ayuda a evitar problemas de sobrescritura y facilita la identificaci√≥n √∫nica de los archivos descargados.

`003-adivinar nombre de la web.py`

```python
import requests
import os
import hashlib
from datetime import datetime
from urllib.parse import urlparse

url = "https://tameformacion.com/"
output_folder = "paginas_html"

# Create folder if it does not exist
os.makedirs(output_folder, exist_ok=True)

def get_filename_from_url(url: str) -> str:
    """Returns a filename based on the URL path, if possible."""
    parsed = urlparse(url)
    path = parsed.path.strip("/")

    # If URL ends with a filename (e.g., page.php, index.html)
    if "." in path:
        return path

    # If URL has a path but no extension (e.g., /contact/)
    if path:
        return path + ".html"

    # If nothing can be extracted, return empty string
    return ""


def generate_hash_filename(url: str) -> str:
    """Generates filename using hash + datetime."""
    sha1 = hashlib.sha1(url.encode()).hexdigest()[:12]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{sha1}_{timestamp}.html"


try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    html_content = response.text

    # Determine filename
    filename = get_filename_from_url(url)
    if not filename:
        filename = generate_hash_filename(url)

    filepath = os.path.join(output_folder, filename)

    # Save HTML
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"HTML saved to: {filepath}")

except requests.exceptions.RequestException as e:
    print(f"Error: {e}")
```

### timeout entre paginas
<small>Creado: 2025-12-02 17:23</small>

#### Explicaci√≥n

Este c√≥digo es un simple programa de web scraping que recorre sitios web para guardar sus p√°ginas HTML en archivos locales. Su objetivo principal es comenzar desde una URL inicial (en este caso, "https://tameformacion.com/") y descargar hasta 200 p√°ginas, guardando cada p√°gina en tu sistema con un nombre de archivo √∫nico.

El programa utiliza varios m√≥dulos de Python como `requests`, que maneja las solicitudes HTTP; `os` para trabajar con el sistema operativo; `hashlib` para generar nombres de archivo √∫nicos usando hashes; y `time` para controlar la velocidad a la que se obtienen las p√°ginas (en este caso, espera un segundo entre cada solicitud).

La configuraci√≥n inicial del c√≥digo define varias variables importantes:
- **START_URL**: La URL desde donde comienza el scrap.
- **OUTPUT_FOLDER**: El directorio donde se guardan los archivos HTML descargados.
- **MAX_PAGES**: Un l√≠mite de seguridad para evitar recorrer demasiadas p√°ginas por error o malicia.

El c√≥digo tiene varias funciones auxiliares que normalizan URLs, generan nombres de archivo √∫nicos (que pueden incluir hashes para URLs sin extensiones v√°lidas), y filtran los enlaces encontrados en las p√°ginas web para asegurar que solo se siguen aquellos que apuntan a contenido relevante.

La funci√≥n principal `crawl` es la encargada de ejecutar todo el proceso. Comienza por agregar la URL inicial al conjunto de URLs visitadas (`visited`) y a una cola (`queue`). Luego, en un bucle infinito mientras haya p√°ginas en la cola y no se alcance el l√≠mite de p√°ginas definido, extrae las URLs desde la cola, las normaliza y verifica que a√∫n no hayan sido visitadas. Si todo est√° en orden, hace una solicitud GET a la URL actual, espera por un segundo para respetar `REQUEST_DELAY_SECONDS`, y guarda el contenido HTML resultante.

Es importante notar que este tipo de web scraping debe hacerse con cuidado para evitar sobrecargar los servidores del sitio web visitado. El c√≥digo incluye una pausa entre las solicitudes (`time.sleep(REQUEST_DELAY_SECONDS)`) para dar tiempo a la p√°gina en cuesti√≥n de responder adecuadamente a cada solicitud.

Este programa es √∫til para aprender sobre c√≥mo recopilar informaci√≥n web autom√°ticamente y puede ser una base para proyectos m√°s avanzados que involucran el an√°lisis o procesamiento de grandes cantidades de datos desde sitios web.

`004-timeout entre paginas.py`

```python
import requests
import os
import hashlib
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, urlunparse
from collections import deque

from bs4 import BeautifulSoup

# -------------------- CONFIGURATION --------------------
START_URL = "https://tameformacion.com/"
OUTPUT_FOLDER = "paginas_html"
MAX_PAGES = 200                     # Safety limit
REQUEST_DELAY_SECONDS = 1          # <<< Delay between pages
# -------------------------------------------------------


# ---------- Helpers for filenames ----------

def get_filename_from_url(url: str) -> str:
    parsed = urlparse(url)
    path = parsed.path.strip("/")

    if "." in path:
        return path

    if path:
        return path + ".html"

    return ""


def generate_hash_filename(url: str) -> str:
    sha1 = hashlib.sha1(url.encode()).hexdigest()[:12]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{sha1}_{timestamp}.html"


def get_output_path_for_url(url: str) -> str:
    filename = get_filename_from_url(url)
    if not filename:
        filename = generate_hash_filename(url)

    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    return os.path.join(OUTPUT_FOLDER, filename)


# ---------- Helpers for normalization / filtering ----------

def normalize_url(url: str) -> str:
    parsed = urlparse(url)
    parsed = parsed._replace(fragment="")
    return urlunparse(parsed)


def same_domain(url1: str, url2: str) -> bool:
    n1 = urlparse(url1).netloc.lower()
    n2 = urlparse(url2).netloc.lower()

    if n1.startswith("www."):
        n1 = n1[4:]
    if n2.startswith("www."):
        n2 = n2[4:]

    return n1 == n2


def is_interesting_link(href: str) -> bool:
    if not href:
        return False
    href = href.strip()
    if href.startswith("#"):
        return False
    if href.startswith("mailto:"):
        return False
    if href.startswith("tel:"):
        return False
    if href.lower().startswith("javascript:"):
        return False
    return True


# ---------- Main crawler ----------

def crawl(start_url: str):
    visited = set()
    queue = deque([start_url])

    while queue and len(visited) < MAX_PAGES:
        current_url = queue.popleft()
        current_url = normalize_url(current_url)

        if current_url in visited:
            continue

        print(f"[{len(visited)+1}] Fetching: {current_url}")
        visited.add(current_url)

        try:
            response = requests.get(current_url, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"  Error fetching {current_url}: {e}")
            continue

        # Respect delay between requests
        print(f"  Waiting {REQUEST_DELAY_SECONDS} seconds...")
        time.sleep(REQUEST_DELAY_SECONDS)

        content_type = response.headers.get("Content-Type", "")
        if "text/html" not in content_type:
            print(f"  Skipping non-HTML content: {content_type}")
            continue

        html_content = response.text

        filepath = get_output_path_for_url(current_url)
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"  Saved to: {filepath}")
        except OSError as e:
            print(f"  Error saving {filepath}: {e}")

        soup = BeautifulSoup(html_content, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not is_interesting_link(href):
                continue

            new_url = urljoin(current_url, href)
            new_url = normalize_url(new_url)

            if same_domain(start_url, new_url) and new_url not in visited:
                queue.append(new_url)


if __name__ == "__main__":
    crawl(START_URL)
    print("Crawl finished.")
```

### tambien quiero los pdf
<small>Creado: 2025-12-02 17:32</small>

#### Explicaci√≥n

Este c√≥digo es un programa que sirve para recopilar y guardar documentos (especialmente PDFs) y p√°ginas web HTML desde una URL de inicio. El objetivo principal es crear un scrapador web simple pero funcional, capaz de navegar a trav√©s del sitio web proporcionado e indexar su contenido.

El script comienza configurando algunos par√°metros clave como la p√°gina inicial donde empieza el recorrido (`START_URL`), la carpeta de salida donde se guardar√°n los documentos y p√°ginas HTML (`OUTPUT_FOLDER`), un l√≠mite m√°ximo de p√°ginas para evitar bucles infinitos (`MAX_PAGES`) y un retraso entre las solicitudes a las p√°ginas web (`REQUEST_DELAY_SECONDS`). 

El programa utiliza varias funciones auxiliares:

1. **Generaci√≥n de nombres de archivos**: Las funciones `get_filename_from_url`, `generate_hash_filename` y `get_output_path_for_url` ayudan a determinar c√≥mo llamar a los documentos guardados, bas√°ndose en la URL y el tipo de contenido (por ejemplo, si es HTML o PDF).

2. **Normalizaci√≥n y filtrado de URLs**: Las funciones `normalize_url` y `same_domain` aseguran que las URLs est√©n correctamente formadas y se filtran para incluir solo aquellas que pertenecen al mismo dominio del sitio web inicial.

3. **Detecci√≥n de enlaces relevantes**: La funci√≥n `is_interesting_link` elimina los enlaces a menudo innecesarios como fragmentos, correos electr√≥nicos o llamadas telef√≥nicas para centrarse solo en los que apuntan a otras partes del sitio web.

El bucle principal se encarga de recoger la p√°gina actual, descargar su contenido y guardar tanto los PDFs como las p√°ginas HTML. Para cada URL visitada, si el tipo de contenido es HTML, el programa busca nuevos enlaces dentro de ella y los a√±ade a una cola para ser procesados en el futuro. Esto asegura que todo el sitio web se explore de manera sistem√°tica hasta alcanzar el l√≠mite configurado de p√°ginas.

Este tipo de scraper es √∫til para crear un respaldo local de un sitio web, recopilar datos espec√≠ficos o incluso entrenar modelos de aprendizaje autom√°tico con material en l√≠nea.

`005-tambien quiero los pdf.py`

```python
import requests
import os
import hashlib
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, urlunparse
from collections import deque

from bs4 import BeautifulSoup

# -------------------- CONFIGURATION --------------------
START_URL = "https://tameformacion.com/"
OUTPUT_FOLDER = "paginas_html"
MAX_PAGES = 200              # Safety limit
REQUEST_DELAY_SECONDS = 1    # Delay between requests
# -------------------------------------------------------


# ---------- Helpers for filenames ----------

def get_filename_from_url(url: str) -> str:
    """
    Try to extract a filename from the URL path.
    If the path contains a dot, we assume it has an extension (e.g. .html, .php, .pdf).
    """
    parsed = urlparse(url)
    path = parsed.path.lstrip("/")  # OJO: no strip(), solo lstrip para conservar subcarpetas

    if "." in os.path.basename(path):
        # Si la √∫ltima parte tiene extensi√≥n, devolvemos toda la ruta relativa
        return path  # p.ej. wp-content/uploads/2023/12/GD_xxx.pdf

    if path:
        # No hay extensi√≥n expl√≠cita, asumimos HTML y mantenemos ruta
        return os.path.join(path, "index.html") if path.endswith("/") else path + ".html"

    # Ra√≠z del sitio ‚Üí nombre por defecto
    return ""


def generate_hash_filename(url: str, ext: str = ".html") -> str:
    """
    Generates filename using SHA-1 hash of the URL + datetime, with the given extension.
    """
    sha1 = hashlib.sha1(url.encode()).hexdigest()[:12]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if not ext.startswith("."):
        ext = "." + ext
    return f"{sha1}_{timestamp}{ext}"


def get_output_path_for_url(url: str, content_type: str | None = None) -> str:
    """
    Decide the filename based on the URL and content type.
    - If URL already has a filename/path, use it.
    - If not, use hash + datetime with appropriate extension.
    """
    filename = get_filename_from_url(url)

    if not filename:
        # Decide extension based on content-type
        ext = ".html"
        if content_type:
            ct = content_type.lower()
            if "pdf" in ct:
                ext = ".pdf"
        filename = generate_hash_filename(url, ext=ext)

    # Ruta absoluta final
    full_path = os.path.join(OUTPUT_FOLDER, filename)

    # Asegurar que existe el directorio padre (paginas_html + posibles subcarpetas)
    parent_dir = os.path.dirname(full_path)
    if parent_dir:
        os.makedirs(parent_dir, exist_ok=True)

    return full_path


# ---------- Helpers for normalization / filtering ----------

def normalize_url(url: str) -> str:
    parsed = urlparse(url)
    # Remove fragment (#...)
    parsed = parsed._replace(fragment="")
    return urlunparse(parsed)


def same_domain(url1: str, url2: str) -> bool:
    n1 = urlparse(url1).netloc.lower()
    n2 = urlparse(url2).netloc.lower()

    if n1.startswith("www."):
        n1 = n1[4:]
    if n2.startswith("www."):
        n2 = n2[4:]

    return n1 == n2


def is_interesting_link(href: str) -> bool:
    if not href:
        return False
    href = href.strip()
    if href.startswith("#"):
        return False
    if href.startswith("mailto:"):
        return False
    if href.startswith("tel:"):
        return False
    if href.lower().startswith("javascript:"):
        return False
    return True


# ---------- Main crawler ----------

def crawl(start_url: str):
    visited = set()
    queue = deque([start_url])

    while queue and len(visited) < MAX_PAGES:
        current_url = queue.popleft()
        current_url = normalize_url(current_url)

        if current_url in visited:
            continue

        print(f"[{len(visited)+1}] Fetching: {current_url}")
        visited.add(current_url)

        try:
            response = requests.get(current_url, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"  Error fetching {current_url}: {e}")
            continue

        # Respect delay between requests
        print(f"  Waiting {REQUEST_DELAY_SECONDS} seconds...")
        time.sleep(REQUEST_DELAY_SECONDS)

        content_type = response.headers.get("Content-Type", "")
        ct_lower = content_type.lower()

        # -------- Save PDF files --------
        if "application/pdf" in ct_lower or current_url.lower().endswith(".pdf"):
            filepath = get_output_path_for_url(current_url, content_type=content_type)
            try:
                with open(filepath, "wb") as f:
                    f.write(response.content)
                print(f"  [PDF] Saved to: {filepath}")
            except OSError as e:
                print(f"  Error saving PDF {filepath}: {e}")
            # No parse links inside PDF
            continue

        # -------- Process HTML pages --------
        if "text/html" in ct_lower:
            html_content = response.text

            filepath = get_output_path_for_url(current_url, content_type=content_type)
            try:
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"  [HTML] Saved to: {filepath}")
            except OSError as e:
                print(f"  Error saving HTML {filepath}: {e}")
                continue

            # Parse links and enqueue same-domain URLs
            soup = BeautifulSoup(html_content, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"]
                if not is_interesting_link(href):
                    continue

                new_url = urljoin(current_url, href)
                new_url = normalize_url(new_url)

                if same_domain(start_url, new_url) and new_url not in visited:
                    queue.append(new_url)

        else:
            # Non-HTML, non-PDF content (images, css, etc.) ‚Üí ignore
            print(f"  Skipping content type: {content_type}")

    print("Crawl finished.")


if __name__ == "__main__":
    crawl(START_URL)
```

### entrenar IA con todos los materiales
<small>Creado: 2025-12-02 18:38</small>

#### Explicaci√≥n

El script que has proporcionado es una herramienta bastante completa para generar pares de preguntas y respuestas (Q&A) a partir de varios tipos de archivos (textos, HTML, PDFs). Aqu√≠ hay un resumen de su estructura y funcionalidad:

1. **Configuraci√≥n inicial**:
   - Define constantes como directorios de entrada y salida, extensiones v√°lidas para los ficheros de entrada, y configuraciones del modelo.
   - Verifica la existencia del servicio Ollama y determina el endpoint correcto.

2. **Recorrido de archivos**:
   - Recorre recursivamente `INPUT_DIR` en busca de archivos con las extensiones permitidas (`txt`, `md`, `html`, `htm`, `pdf`).

3. **Procesamiento de archivos**:
   - Lee cada archivo y lo divide en bloques m√°s peque√±os para facilitar el procesamiento.
   - Genera Q&A por bloque, utilizando un modelo Ollama configurado previamente.

4. **Rastreo del progreso**:
   - Mantiene un seguimiento detallado de la cantidad total de bloques y avanza en una barra de progreso global.

5. **Generaci√≥n y guardado de resultados**:
   - Para cada archivo, se generan Q&A y se escriben en archivos JSONL individuales.
   - Se actualiza el registro de qu√© archivos han sido procesados para futuras ejecuciones del script.

6. **Resumen final**:
   - Proporciona un resumen detallado del proceso completo, incluyendo estad√≠sticas sobre el n√∫mero total de archivos encontrados y procesados, bloques generados, etc.

### Requerimientos principales
- Python 3.x
- `requests` para la interacci√≥n con Ollama
- Otros m√≥dulos como `os`, `json`, `re` (para expresiones regulares), `PIL` (para manipulaci√≥n de im√°genes en archivos PDF)

### Mejoras potenciales
1. **Validaciones adicionales**: Aseg√∫rate de manejar correctamente los casos donde las solicitudes a Ollama fallan.
2. **Documentaci√≥n mejorada**: Agregar documentaci√≥n detallada para cada funci√≥n y m√≥dulo para facilitar su mantenimiento y extensi√≥n.
3. **Informes personalizados**: Integrar un sistema para enviar informes de progreso y resumen final por correo electr√≥nico o a otra herramienta de seguimiento.

### Ejecuci√≥n del script
Para ejecutar el script, simplemente aseg√∫rate de tener todos los requisitos necesarios instalados (`pip install -r requirements.txt` si tienes un archivo `requirements.txt`) y luego ejecuta:

```bash
python generar_qa.py
```

Este script es una soluci√≥n robusta para la generaci√≥n automatizada de Q&A desde documentos variados, lo que puede ser muy √∫til en contextos educativos o de formaci√≥n.

`006-entrenar IA con todos los materiales.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generador de pares Q/A en formato JSONL a partir de HTML, PDF, texto plano o Markdown.

- Recorre recursivamente todos los ficheros de INPUT_DIR con extensi√≥n:
    .html, .htm, .pdf, .txt, .md
- Para HTML:
    * Lee el archivo.
    * Limpia el markup (quita <script>, <style>, etc.).
    * Extrae el texto relevante.
- Para PDF:
    * Extrae el texto con PyPDF2.
- Para .txt / .md:
    * Igual que antes: limpia Markdown si procede.
- Trocea cada texto en bloques con solape.
- Para cada bloque lanza 2 prompts a Ollama:
    * Preguntas f√°ciles / introductorias.
    * Preguntas intermedias / avanzadas.
- Genera UN JSONL POR ARCHIVO de entrada en OUTPUT_DIR.
- Mantiene un log.json con la lista de ficheros ya procesados para no duplicar materiales.

Caracter√≠sticas extra:
- Comprueba al inicio si Ollama est√° accesible.
- Detecta autom√°ticamente si debe usar /api/chat o /api/generate.
- Guarda las Q/A sobre la marcha (bloque a bloque) en el JSONL, sin esperar al final.
- Muestra una barra de progreso global con:
    * porcentaje completado,
    * tiempo transcurrido,
    * tiempo estimado restante (ETA).
"""

import os
import re
import json
import time
import shutil
import requests
from typing import List, Dict, Optional

from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

# Carpeta de entrada: aqu√≠ pondr√°s tu 'paginas_html'
INPUT_DIR = "paginas_html"

# Carpeta de salida (JSONL)
OUTPUT_DIR = "outputs"
LOG_FILE = os.path.join(OUTPUT_DIR, "log.json")

# Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
# Ajusta aqu√≠ el modelo que vayas a usar
MODEL_NAME = "gemma2:9b-instruct-q4_0"

# Estos se rellenar√°n en detect_ollama_mode()
OLLAMA_MODE: Optional[str] = None   # "chat" o "generate"
OLLAMA_URL: Optional[str] = None    # URL completa del endpoint elegido

# Troceado del texto
MAX_CHARS_PER_BLOCK = 3500  # tama√±o objetivo de cada bloque
BLOCK_OVERLAP = 500         # solape entre bloques para no perder contexto

# Generaci√≥n de Q/A
TEMPERATURE = 0.3           # baja = m√°s estable
MAX_TOKENS = 512            # l√≠mite aproximado de tokens generados

# Extensiones aceptadas
VALID_EXTS = (".html", ".htm", ".pdf", ".txt", ".md")


# =========================
# UTILIDADES B√ÅSICAS
# =========================

def ensure_dirs():
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()


def normalize_whitespace(text: str) -> str:
    # Quita espacios duplicados, saltos de l√≠nea raros, etc.
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """
    Elimina en lo posible el "ruido" de Markdown para dejar solo texto √∫til.
    (Igual que en tu script original)
    """

    # Bloques de c√≥digo triple
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)

    # C√≥digo en l√≠nea `code`
    text = re.sub(r"`([^`]*)`", r"\1", text)

    # Im√°genes ![alt](url)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)

    # Enlaces [texto](url) -> texto
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # Cabeceras tipo #, ##, ### al inicio de l√≠nea
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)

    # Marcadores de lista al inicio de l√≠nea: -, *, +
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)

    # Negritas/cursivas: **texto**, *texto*, __texto__, _texto_
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "")

    return text


def split_into_blocks(text: str,
                      max_chars: int = MAX_CHARS_PER_BLOCK,
                      overlap: int = BLOCK_OVERLAP) -> List[str]:
    """
    Trocea el texto en bloques de tama√±o aproximado `max_chars`,
    con un solape de `overlap` caracteres entre bloques consecutivos.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text] if text else []

    blocks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + max_chars, n)

        # Intentar cortar cerca de un final de frase (., ?, !)
        split_pos = end
        for sep in [".", "?", "!", "¬ø", "¬°"]:
            pos = text.rfind(sep, start + int(max_chars * 0.6), end)
            if pos != -1 and pos > start:
                split_pos = max(split_pos, pos + 1)

        if split_pos == end:  # no encontr√≥ nada razonable
            split_pos = end

        block = text[start:split_pos].strip()
        if block:
            blocks.append(block)

        if split_pos >= n:
            break

        # Retrocede un poco para crear solape
        start = max(0, split_pos - overlap)

    return blocks


# =========================
# EXTRACCI√ìN DESDE HTML Y PDF
# =========================

def extract_text_from_html(path: str) -> str:
    """
    Extrae texto "importante" de un HTML:
    - Elimina <script>, <style>, <noscript>.
    - Se queda con encabezados, p√°rrafos, listas, etc.
    - Devuelve el texto plano concatenado.
    """
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            html = f.read()
    except Exception as e:
        print(f"[WARN] No se pudo leer HTML {path}: {e}")
        return ""

    soup = BeautifulSoup(html, "html.parser")

    # Eliminar ruido
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # Tambi√©n puedes eliminar nav/footer si quieres:
    for tag in soup(["nav", "footer"]):
        tag.decompose()

    text = soup.get_text(separator=" ", strip=True)
    return text


def extract_text_from_pdf(path: str) -> str:
    """
    Extrae texto de un PDF usando PyPDF2.
    """
    try:
        reader = PdfReader(path)
    except Exception as e:
        print(f"[WARN] No se pudo abrir PDF {path}: {e}")
        return ""

    pages_text = []
    for i, page in enumerate(reader.pages):
        try:
            t = page.extract_text()
        except Exception as e:
            print(f"[WARN] Error extrayendo texto de la p√°gina {i} en {path}: {e}")
            t = None
        if t:
            pages_text.append(t)

    return "\n".join(pages_text)


# =========================
# GESTI√ìN DEL LOG
# =========================

def load_log() -> Dict:
    """
    Carga el log de ficheros procesados.

    Estructura:
    {
        "processed_files": [
            "paginas_html/index.html",
            "paginas_html/wp-content/...",
            ...
        ]
    }
    """
    if not os.path.exists(LOG_FILE):
        return {"processed_files": []}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "processed_files" not in data or not isinstance(data["processed_files"], list):
            return {"processed_files": []}
        return data
    except Exception:
        # Si el log est√° corrupto, empezamos de cero para no bloquear el proceso.
        return {"processed_files": []}


def save_log(log: Dict):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


# =========================
# TRACKER DE PROGRESO
# =========================

class ProgressTracker:
    """
    Barra de progreso global basada en n√∫mero total de bloques.
    Muestra:
    - porcentaje completado
    - tiempo transcurrido
    - ETA estimada
    """

    def __init__(self, total_units: int):
        self.total = max(1, total_units)
        self.current = 0
        self.start_time = time.time()

    @staticmethod
    def _format_seconds(secs: float) -> str:
        secs = int(secs)
        h = secs // 3600
        m = (secs % 3600) // 60
        s = secs % 60
        if h > 0:
            return f"{h:02d}:{m:02d}:{s:02d}"
        return f"{m:02d}:{s:02d}"

    def update(self, step: int = 1, prefix: str = ""):
        self.current += step
        if self.current > self.total:
            self.current = self.total

        elapsed = time.time() - self.start_time
        percent = (self.current / self.total) * 100.0

        if self.current > 0:
            rate = elapsed / self.current
            remaining = rate * (self.total - self.current)
        else:
            remaining = 0.0

        try:
            term_width = shutil.get_terminal_size((80, 20)).columns
        except Exception:
            term_width = 80

        bar_len = max(10, term_width - 55)
        filled = int(bar_len * self.current / self.total)
        bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)

        msg = (
            f"{prefix} [{bar}] {percent:6.2f}% "
            f"| t+{self._format_seconds(elapsed)} "
            f"| ETA {self._format_seconds(remaining)}"
        )

        msg = msg[:term_width - 1]
        print("\r" + msg, end="", flush=True)

    def finish(self, prefix: str = ""):
        self.update(step=0, prefix=prefix)
        print()  # salto de l√≠nea final


# =========================
# DETECCI√ìN DEL MODO OLLAMA
# =========================

def detect_ollama_mode() -> bool:
    """
    Detecta si Ollama expone /api/chat o /api/generate y configura
    las variables globales OLLAMA_MODE y OLLAMA_URL.
    """
    global OLLAMA_MODE, OLLAMA_URL

    # Primero probamos /api/chat
    chat_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"[INFO] Probando endpoint: {chat_url}")
    try:
        payload_chat = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Responde con una sola palabra: OK"},
                {"role": "user", "content": "OK"}
            ],
            "stream": False
        }
        resp = requests.post(chat_url, json=payload_chat, timeout=10)

        if resp.status_code == 404:
            print("[INFO] /api/chat devuelve 404, se probar√° /api/generate.")
        else:
            resp.raise_for_status()
            data = resp.json()
            if "message" in data and "content" in data["message"]:
                print("[INFO] Endpoint /api/chat detectado correctamente.")
                OLLAMA_MODE = "chat"
                OLLAMA_URL = chat_url
                return True
            else:
                print("[WARN] /api/chat respondi√≥ pero no con el formato esperado.")
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[WARN] Error al probar /api/chat: {e}")

    # Si /api/chat no sirve, probamos /api/generate
    gen_url = f"{OLLAMA_BASE_URL}/api/generate"
    print(f"[INFO] Probando endpoint: {gen_url}")
    try:
        payload_gen = {
            "model": MODEL_NAME,
            "prompt": "Responde con una sola palabra: OK",
            "stream": False
        }
        resp = requests.post(gen_url, json=payload_gen, timeout=10)

        if resp.status_code == 404:
            print("[ERROR] /api/generate tambi√©n devuelve 404.")
            print("[ERROR] Ninguno de los endpoints est√°ndar de Ollama est√° disponible.")
            print("[ERROR] Revisa el nombre del modelo o si Ollama est√° arrancado.")
            print("       Modelo actual:", MODEL_NAME)
            return False

        resp.raise_for_status()
        data = resp.json()
        if "response" in data:
            print("[INFO] Endpoint /api/generate detectado correctamente.")
            OLLAMA_MODE = "generate"
            OLLAMA_URL = gen_url
            return True
        else:
            print("[WARN] /api/generate respondi√≥ pero no con el formato esperado.")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] No se puede conectar a Ollama en {OLLAMA_BASE_URL}")
        print("       ¬øEst√° arrancado? Ejecuta:  ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Error al probar /api/generate: {e}")
        return False


# =========================
# LLAMADA A OLLAMA (seg√∫n modo detectado)
# =========================

def call_ollama(system_prompt: str, user_prompt: str) -> str:
    """
    Llama a Ollama usando el endpoint detectado (chat o generate).
    """
    if OLLAMA_MODE is None or OLLAMA_URL is None:
        raise RuntimeError("OLLAMA_MODE no est√° configurado. Llama antes a detect_ollama_mode().")

    if OLLAMA_MODE == "chat":
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    elif OLLAMA_MODE == "generate":
        full_prompt = f"""{system_prompt.strip()}

Usuario:
{user_prompt.strip()}
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            }
        }
    else:
        raise RuntimeError(f"Modo de Ollama desconocido: {OLLAMA_MODE}")

    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()

    if OLLAMA_MODE == "chat":
        return data["message"]["content"]
    else:  # generate
        return data.get("response", "")


# =========================
# PROMPTS PARA Q/A
# =========================

SYSTEM_PROMPT_QA = """
Eres un generador de preguntas y respuestas de alta calidad en espa√±ol
para entrenar un modelo de lenguaje educativo.

Tu tarea:
- Leer con mucha atenci√≥n un bloque de texto t√©cnico o formativo en espa√±ol
  (puede venir originalmente de HTML o PDF).
- Identificar TODOS los conceptos importantes posibles (t√©rminos, pasos, advertencias,
  decisiones de dise√±o, buenas pr√°cticas, errores habituales, matices, etc.).
- A partir de esos conceptos, generar muchas preguntas y respuestas √∫tiles para entrenamiento.

Reglas generales:
- No inventes conceptos que no aparezcan o no se deduzcan claramente del texto.
- Las respuestas deben ser completas pero concisas, sin relleno.
- Siempre responde en espa√±ol neutro.
- Usa el formato JSON Lines: cada l√≠nea un objeto JSON con campos:
  {"question": "...", "answer": "..."}
- No a√±adas texto fuera de ese formato (ni comentarios, ni encabezados).
- No pongas comas finales despu√©s del objeto JSON en cada l√≠nea.
""".strip()


def build_user_prompt_easy(block: str) -> str:
    return f"""
Genera PREGUNTAS F√ÅCILES (nivel introductorio) con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * definici√≥n (¬øqu√© es...?),
  * prop√≥sito (¬øpara qu√© sirve...?),
  * pasos b√°sicos (¬øcu√°l es el primer paso para...?),
  * identificaci√≥n (¬øqu√© nombre recibe...?),
  * ventajas / desventajas claras.
- Cubre todos los conceptos b√°sicos que veas.
- Intenta generar muchas preguntas (por ejemplo, 10-20 por bloque si el texto lo permite,
  o m√°s si hay muchos conceptos).
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


def build_user_prompt_advanced(block: str) -> str:
    return f"""
Genera PREGUNTAS INTERMEDIAS y AVANZADAS con sus respuestas a partir del siguiente texto.

Requisitos:
- Preguntas de tipo:
  * razonamiento (¬øpor qu√© es recomendable...?, ¬øqu√© ocurre si no se hace...?),
  * comparaci√≥n (¬øqu√© diferencia hay entre... y ...?),
  * casos pr√°cticos (¬øqu√© har√≠as si...?, ¬øen qu√© situaci√≥n conviene...?),
  * consecuencias (¬øqu√© puede pasar si...?, ¬øqu√© problema se evita al...?),
  * buenas pr√°cticas y advertencias.
- Exprime al m√°ximo el contenido: si hay muchos matices, genera muchas preguntas.
- Puedes reutilizar un mismo concepto con enfoques distintos.
- Formato OBLIGATORIO: JSON Lines, cada l√≠nea:
  {{"question": "texto de la pregunta", "answer": "texto de la respuesta"}}

Texto:
\"\"\"{block}\"\"\"
""".strip()


# =========================
# PARSEO SEGURO DEL JSONL
# =========================

def parse_jsonl_from_llm(text: str) -> List[Dict[str, str]]:
    """
    Intenta extraer l√≠neas JSON v√°lidas del texto devuelto por el modelo.
    Ignora l√≠neas vac√≠as o mal formadas.
    """
    pairs = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        if line.startswith("- "):
            line = line[2:].strip()
        if line.startswith("* "):
            line = line[2:].strip()

        if not (line.startswith("{") and line.endswith("}")):
            if "{" in line and "}" in line:
                line = line[line.find("{"):line.rfind("}") + 1]
            else:
                continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue

        q = obj.get("question")
        a = obj.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            pairs.append({"question": q.strip(), "answer": a.strip()})

    return pairs


# =========================
# PREPARACI√ìN DE CADA FICHERO (LECTURA + TROCEO)
# =========================

def prepare_blocks_for_file(path: str) -> List[str]:
    """
    Lee el fichero seg√∫n su extensi√≥n, extrae texto √∫til y lo trocea en bloques.
    """
    print(f"\n[INFO] Preparando archivo: {path}")
    ext = os.path.splitext(path)[1].lower()

    raw_text = ""

    if ext in (".txt", ".md"):
        raw_text = read_text_file(path)
        if ext == ".md":
            raw_text = strip_markdown(raw_text)
    elif ext in (".html", ".htm"):
        raw_text = extract_text_from_html(path)
    elif ext == ".pdf":
        raw_text = extract_text_from_pdf(path)
    else:
        print(f"[WARN] Extensi√≥n no soportada (deber√≠a haberse filtrado antes): {ext}")
        return []

    if not raw_text or not raw_text.strip():
        print("[WARN] Archivo vac√≠o o sin texto √∫til, se ignorar√° (0 bloques).")
        return []

    text = normalize_whitespace(raw_text)

    if not text.strip():
        print("[WARN] Tras normalizar espacios el archivo qued√≥ vac√≠o, se ignorar√° (0 bloques).")
        return []

    blocks = split_into_blocks(text)
    print(f"[INFO]   -> {len(blocks)} bloques detectados.")
    return blocks


# =========================
# L√ìGICA PRINCIPAL POR BLOQUE
# =========================

def generate_qa_for_block(block: str) -> List[Dict[str, str]]:
    all_pairs: List[Dict[str, str]] = []

    # Preguntas f√°ciles
    try:
        easy_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_easy(block)
        )
        easy_pairs = parse_jsonl_from_llm(easy_text)
        all_pairs.extend(easy_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A f√°ciles: {e}")

    # Preguntas intermedias / avanzadas
    try:
        adv_text = call_ollama(
            SYSTEM_PROMPT_QA,
            build_user_prompt_advanced(block)
        )
        adv_pairs = parse_jsonl_from_llm(adv_text)
        all_pairs.extend(adv_pairs)
    except Exception as e:
        print(f"\n[WARN] Error generando Q/A avanzadas: {e}")

    return all_pairs


# =========================
# PROCESAMIENTO DE UN FICHERO (GUARDADO SOBRE LA MARCHA)
# =========================

def process_single_file(path: str,
                        output_path: str,
                        blocks: List[str],
                        tracker: Optional[ProgressTracker],
                        file_index: int,
                        total_files: int) -> int:
    print(f"\n[INFO] Procesando archivo {file_index}/{total_files}: {path}")

    if not blocks:
        with open(output_path, "w", encoding="utf-8"):
            pass
        print("[INFO] Archivo sin bloques, JSONL vac√≠o generado.")
        return 0

    with open(output_path, "w", encoding="utf-8"):
        pass

    total_pairs_for_file = 0

    for block in blocks:
        block_pairs = generate_qa_for_block(block)

        if block_pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in block_pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

        total_pairs_for_file += len(block_pairs)

        if tracker is not None:
            tracker.update(
                step=1,
                prefix=f"[PROGRESO] Archivo {file_index}/{total_files}"
            )

    print(f"\n[INFO] Total pares Q/A para {os.path.basename(path)}: {total_pairs_for_file}")
    print(f"[INFO] JSONL generado para {path}: {output_path}")
    return total_pairs_for_file


# =========================
# MAIN
# =========================

def main():
    ensure_dirs()

    print("[INFO] Comprobando servicio de Ollama y detectando endpoint...")
    if not detect_ollama_mode():
        print("[FATAL] No se ha podido detectar un endpoint v√°lido de Ollama. Abortando.")
        return

    # Recorre INPUT_DIR de forma recursiva
    input_files = []
    for root, _, files in os.walk(INPUT_DIR):
        for fn in files:
            if fn.lower().endswith(VALID_EXTS):
                full_path = os.path.join(root, fn)
                input_files.append(full_path)

    if not input_files:
        print(f"[INFO] No se han encontrado ficheros v√°lidos en {INPUT_DIR}.")
        print("      Extensiones admitidas:", ", ".join(VALID_EXTS))
        return

    log = load_log()
    processed_files = set(log.get("processed_files", []))

    pending_files = [p for p in sorted(input_files) if p not in processed_files]

    if not pending_files:
        print("[INFO] Todos los archivos presentes ya estaban procesados seg√∫n log.json.")
        print(f"[INFO] Log de materiales procesados: {LOG_FILE}")
        return

    # Primera pasada: preparar bloques por fichero y contar bloques totales
    print("\n[INFO] Calculando n√∫mero total de bloques para la barra de progreso global...")
    file_blocks_map: Dict[str, List[str]] = {}
    total_blocks = 0

    for path in pending_files:
        blocks = prepare_blocks_for_file(path)
        file_blocks_map[path] = blocks
        total_blocks += len(blocks)

    if total_blocks == 0:
        print("[WARN] No se han encontrado bloques de texto √∫tiles en los ficheros pendientes.")
        print("       Se actualizar√° el log, pero no se generar√°n Q/A.")
        newly_processed_count = 0
        for path in pending_files:
            base_name = os.path.splitext(os.path.basename(path))[0]
            per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")
            with open(per_file_output, "w", encoding="utf-8"):
                pass
            log.setdefault("processed_files", []).append(path)
            save_log(log)
            newly_processed_count += 1

        print("\n[RESUMEN]")
        print(f"Archivos encontrados              : {len(input_files)}")
        print(f"Archivos ya procesados (skip)     : {len(input_files) - len(pending_files)}")
        print(f"Archivos procesados en esta run   : {newly_processed_count}")
        print(f"Pares Q/A generados en esta run   : 0")
        print(f"Log de materiales procesados      : {LOG_FILE}")
        print(f"JSONL individuales en             : {OUTPUT_DIR}")
        return

    tracker = ProgressTracker(total_blocks)

    total_pairs = 0
    newly_processed_count = 0
    skipped_count = len(input_files) - len(pending_files)

    print(f"[INFO] Total de bloques a procesar: {total_blocks}")
    print("[INFO] Iniciando generaci√≥n de Q/A con barra de progreso global...\n")

    for idx, path in enumerate(pending_files, start=1):
        base_name = os.path.splitext(os.path.basename(path))[0]
        per_file_output = os.path.join(OUTPUT_DIR, f"{base_name}.jsonl")

        blocks = file_blocks_map.get(path, [])
        pairs_count = process_single_file(
            path=path,
            output_path=per_file_output,
            blocks=blocks,
            tracker=tracker,
            file_index=idx,
            total_files=len(pending_files)
        )
        total_pairs += pairs_count

        log.setdefault("processed_files", []).append(path)
        save_log(log)
        newly_processed_count += 1

    tracker.finish(prefix="[PROGRESO]")

    print("\n[RESUMEN]")
    print(f"Archivos encontrados              : {len(input_files)}")
    print(f"Archivos ya procesados (skip)     : {skipped_count}")
    print(f"Archivos procesados en esta run   : {newly_processed_count}")
    print(f"Pares Q/A generados en esta run   : {total_pairs}")
    print(f"Log de materiales procesados      : {LOG_FILE}")
    print(f"JSONL individuales en             : {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### Actividades propuestas

El script que proporcionaste es una herramienta bastante completa para generar pares de preguntas y respuestas (Q/A) a partir de diversos tipos de archivos de entrada. A continuaci√≥n, se detallan las caracter√≠sticas principales del script:

1. **Configuraci√≥n**:
   - Define directorios y configuraciones necesarios (`INPUT_DIR`, `OUTPUT_DIR`, etc.)
   - Utiliza un archivo de registro para llevar un seguimiento de los archivos procesados.

2. **Preparaci√≥n del entorno**:
   - Comprueba la disponibilidad y el endpoint correcto del servicio Ollama.
   - Recorre recursivamente los directorios de entrada buscando archivos v√°lidos bas√°ndose en una lista de extensiones permitidas.

3. **Lectura y procesamiento de archivos**:
   - Prepara bloques de texto √∫tiles de diferentes tipos de archivos (txt, markdown, html, pdf).
   - Genera pares Q/A para cada bloque utilizando un modelo entrenado a trav√©s del servicio Ollama.
   
4. **Progreso y seguimiento**:
   - Utiliza una clase `ProgressTracker` para mostrar el progreso global durante la generaci√≥n de Q/A.

5. **Funcionalidades adicionales**:
   - Normalizaci√≥n del texto, extracci√≥n de contenido √∫til de archivos HTML y PDF.
   - Manejo de errores y logs.

Aqu√≠ hay algunas mejoras o consideraciones que podr√≠as tener en cuenta:

- **Optimizaci√≥n de rendimiento**: Considerar el uso de hilos para procesar m√∫ltiples archivos simult√°neamente. Esto puede ser especialmente beneficioso si se tienen muchos archivos grandes por procesar.
  
- **Validaci√≥n adicional del texto**: Asegurarse de que los bloques no contengan contenido inapropiado o sensible antes de generar Q/A.

- **Personalizaci√≥n del modelo**: Permitir la configuraci√≥n del modelo utilizado para generar Q/A, ya sea cambiando el prompt del sistema o ajustando par√°metros espec√≠ficos del modelo.

- **Soporte adicional de formatos**: A√±adir soporte para m√°s tipos de archivos como .docx, .pptx si es necesario.

Aqu√≠ hay un ejemplo simplificado de c√≥mo podr√≠as implementar la opci√≥n de utilizar hilos:

```python
import concurrent.futures

def process_file_in_thread(path: str):
    blocks = prepare_blocks_for_file(path)
    output_path = os.path.join(OUTPUT_DIR, f"{os.path.splitext(os.path.basename(path))[0]}.jsonl")
    
    with open(output_path, "w", encoding="utf-8"):
        pass
    
    for block in blocks:
        pairs = generate_qa_for_block(block)
        
        if pairs:
            with open(output_path, "a", encoding="utf-8") as f:
                for p in pairs:
                    f.write(json.dumps(p, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    input_files = [path for path in os.listdir(INPUT_DIR) if any(path.endswith(ext) for ext in VALID_EXTS)]
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(process_file_in_thread, file_path): file_path for file_path in input_files}
        
        for future in concurrent.futures.as_completed(futures):
            print(f"Procesado: {futures[future]}")
```

Con estas modificaciones, el script puede ser m√°s eficiente y flexible para manejar una amplia variedad de archivos e implementar mejoras adicionales seg√∫n sea necesario.


<a id="interfaz-natural-ia"></a>
## interfaz natural IA



<a id="actividad-libre-de-final-de-evaluacion-la-milla-extra"></a>
# Actividad libre de final de evaluaci√≥n - La milla extra

<a id="la-milla-extra-primera-evaluacion"></a>
## La Milla Extra - Primera evaluaci√≥n

### Introducci√≥n a los ejercicios

El archivo "ejercicio.md" en esta carpeta contiene una descripci√≥n del proyecto y las tareas que debes completar como parte de la evaluaci√≥n final en el m√≥dulo de Proyecto Intermodular II. Este conjunto de ejercicios est√° dise√±ado para que practiques tus habilidades de programaci√≥n aplic√°ndolas a un proyecto real, enfoc√°ndose principalmente en la estructuraci√≥n y documentaci√≥n del c√≥digo. A trav√©s de este ejercicio, mejorar√°s tu capacidad para resolver problemas complejos y trabajar con buenas pr√°cticas de desarrollo, como el control de versiones y la colaboraci√≥n en equipo mediante herramientas como GitHub.

### Actividades propuestas

Bas√°ndome en la ubicaci√≥n del archivo y su contexto, parece que se trata de una carpeta relacionada con un proyecto interdisciplinario para alumnos de Formaci√≥n Profesional, espec√≠ficamente del DAM (Desarrollo de Aplicaciones Multiplataforma). Aunque no he recibido el contenido literal del archivo `ejercicio.md`, puedo inferir algunas actividades basadas en los t√≠tulos y rutas proporcionados.

1. **An√°lisis de Requisitos**
   - **Descripci√≥n:** Los alumnos deben revisar la descripci√≥n del proyecto (que se encuentre dentro del archivo `ejercicio.md`) y escribir un informe donde identifiquen las caracter√≠sticas necesarias para el desarrollo del programa propuesto en el ejercicio.
   
2. **Planificaci√≥n de Casos de Uso**
   - **Descripci√≥n:** Los estudiantes deber√°n crear una lista detallada de los casos de uso que se requieren para satisfacer todas las funcionalidades del proyecto, bas√°ndose en la descripci√≥n proporcionada.

3. **Dise√±o de Interfaz Usuario (UI) B√°sica**
   - **Descripci√≥n:** Crear un dise√±o b√°sico (puede ser a mano o con herramientas digitales como Balsamiq) de c√≥mo se ver√≠a la interfaz del usuario para cada una de las funcionalidades identificadas.

4. **Codificaci√≥n Inicial del Proyecto**
   - **Descripci√≥n:** Los estudiantes deben comenzar a escribir el c√≥digo inicial para configurar su entorno de desarrollo y establecer los elementos b√°sicos (estructura del proyecto, paquetes necesarios) seg√∫n la informaci√≥n proporcionada en `ejercicio.md`.

5. **Implementaci√≥n del Men√∫ Principal**
   - **Descripci√≥n:** Bas√°ndose en el dise√±o UI realizado previamente, implementar funcionalidades b√°sicas para mostrar un men√∫ principal que permita a los usuarios navegar entre diferentes partes de la aplicaci√≥n.

6. **Funciones L√≥gicas B√°sicas**
   - **Descripci√≥n:** Implementar las funciones l√≥gicas necesarias seg√∫n lo definido en los casos de uso, asegur√°ndose de que el c√≥digo est√© bien estructurado y siga las mejores pr√°cticas de programaci√≥n.

7. **Pruebas Unitarias Iniciales**
   - **Descripci√≥n:** Desarrollar pruebas unitarias para verificar la correcta implementaci√≥n de cada una de las funciones l√≥gicas creadas hasta ahora.

8. **Refactorizaci√≥n y Mejora del C√≥digo**
   - **Descripci√≥n:** Una vez que el c√≥digo base funcione correctamente, los alumnos deben refactorizarlo para mejorar su calidad (como reducir duplicidad o mejorar la legibilidad) sin cambiar su comportamiento observable.

9. **Documentaci√≥n Detallada**
   - **Descripci√≥n:** Crear documentaci√≥n detallada sobre las decisiones tomadas durante el desarrollo del proyecto, incluyendo diagramas de flujo y explicaciones claras sobre cada funci√≥n implementada.

10. **Presentaci√≥n Oral del Proyecto**
    - **Descripci√≥n:** Los alumnos deben preparar una presentaci√≥n oral que explique de manera clara y concisa la estructura del proyecto, las decisiones tomadas durante el desarrollo y c√≥mo se lograron los objetivos propuestos en `ejercicio.md`.

Estas actividades est√°n dise√±adas para proporcionar a los estudiantes de Formaci√≥n Profesional una experiencia pr√°ctica completa desde el an√°lisis hasta la presentaci√≥n final, asegurando que dominen todas las etapas del ciclo de vida de un proyecto de desarrollo.
