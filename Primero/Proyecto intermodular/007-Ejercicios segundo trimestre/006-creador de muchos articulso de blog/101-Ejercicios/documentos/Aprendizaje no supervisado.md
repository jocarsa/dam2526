---
slug: aprendizaje-no-supervisado
title: Aprendizaje no supervisado (comoprogramar.es)
description: Aprende aprendizaje no supervisado para descubrir patrones ocultos en los datos sin usar etiquetas. Domina clustering, reducción de dimensionalidad y detección de estructuras para análisis exploratorio y ciencia de datos.
keywords:
  - aprendizaje no supervisado
  - clustering
  - reducción de dimensionalidad
  - patrones en datos
  - machine learning no supervisado
  - análisis exploratorio avanzado
level: Intermedio
duration_estimate: "30-45 horas"
prerequisites:
  - "Machine Learning desde cero"
  - "Manipulación de datos con NumPy y Pandas"
  - "Álgebra lineal aplicada a IA (recomendado)"
audience:
  - "Estudiantes de ciencia de datos"
  - "Científicos de datos en formación"
  - "Analistas de datos"
  - "Ingenieros de IA"
updated: 2025-12-26
---

# Aprendizaje no supervisado

## Objetivos del curso

## Cómo usar este curso

## Qué significa aprender sin etiquetas

---

# Unidad 1 — Qué es el aprendizaje no supervisado

## 1.1 — Aprender sin respuestas correctas

### Lección 1.1.1 — Diferencias con aprendizaje supervisado
### Lección 1.1.2 — Qué tipo de conocimiento se obtiene
### Lección 1.1.3 — Riesgos de interpretación

## 1.2 — Cuándo usar aprendizaje no supervisado

### Lección 1.2.1 — Exploración de datos
### Lección 1.2.2 — Descubrimiento de estructura
### Lección 1.2.3 — Cuándo NO usarlo

---

# Unidad 2 — Preparación de datos para aprendizaje no supervisado

## 2.1 — Importancia de la representación

### Lección 2.1.1 — Variables numéricas
### Lección 2.1.2 — Escalado y normalización
### Lección 2.1.3 — Impacto en resultados

## 2.2 — Problemas frecuentes en los datos

### Lección 2.2.1 — Variables irrelevantes
### Lección 2.2.2 — Ruido
### Lección 2.2.3 — Alta dimensionalidad

---

# Unidad 3 — Clustering: agrupar datos

## 3.1 — Qué es agrupar

### Lección 3.1.1 — Intuición del clustering
### Lección 3.1.2 — Similitud y distancia
### Lección 3.1.3 — Subjetividad del resultado

## 3.2 — k-means

### Lección 3.2.1 — Funcionamiento básico
### Lección 3.2.2 — Elección de k
### Lección 3.2.3 — Ventajas y limitaciones

---

# Unidad 4 — Clustering avanzado

## 4.1 — Clustering jerárquico

### Lección 4.1.1 — Enfoque aglomerativo
### Lección 4.1.2 — Dendrogramas
### Lección 4.1.3 — Interpretación

## 4.2 — DBSCAN

### Lección 4.2.1 — Densidad como criterio
### Lección 4.2.2 — Detección de ruido
### Lección 4.2.3 — Ventajas frente a k-means

---

# Unidad 5 — Evaluación de clustering

## 5.1 — Cómo evaluar sin etiquetas

### Lección 5.1.1 — Dificultad inherente
### Lección 5.1.2 — Métricas internas
### Lección 5.1.3 — Interpretación responsable

## 5.2 — Métricas habituales

### Lección 5.2.1 — Silhouette score
### Lección 5.2.2 — Distancia intra/inter cluster
### Lección 5.2.3 — Limitaciones de las métricas

---

# Unidad 6 — Reducción de dimensionalidad

## 6.1 — Por qué reducir dimensiones

### Lección 6.1.1 — Mal de la dimensionalidad
### Lección 6.1.2 — Visualización
### Lección 6.1.3 — Ruido vs información

## 6.2 — PCA

### Lección 6.2.1 — Intuición geométrica
### Lección 6.2.2 — Componentes principales
### Lección 6.2.3 — Interpretación correcta

---

# Unidad 7 — Técnicas no lineales de reducción

## 7.1 — t-SNE

### Lección 7.1.1 — Preservar vecindarios
### Lección 7.1.2 — Visualización
### Lección 7.1.3 — Errores de interpretación

## 7.2 — UMAP

### Lección 7.2.1 — Idea general
### Lección 7.2.2 — Ventajas frente a t-SNE
### Lección 7.2.3 — Uso práctico

---

# Unidad 8 — Detección de anomalías

## 8.1 — Qué es una anomalía

### Lección 8.1.1 — Definición contextual
### Lección 8.1.2 — Rareza vs error
### Lección 8.1.3 — Casos reales

## 8.2 — Técnicas de detección

### Lección 8.2.1 — Distancia
### Lección 8.2.2 — Densidad
### Lección 8.2.3 — Isolation Forest (conceptual)

---

# Unidad 9 — Interpretación y riesgos

## 9.1 — El peligro de “ver patrones”

### Lección 9.1.1 — Pareidolia en datos
### Lección 9.1.2 — Confirmación de sesgos
### Lección 9.1.3 — Validación externa

## 9.2 — Uso responsable del aprendizaje no supervisado

### Lección 9.2.1 — No automatizar decisiones críticas
### Lección 9.2.2 — Apoyo a análisis humano
### Lección 9.2.3 — Transparencia

---

# Unidad 10 — Aprendizaje no supervisado en proyectos reales

## 10.1 — Casos de uso habituales

### Lección 10.1.1 — Segmentación de clientes
### Lección 10.1.2 — Análisis exploratorio
### Lección 10.1.3 — Preprocesado para ML supervisado

## 10.2 — Integración con otros enfoques

### Lección 10.2.1 — No supervisado + supervisado
### Lección 10.2.2 — Feature engineering
### Lección 10.2.3 — Flujo híbrido

---

# Unidad 11 — Pensar correctamente sin etiquetas

## 11.1 — Qué conclusiones se pueden extraer

### Lección 11.1.1 — Hipótesis, no verdades
### Lección 11.1.2 — Exploración guiada
### Lección 11.1.3 — Criterio profesional

## 11.2 — Errores comunes

### Lección 11.2.1 — Sobreinterpretación
### Lección 11.2.2 — Clusters “forzados”
### Lección 11.2.3 — Falta de validación

---

# Unidad 12 — Mini-proyecto no supervisado

## 12.1 — Proyecto guiado

### Lección 12.1.1 — Exploración de un dataset real
### Lección 12.1.2 — Aplicación de clustering
### Lección 12.1.3 — Reducción de dimensionalidad
### Lección 12.1.4 — Interpretación de resultados
### Lección 12.1.5 — Conclusiones razonadas

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Aprendizaje supervisado avanzado
### Lección 13.1.2 — Deep Learning
### Lección 13.1.3 — Análisis exploratorio avanzado

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Aprendizaje supervisado
### Lección 13.2.2 — Machine Learning clásico
### Lección 13.2.3 — Deep Learning desde cero

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

