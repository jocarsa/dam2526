---
slug: machine-learning-clasico
title: Machine Learning clásico (comoprogramar.es)
description: Aprende los algoritmos fundamentales del machine learning clásico: regresión, clasificación, clustering y reducción de dimensión. Comprende cómo funcionan, cuándo usarlos y cuáles son sus límites en proyectos reales de IA.
keywords:
  - machine learning clásico
  - algoritmos de machine learning
  - regresión
  - clasificación
  - clustering
  - modelos predictivos
  - aprendizaje automático
level: Intermedio
duration_estimate: "40-60 horas"
prerequisites:
  - "Probabilidad y estadística para IA"
  - "Álgebra lineal aplicada a IA"
  - "Manipulación de datos con NumPy y Pandas"
audience:
  - "Estudiantes de ciencia de datos"
  - "Científicos de datos en formación"
  - "Desarrolladores que quieren entender ML"
  - "Ingenieros de IA"
updated: 2025-12-26
---

# Machine Learning clásico

## Objetivos del curso

## Cómo usar este curso

## Qué entendemos por “machine learning clásico”

---

# Unidad 1 — Qué es realmente el Machine Learning

## 1.1 — Aprender a partir de datos

### Lección 1.1.1 — Programar reglas vs aprender patrones
### Lección 1.1.2 — Qué problemas resuelve el ML
### Lección 1.1.3 — Qué problemas NO resuelve

## 1.2 — Tipos de aprendizaje automático

### Lección 1.2.1 — Aprendizaje supervisado
### Lección 1.2.2 — Aprendizaje no supervisado
### Lección 1.2.3 — Aprendizaje semisupervisado

## 1.3 — ML dentro del flujo de un proyecto de IA

### Lección 1.3.1 — Relación con los datos
### Lección 1.3.2 — Relación con negocio
### Lección 1.3.3 — Métricas y evaluación

---

# Unidad 2 — Regresión: predecir valores numéricos

## 2.1 — Regresión lineal

### Lección 2.1.1 — Intuición geométrica
### Lección 2.1.2 — Ajuste de una recta
### Lección 2.1.3 — Interpretación de coeficientes

## 2.2 — Regresión múltiple

### Lección 2.2.1 — Múltiples variables explicativas
### Lección 2.2.2 — Colinealidad
### Lección 2.2.3 — Limitaciones del modelo lineal

## 2.3 — Regularización

### Lección 2.3.1 — Overfitting en regresión
### Lección 2.3.2 — Ridge
### Lección 2.3.3 — Lasso y Elastic Net

---

# Unidad 3 — Clasificación: tomar decisiones

## 3.1 — Clasificación binaria

### Lección 3.1.1 — Problemas de decisión
### Lección 3.1.2 — Fronteras de decisión
### Lección 3.1.3 — Interpretación de probabilidades

## 3.2 — Regresión logística

### Lección 3.2.1 — Función sigmoide
### Lección 3.2.2 — Log-odds
### Lección 3.2.3 — Umbrales de decisión

## 3.3 — k-Nearest Neighbors (k-NN)

### Lección 3.3.1 — Distancia como criterio
### Lección 3.3.2 — Elección de k
### Lección 3.3.3 — Coste computacional

---

# Unidad 4 — Árboles de decisión y modelos basados en árboles

## 4.1 — Árboles de decisión

### Lección 4.1.1 — División del espacio
### Lección 4.1.2 — Criterios de impureza
### Lección 4.1.3 — Interpretabilidad

## 4.2 — Random Forest

### Lección 4.2.1 — Ensambles
### Lección 4.2.2 — Reducción de varianza
### Lección 4.2.3 — Ventajas prácticas

## 4.3 — Gradient Boosting

### Lección 4.3.1 — Aprendizaje secuencial
### Lección 4.3.2 — Corrección de errores
### Lección 4.3.3 — Potencia y riesgos

---

# Unidad 5 — Support Vector Machines (SVM)

## 5.1 — Intuición geométrica

### Lección 5.1.1 — Márgenes máximos
### Lección 5.1.2 — Vectores soporte
### Lección 5.1.3 — Separabilidad

## 5.2 — Kernels

### Lección 5.2.1 — Transformaciones implícitas
### Lección 5.2.2 — Kernels comunes
### Lección 5.2.3 — Coste computacional

---

# Unidad 6 — Aprendizaje no supervisado

## 6.1 — Clustering

### Lección 6.1.1 — Qué es agrupar
### Lección 6.1.2 — k-means
### Lección 6.1.3 — Elección del número de clusters

## 6.2 — Clustering avanzado

### Lección 6.2.1 — Clustering jerárquico
### Lección 6.2.2 — DBSCAN
### Lección 6.2.3 — Detección de ruido

---

# Unidad 7 — Reducción de dimensionalidad

## 7.1 — Por qué reducir dimensiones

### Lección 7.1.1 — Mal de la dimensionalidad
### Lección 7.1.2 — Visualización
### Lección 7.1.3 — Rendimiento del modelo

## 7.2 — PCA

### Lección 7.2.1 — Intuición geométrica
### Lección 7.2.2 — Componentes principales
### Lección 7.2.3 — Interpretación de resultados

---

# Unidad 8 — Evaluación de modelos

## 8.1 — Métricas para regresión

### Lección 8.1.1 — MAE
### Lección 8.1.2 — MSE y RMSE
### Lección 8.1.3 — R²

## 8.2 — Métricas para clasificación

### Lección 8.2.1 — Accuracy
### Lección 8.2.2 — Precision, recall y F1
### Lección 8.2.3 — ROC y AUC

---

# Unidad 9 — Selección y comparación de modelos

## 9.1 — Bias vs variance

### Lección 9.1.1 — Compromiso fundamental
### Lección 9.1.2 — Señales prácticas
### Lección 9.1.3 — Ajuste del modelo

## 9.2 — Validación cruzada

### Lección 9.2.1 — Cross-validation
### Lección 9.2.2 — Evitar data leakage
### Lección 9.2.3 — Selección final

---

# Unidad 10 — Feature engineering en ML clásico

## 10.1 — Importancia de las variables

### Lección 10.1.1 — Variables relevantes
### Lección 10.1.2 — Variables irrelevantes
### Lección 10.1.3 — Impacto en modelos clásicos

## 10.2 — Escalado y normalización

### Lección 10.2.1 — Por qué escalar
### Lección 10.2.2 — Estandarización
### Lección 10.2.3 — Casos reales

---

# Unidad 11 — Límites del Machine Learning clásico

## 11.1 — Cuándo funciona bien

### Lección 11.1.1 — Datos estructurados
### Lección 11.1.2 — Volúmenes moderados
### Lección 11.1.3 — Alta interpretabilidad

## 11.2 — Cuándo no es suficiente

### Lección 11.2.1 — Datos no estructurados
### Lección 11.2.2 — Representaciones complejas
### Lección 11.2.3 — Necesidad de Deep Learning

---

# Unidad 12 — Mini-proyecto de ML clásico

## 12.1 — Proyecto guiado completo

### Lección 12.1.1 — Definición del problema
### Lección 12.1.2 — Preparación de datos
### Lección 12.1.3 — Entrenamiento de varios modelos
### Lección 12.1.4 — Comparación y selección
### Lección 12.1.5 — Conclusiones razonadas

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Deep Learning
### Lección 13.1.2 — IA generativa
### Lección 13.1.3 — MLOps

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Deep Learning desde cero
### Lección 13.2.2 — IA generativa
### Lección 13.2.3 — Proyectos de IA end-to-end

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

