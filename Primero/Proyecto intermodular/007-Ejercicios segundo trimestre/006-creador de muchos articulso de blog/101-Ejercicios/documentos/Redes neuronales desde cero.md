---
slug: redes-neuronales-desde-cero
title: Redes neuronales desde cero (comoprogramar.es)
description: Aprende redes neuronales desde cero, paso a paso y sin cajas negras. Comprende cómo funcionan las neuronas artificiales, el entrenamiento mediante backpropagation y los fundamentos que sustentan el deep learning moderno.
keywords:
  - redes neuronales desde cero
  - neuronas artificiales
  - backpropagation
  - deep learning fundamentos
  - redes neuronales explicadas
  - machine learning avanzado
level: Intermedio
duration_estimate: "40-60 horas"
prerequisites:
  - "Machine Learning desde cero"
  - "Álgebra lineal aplicada a IA (recomendado)"
  - "Probabilidad y estadística para IA (recomendado)"
audience:
  - "Estudiantes de inteligencia artificial"
  - "Científicos de datos en formación"
  - "Desarrolladores que quieren entender deep learning"
updated: 2025-12-26
---

# Redes neuronales desde cero

## Objetivos del curso

## Cómo usar este curso

## Por qué las redes neuronales funcionan (y cuándo no)

---

# Unidad 1 — De modelos clásicos a redes neuronales

## 1.1 — Limitaciones del Machine Learning clásico

### Lección 1.1.1 — Fronteras lineales
### Lección 1.1.2 — Feature engineering manual
### Lección 1.1.3 — Necesidad de modelos más flexibles

## 1.2 — Qué es una red neuronal

### Lección 1.2.1 — Inspiración biológica (sin mitos)
### Lección 1.2.2 — Red como función matemática
### Lección 1.2.3 — Parámetros entrenables

---

# Unidad 2 — La neurona artificial

## 2.1 — Modelo de neurona

### Lección 2.1.1 — Entradas, pesos y bias
### Lección 2.1.2 — Suma ponderada
### Lección 2.1.3 — Interpretación geométrica

## 2.2 — Funciones de activación

### Lección 2.2.1 — Por qué no basta una función lineal
### Lección 2.2.2 — Sigmoid
### Lección 2.2.3 — ReLU y variantes

---

# Unidad 3 — Redes multicapa (MLP)

## 3.1 — Capas y arquitectura

### Lección 3.1.1 — Capa de entrada
### Lección 3.1.2 — Capas ocultas
### Lección 3.1.3 — Capa de salida

## 3.2 — Capacidad de representación

### Lección 3.2.1 — Aproximación de funciones
### Lección 3.2.2 — Profundidad vs anchura
### Lección 3.2.3 — Coste computacional

---

# Unidad 4 — Propagación hacia adelante (forward pass)

## 4.1 — Cálculo de la salida

### Lección 4.1.1 — Flujo de datos
### Lección 4.1.2 — Operaciones matriciales
### Lección 4.1.3 — Salida del modelo

## 4.2 — Redes como composiciones de funciones

### Lección 4.2.1 — Funciones encadenadas
### Lección 4.2.2 — Interpretación matemática
### Lección 4.2.3 — Consecuencias prácticas

---

# Unidad 5 — Función de pérdida

## 5.1 — Medir el error

### Lección 5.1.1 — Error en regresión
### Lección 5.1.2 — Error en clasificación
### Lección 5.1.3 — Qué se optimiza realmente

## 5.2 — Elección de la función de pérdida

### Lección 5.2.1 — MSE
### Lección 5.2.2 — Cross-entropy
### Lección 5.2.3 — Impacto en el entrenamiento

---

# Unidad 6 — Descenso por gradiente

## 6.1 — Idea fundamental del gradiente

### Lección 6.1.1 — Pendiente del error
### Lección 6.1.2 — Dirección de mejora
### Lección 6.1.3 — Tasa de aprendizaje

## 6.2 — Variantes del descenso por gradiente

### Lección 6.2.1 — Batch
### Lección 6.2.2 — Mini-batch
### Lección 6.2.3 — Stochastic

---

# Unidad 7 — Backpropagation

## 7.1 — Por qué es necesario

### Lección 7.1.1 — Ajustar millones de parámetros
### Lección 7.1.2 — Propagación del error
### Lección 7.1.3 — Regla de la cadena

## 7.2 — Backpropagation paso a paso

### Lección 7.2.1 — Cálculo de gradientes
### Lección 7.2.2 — Actualización de pesos
### Lección 7.2.3 — Errores comunes

---

# Unidad 8 — Entrenamiento de una red neuronal

## 8.1 — Ciclo de entrenamiento

### Lección 8.1.1 — Epochs
### Lección 8.1.2 — Iteraciones
### Lección 8.1.3 — Convergencia

## 8.2 — Problemas habituales

### Lección 8.2.1 — Gradientes que explotan o desaparecen
### Lección 8.2.2 — Aprendizaje inestable
### Lección 8.2.3 — Inicialización de pesos

---

# Unidad 9 — Overfitting en redes neuronales

## 9.1 — Alta capacidad y riesgos

### Lección 9.1.1 — Memorización
### Lección 9.1.2 — Sensibilidad a los datos
### Lección 9.1.3 — Señales de sobreajuste

## 9.2 — Técnicas de regularización

### Lección 9.2.1 — L1 y L2
### Lección 9.2.2 — Dropout
### Lección 9.2.3 — Early stopping

---

# Unidad 10 — Evaluación de redes neuronales

## 10.1 — Métricas según el problema

### Lección 10.1.1 — Regresión
### Lección 10.1.2 — Clasificación
### Lección 10.1.3 — Interpretación del rendimiento

## 10.2 — Validación correcta

### Lección 10.2.1 — Separación de datos
### Lección 10.2.2 — Curvas de aprendizaje
### Lección 10.2.3 — Evitar falsas conclusiones

---

# Unidad 11 — Implementar una red desde cero (sin frameworks)

## 11.1 — Red neuronal básica en Python

### Lección 11.1.1 — Implementar forward pass
### Lección 11.1.2 — Implementar pérdida
### Lección 11.1.3 — Implementar backpropagation

## 11.2 — Límites del enfoque manual

### Lección 11.2.1 — Escalabilidad
### Lección 11.2.2 — Rendimiento
### Lección 11.2.3 — Por qué existen frameworks

---

# Unidad 12 — Mini-proyecto de redes neuronales

## 12.1 — Proyecto guiado

### Lección 12.1.1 — Definición del problema
### Lección 12.1.2 — Implementación de la red
### Lección 12.1.3 — Entrenamiento
### Lección 12.1.4 — Evaluación
### Lección 12.1.5 — Conclusiones razonadas

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Deep Learning desde cero
### Lección 13.1.2 — PyTorch o TensorFlow
### Lección 13.1.3 — Redes convolucionales y recurrentes

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Deep Learning desde cero
### Lección 13.2.2 — Redes neuronales aplicadas
### Lección 13.2.3 — Flujo completo de un proyecto de IA

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

