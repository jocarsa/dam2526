---
slug: clustering-y-reduccion-de-dimensionalidad
title: Clustering y reducción de dimensionalidad (comoprogramar.es)
description: Aprende a descubrir estructura en los datos mediante clustering y a simplificar información compleja con técnicas de reducción de dimensionalidad. Un enfoque práctico para análisis exploratorio, segmentación y visualización en ciencia de datos.
keywords:
  - clustering
  - reducción de dimensionalidad
  - k-means
  - PCA
  - t-SNE
  - UMAP
  - aprendizaje no supervisado
level: Intermedio
duration_estimate: "30-45 horas"
prerequisites:
  - "Machine Learning desde cero"
  - "Aprendizaje no supervisado (recomendado)"
  - "Álgebra lineal aplicada a IA (recomendado)"
audience:
  - "Estudiantes de ciencia de datos"
  - "Analistas de datos"
  - "Científicos de datos en formación"
  - "Ingenieros de IA"
updated: 2025-12-26
---

# Clustering y reducción de dimensionalidad

## Objetivos del curso

## Cómo usar este curso

## Descubrir estructura y simplificar datos

---

# Unidad 1 — Aprender sin etiquetas: visión general

## 1.1 — Clustering y reducción como herramientas exploratorias

### Lección 1.1.1 — Qué preguntas responden
### Lección 1.1.2 — Qué NO pueden responder
### Lección 1.1.3 — Riesgos de interpretación

## 1.2 — Relación entre ambas técnicas

### Lección 1.2.1 — Agrupar vs proyectar
### Lección 1.2.2 — Complementariedad
### Lección 1.2.3 — Flujo típico de uso

---

# Unidad 2 — Preparación de datos

## 2.1 — Representación adecuada de los datos

### Lección 2.1.1 — Variables numéricas
### Lección 2.1.2 — Escalado y normalización
### Lección 2.1.3 — Impacto en resultados

## 2.2 — Problemas frecuentes

### Lección 2.2.1 — Variables irrelevantes
### Lección 2.2.2 — Ruido y outliers
### Lección 2.2.3 — Alta dimensionalidad

---

# Unidad 3 — Fundamentos del clustering

## 3.1 — Qué significa agrupar

### Lección 3.1.1 — Similitud y distancia
### Lección 3.1.2 — Subjetividad del clustering
### Lección 3.1.3 — Interpretación responsable

## 3.2 — Métricas de distancia

### Lección 3.2.1 — Euclídea
### Lección 3.2.2 — Manhattan
### Lección 3.2.3 — Efecto del escalado

---

# Unidad 4 — k-means

## 4.1 — Funcionamiento del algoritmo

### Lección 4.1.1 — Centroides
### Lección 4.1.2 — Iteraciones
### Lección 4.1.3 — Convergencia

## 4.2 — Elección del número de clusters

### Lección 4.2.1 — Método del codo
### Lección 4.2.2 — Silhouette score
### Lección 4.2.3 — Limitaciones prácticas

---

# Unidad 5 — Clustering avanzado

## 5.1 — Clustering jerárquico

### Lección 5.1.1 — Aglomerativo vs divisivo
### Lección 5.1.2 — Dendrogramas
### Lección 5.1.3 — Interpretación visual

## 5.2 — DBSCAN

### Lección 5.2.1 — Densidad como criterio
### Lección 5.2.2 — Identificación de ruido
### Lección 5.2.3 — Ventajas frente a k-means

---

# Unidad 6 — Evaluación del clustering

## 6.1 — Dificultad de evaluar sin etiquetas

### Lección 6.1.1 — No hay “respuesta correcta”
### Lección 6.1.2 — Validación interna
### Lección 6.1.3 — Validación externa (cuando existe)

## 6.2 — Métricas habituales

### Lección 6.2.1 — Silhouette
### Lección 6.2.2 — Distancia intra/inter cluster
### Lección 6.2.3 — Limitaciones de las métricas

---

# Unidad 7 — Reducción de dimensionalidad: fundamentos

## 7.1 — Por qué reducir dimensiones

### Lección 7.1.1 — Mal de la dimensionalidad
### Lección 7.1.2 — Ruido vs información
### Lección 7.1.3 — Visualización

## 7.2 — Proyección y pérdida de información

### Lección 7.2.1 — Compresión
### Lección 7.2.2 — Trade-off información/simplicidad
### Lección 7.2.3 — Interpretación cuidadosa

---

# Unidad 8 — PCA (Análisis de Componentes Principales)

## 8.1 — Intuición geométrica

### Lección 8.1.1 — Varianza máxima
### Lección 8.1.2 — Componentes principales
### Lección 8.1.3 — Ortogonalidad

## 8.2 — Uso práctico de PCA

### Lección 8.2.1 — Elección del número de componentes
### Lección 8.2.2 — Interpretación de cargas
### Lección 8.2.3 — Casos de uso reales

---

# Unidad 9 — Técnicas no lineales de reducción

## 9.1 — t-SNE

### Lección 9.1.1 — Preservar vecindarios
### Lección 9.1.2 — Visualización de alta dimensión
### Lección 9.1.3 — Errores comunes de interpretación

## 9.2 — UMAP

### Lección 9.2.1 — Idea general
### Lección 9.2.2 — Ventajas frente a t-SNE
### Lección 9.2.3 — Uso práctico

---

# Unidad 10 — Clustering + reducción de dimensionalidad

## 10.1 — Flujo combinado

### Lección 10.1.1 — Reducir antes de agrupar
### Lección 10.1.2 — Visualizar clusters
### Lección 10.1.3 — Errores habituales

## 10.2 — Casos reales

### Lección 10.2.1 — Segmentación de clientes
### Lección 10.2.2 — Análisis exploratorio avanzado
### Lección 10.2.3 — Preparación para ML supervisado

---

# Unidad 11 — Interpretación y riesgos

## 11.1 — Peligro de sobreinterpretar

### Lección 11.1.1 — Ver patrones inexistentes
### Lección 11.1.2 — Clusters forzados
### Lección 11.1.3 — Validación con conocimiento del dominio

## 11.2 — Uso responsable

### Lección 11.2.1 — Apoyo a decisiones humanas
### Lección 11.2.2 — No automatizar conclusiones
### Lección 11.2.3 — Transparencia

---

# Unidad 12 — Mini-proyecto integrador

## 12.1 — Proyecto guiado

### Lección 12.1.1 — Exploración del dataset
### Lección 12.1.2 — Aplicación de reducción de dimensionalidad
### Lección 12.1.3 — Clustering
### Lección 12.1.4 — Visualización e interpretación
### Lección 12.1.5 — Conclusiones razonadas

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Aprendizaje supervisado
### Lección 13.1.2 — Machine Learning clásico
### Lección 13.1.3 — Deep Learning

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Aprendizaje no supervisado
### Lección 13.2.2 — Modelos de clasificación
### Lección 13.2.3 — Deep Learning desde cero

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

