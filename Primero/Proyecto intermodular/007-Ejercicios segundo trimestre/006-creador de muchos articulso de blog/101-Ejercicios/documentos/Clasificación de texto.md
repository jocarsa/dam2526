---
slug: clasificacion-de-texto
title: Clasificación de texto (comoprogramar.es)
description: Aprende a clasificar texto con NLP y Machine Learning. Desde enfoques clásicos hasta modelos modernos, construye clasificadores fiables en español para temas, intención, spam y otros casos reales.
keywords:
  - clasificacion de texto
  - text classification
  - nlp en español
  - clasificacion tematica
  - deteccion de spam
  - machine learning nlp
level: Intermedio
duration_estimate: "35-50 horas"
prerequisites:
  - "NLP desde cero"
  - "Procesamiento de texto con Python"
  - "Tokenización, embeddings y vectores"
audience:
  - "Estudiantes de NLP"
  - "Científicos de datos"
  - "Desarrolladores"
  - "Profesionales de producto y soporte"
updated: 2025-12-26
---

# Clasificación de texto

## Objetivos del curso

## Cómo usar este curso

## Decidir a qué pertenece un texto

---

# Unidad 1 — Qué es la clasificación de texto

## 1.1 — Definición del problema

### Lección 1.1.1 — Texto → etiqueta
### Lección 1.1.2 — Clases mutuamente excluyentes
### Lección 1.1.3 — Casos de uso reales

## 1.2 — Tipos de clasificación

### Lección 1.2.1 — Binaria
### Lección 1.2.2 — Multiclase
### Lección 1.2.3 — Multietiqueta

---

# Unidad 2 — Datos y etiquetas

## 2.1 — Diseño de las clases

### Lección 2.1.1 — Definiciones claras
### Lección 2.1.2 — Granularidad adecuada
### Lección 2.1.3 — Errores de diseño frecuentes

## 2.2 — Calidad del etiquetado

### Lección 2.2.1 — Ruido en las etiquetas
### Lección 2.2.2 — Consistencia inter-etiquetador
### Lección 2.2.3 — Impacto en el modelo

---

# Unidad 3 — Preprocesado específico para clasificación

## 3.1 — Limpieza orientada a la tarea

### Lección 3.1.1 — Qué eliminar y qué conservar
### Lección 3.1.2 — Normalización controlada
### Lección 3.1.3 — Riesgos de limpiar en exceso

## 3.2 — Tokenización y n-gramas

### Lección 3.2.1 — Unigramas
### Lección 3.2.2 — Bigramas y trigramas
### Lección 3.2.3 — Compromisos de dimensionalidad

---

# Unidad 4 — Representación del texto

## 4.1 — Enfoques clásicos

### Lección 4.1.1 — Bag of Words
### Lección 4.1.2 — TF-IDF
### Lección 4.1.3 — Ventajas y límites

## 4.2 — Representaciones densas

### Lección 4.2.1 — Embeddings de palabras
### Lección 4.2.2 — Vectores de frases
### Lección 4.2.3 — Generalización semántica

---

# Unidad 5 — Clasificadores clásicos

## 5.1 — Modelos lineales

### Lección 5.1.1 — Regresión logística
### Lección 5.1.2 — SVM
### Lección 5.1.3 — Interpretabilidad

## 5.2 — Modelos probabilísticos

### Lección 5.2.1 — Naive Bayes
### Lección 5.2.2 — Suposiciones del modelo
### Lección 5.2.3 — Casos donde funciona bien

---

# Unidad 6 — Clasificación con Deep Learning

## 6.1 — Redes neuronales para texto

### Lección 6.1.1 — CNN para clasificación
### Lección 6.1.2 — RNN y LSTM
### Lección 6.1.3 — Coste y rendimiento

## 6.2 — Modelos basados en atención

### Lección 6.2.1 — Contexto completo
### Lección 6.2.2 — Sensibilidad al orden
### Lección 6.2.3 — Ventajas prácticas

---

# Unidad 7 — Entrenamiento y ajuste

## 7.1 — División de datos

### Lección 7.1.1 — Train / validation / test
### Lección 7.1.2 — Validación cruzada
### Lección 7.1.3 — Evitar data leakage

## 7.2 — Ajuste de hiperparámetros

### Lección 7.2.1 — Regularización
### Lección 7.2.2 — Umbrales de decisión
### Lección 7.2.3 — Cambios controlados

---

# Unidad 8 — Evaluación correcta

## 8.1 — Métricas clave

### Lección 8.1.1 — Accuracy
### Lección 8.1.2 — Precision y recall
### Lección 8.1.3 — F1-score

## 8.2 — Casos especiales

### Lección 8.2.1 — Clases desbalanceadas
### Lección 8.2.2 — Métricas por clase
### Lección 8.2.3 — Interpretación responsable

---

# Unidad 9 — Clasificación multietiqueta

## 9.1 — Problema multietiqueta

### Lección 9.1.1 — Textos con múltiples categorías
### Lección 9.1.2 — Independencia de etiquetas
### Lección 9.1.3 — Complejidad adicional

## 9.2 — Estrategias comunes

### Lección 9.2.1 — Binary relevance
### Lección 9.2.2 — Classifier chains
### Lección 9.2.3 — Evaluación multietiqueta

---

# Unidad 10 — Dominio, sesgos y drift

## 10.1 — Dependencia del dominio

### Lección 10.1.1 — Cambios de vocabulario
### Lección 10.1.2 — Degradación del rendimiento
### Lección 10.1.3 — Señales de alerta

## 10.2 — Sesgos y riesgos

### Lección 10.2.1 — Etiquetas subjetivas
### Lección 10.2.2 — Amplificación de sesgos
### Lección 10.2.3 — Mitigación básica

---

# Unidad 11 — Clasificación en producción

## 11.1 — Integración en sistemas

### Lección 11.1.1 — APIs de clasificación
### Lección 11.1.2 — Procesamiento por lotes
### Lección 11.1.3 — Latencia y coste

## 11.2 — Mantenimiento del modelo

### Lección 11.2.1 — Reentrenamiento
### Lección 11.2.2 — Monitorización de métricas
### Lección 11.2.3 — Gestión de versiones

---

# Unidad 12 — Mini-proyecto de clasificación de texto

## 12.1 — Proyecto guiado completo

### Lección 12.1.1 — Definición del problema
### Lección 12.1.2 — Preparación del dataset
### Lección 12.1.3 — Entrenamiento del clasificador
### Lección 12.1.4 — Evaluación crítica
### Lección 12.1.5 — Despliegue básico

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Modelos de lenguaje
### Lección 13.1.2 — Transformers para clasificación
### Lección 13.1.3 — Sistemas RAG y routing semántico

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Análisis de sentimiento
### Lección 13.2.2 — Modelos de lenguaje
### Lección 13.2.3 — NLP con Deep Learning

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

