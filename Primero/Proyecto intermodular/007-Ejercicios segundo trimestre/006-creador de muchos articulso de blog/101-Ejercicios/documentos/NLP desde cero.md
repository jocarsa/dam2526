---
slug: nlp-desde-cero
title: NLP desde cero (comoprogramar.es)
description: Aprende Procesamiento del Lenguaje Natural (NLP) desde cero. Comprende cómo las máquinas representan, procesan y analizan texto, desde tokens y conteos hasta modelos estadísticos y bases del NLP moderno.
keywords:
  - nlp desde cero
  - procesamiento del lenguaje natural
  - analisis de texto
  - linguistica computacional
  - nlp python
  - fundamentos de nlp
level: Principiante–Intermedio
duration_estimate: "40-60 horas"
prerequisites:
  - "Python desde cero"
  - "Probabilidad y estadística básica (recomendado)"
audience:
  - "Principiantes en IA"
  - "Estudiantes de informática"
  - "Desarrolladores"
  - "Autodidactas"
updated: 2025-12-26
---

# NLP desde cero

## Objetivos del curso

## Cómo usar este curso

## Enseñar a una máquina a trabajar con texto

---

# Unidad 1 — Qué es el NLP

## 1.1 — Lenguaje humano y computación

### Lección 1.1.1 — Qué es el lenguaje natural
### Lección 1.1.2 — Por qué el lenguaje es difícil para las máquinas
### Lección 1.1.3 — Aplicaciones reales del NLP

## 1.2 — NLP clásico vs NLP moderno

### Lección 1.2.1 — Enfoques basados en reglas
### Lección 1.2.2 — Enfoques estadísticos
### Lección 1.2.3 — Enfoques basados en Deep Learning

---

# Unidad 2 — Texto como dato

## 2.1 — Características del texto

### Lección 2.1.1 — Texto no estructurado
### Lección 2.1.2 — Ambigüedad y contexto
### Lección 2.1.3 — Variabilidad lingüística

## 2.2 — Corpus y datasets

### Lección 2.2.1 — Qué es un corpus
### Lección 2.2.2 — Datos etiquetados y no etiquetados
### Lección 2.2.3 — Calidad de los datos

---

# Unidad 3 — Preprocesado de texto

## 3.1 — Limpieza básica

### Lección 3.1.1 — Normalización
### Lección 3.1.2 — Eliminación de ruido
### Lección 3.1.3 — Tokenización básica

## 3.2 — Stopwords y stemming

### Lección 3.2.1 — Palabras vacías
### Lección 3.2.2 — Stemming
### Lección 3.2.3 — Lemmatización (intuición)

---

# Unidad 4 — Tokenización en profundidad

## 4.1 — Qué es un token

### Lección 4.1.1 — Palabras
### Lección 4.1.2 — Subpalabras
### Lección 4.1.3 — Caracteres

## 4.2 — Problemas de tokenización

### Lección 4.2.1 — Idiomas flexivos
### Lección 4.2.2 — Palabras desconocidas
### Lección 4.2.3 — Compromisos prácticos

---

# Unidad 5 — Representación numérica del texto

## 5.1 — Modelos de bolsa de palabras

### Lección 5.1.1 — Bag of Words
### Lección 5.1.2 — Frecuencias
### Lección 5.1.3 — Limitaciones

## 5.2 — TF-IDF

### Lección 5.2.1 — Intuición estadística
### Lección 5.2.2 — Importancia relativa
### Lección 5.2.3 — Casos de uso

---

# Unidad 6 — Semántica básica y similitud

## 6.1 — Similitud entre textos

### Lección 6.1.1 — Cosine similarity
### Lección 6.1.2 — Distancias
### Lección 6.1.3 — Interpretación práctica

## 6.2 — Limitaciones del enfoque clásico

### Lección 6.2.1 — Sin contexto
### Lección 6.2.2 — Polisemia
### Lección 6.2.3 — Orden de palabras ignorado

---

# Unidad 7 — Modelos estadísticos de lenguaje

## 7.1 — N-grams

### Lección 7.1.1 — Bigramas y trigramas
### Lección 7.1.2 — Probabilidades condicionales
### Lección 7.1.3 — Suavizado

## 7.2 — Uso práctico de modelos n-gram

### Lección 7.2.1 — Predicción de texto
### Lección 7.2.2 — Detección de idioma
### Lección 7.2.3 — Generación simple

---

# Unidad 8 — Introducción al NLP con Machine Learning

## 8.1 — Texto como features

### Lección 8.1.1 — Vectores de características
### Lección 8.1.2 — Clasificación de texto
### Lección 8.1.3 — Pipelines básicos

## 8.2 — Tareas clásicas de NLP

### Lección 8.2.1 — Análisis de sentimiento
### Lección 8.2.2 — Clasificación temática
### Lección 8.2.3 — Detección de spam

---

# Unidad 9 — Evaluación en NLP

## 9.1 — Métricas habituales

### Lección 9.1.1 — Accuracy
### Lección 9.1.2 — Precision y recall
### Lección 9.1.3 — F1-score

## 9.2 — Errores comunes

### Lección 9.2.1 — Datasets desbalanceados
### Lección 9.2.2 — Overfitting textual
### Lección 9.2.3 — Interpretación errónea

---

# Unidad 10 — Idiomas, sesgos y ambigüedad

## 10.1 — NLP multilingüe

### Lección 10.1.1 — Retos del español
### Lección 10.1.2 — Variantes lingüísticas
### Lección 10.1.3 — Recursos disponibles

## 10.2 — Sesgos en lenguaje

### Lección 10.2.1 — Sesgos en los datos
### Lección 10.2.2 — Reproducción de estereotipos
### Lección 10.2.3 — Responsabilidad técnica

---

# Unidad 11 — Límites del NLP clásico

## 11.1 — Por qué falla el enfoque tradicional

### Lección 11.1.1 — Falta de contexto profundo
### Lección 11.1.2 — Escalabilidad
### Lección 11.1.3 — Dependencia de reglas

## 11.2 — Transición hacia Deep Learning

### Lección 11.2.1 — Representaciones distribuidas
### Lección 11.2.2 — Modelos neuronales
### Lección 11.2.3 — Cambio de paradigma

---

# Unidad 12 — Mini-proyecto de NLP clásico

## 12.1 — Proyecto guiado

### Lección 12.1.1 — Definición del problema textual
### Lección 12.1.2 — Preprocesado completo
### Lección 12.1.3 — Vectorización
### Lección 12.1.4 — Entrenamiento de un clasificador
### Lección 12.1.5 — Evaluación y conclusiones

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Word embeddings
### Lección 13.1.2 — NLP con Deep Learning
### Lección 13.1.3 — Transformers y modelos de lenguaje

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Machine Learning clásico
### Lección 13.2.2 — Deep Learning con Python
### Lección 13.2.3 — NLP con Deep Learning

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

