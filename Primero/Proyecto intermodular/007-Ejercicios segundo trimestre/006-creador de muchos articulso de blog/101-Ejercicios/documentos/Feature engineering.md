---
slug: feature-engineering
title: Feature engineering (comoprogramar.es)
description: Aprende a diseñar, transformar y seleccionar variables (features) para mejorar el rendimiento y la interpretabilidad de modelos de machine learning. Un enfoque práctico para convertir datos brutos en información útil.
keywords:
  - feature engineering
  - ingeniería de características
  - selección de variables
  - transformación de datos
  - machine learning práctico
  - preparación de datos
level: Intermedio
duration_estimate: "30-45 horas"
prerequisites:
  - "Machine Learning desde cero"
  - "Aprendizaje supervisado"
  - "Manipulación de datos con NumPy y Pandas"
audience:
  - "Estudiantes de ciencia de datos"
  - "Científicos de datos en formación"
  - "Ingenieros de IA"
  - "Analistas de datos"
updated: 2025-12-26
---

# Feature engineering

## Objetivos del curso

## Cómo usar este curso

## Por qué las variables importan más que el modelo

---

# Unidad 1 — Qué es el feature engineering

## 1.1 — De datos brutos a variables útiles

### Lección 1.1.1 — Qué es una feature
### Lección 1.1.2 — Diferencia entre dato y variable
### Lección 1.1.3 — Impacto en el rendimiento del modelo

## 1.2 — Feature engineering en el flujo de un proyecto de IA

### Lección 1.2.1 — Antes del modelado
### Lección 1.2.2 — Relación con el tipo de modelo
### Lección 1.2.3 — Riesgos de hacerlo mal

---

# Unidad 2 — Comprender los datos antes de transformar

## 2.1 — Análisis exploratorio orientado a features

### Lección 2.1.1 — Distribuciones
### Lección 2.1.2 — Correlaciones
### Lección 2.1.3 — Variables redundantes

## 2.2 — Errores comunes de principiantes

### Lección 2.2.1 — Crear variables sin sentido
### Lección 2.2.2 — Usar información futura
### Lección 2.2.3 — Data leakage

---

# Unidad 3 — Tratamiento de valores faltantes

## 3.1 — Comprender por qué faltan datos

### Lección 3.1.1 — Missing completamente al azar
### Lección 3.1.2 — Missing dependiente
### Lección 3.1.3 — Impacto en el modelo

## 3.2 — Estrategias de imputación

### Lección 3.2.1 — Eliminación de registros
### Lección 3.2.2 — Imputación simple
### Lección 3.2.3 — Imputación basada en modelos

---

# Unidad 4 — Transformación de variables numéricas

## 4.1 — Escalado y normalización

### Lección 4.1.1 — Min-max scaling
### Lección 4.1.2 — Estandarización
### Lección 4.1.3 — Impacto según el modelo

## 4.2 — Transformaciones no lineales

### Lección 4.2.1 — Logaritmos
### Lección 4.2.2 — Box-Cox y Yeo-Johnson
### Lección 4.2.3 — Interpretación tras transformar

---

# Unidad 5 — Variables categóricas

## 5.1 — Tipos de variables categóricas

### Lección 5.1.1 — Nominales
### Lección 5.1.2 — Ordinales
### Lección 5.1.3 — Cardinalidad alta

## 5.2 — Codificación de categorías

### Lección 5.2.1 — Label encoding
### Lección 5.2.2 — One-hot encoding
### Lección 5.2.3 — Target encoding (riesgos)

---

# Unidad 6 — Creación de nuevas features

## 6.1 — Features derivadas

### Lección 6.1.1 — Combinaciones de variables
### Lección 6.1.2 — Ratios y diferencias
### Lección 6.1.3 — Features de dominio

## 6.2 — Features temporales

### Lección 6.2.1 — Día, mes, hora
### Lección 6.2.2 — Ventanas temporales
### Lección 6.2.3 — Riesgos en series temporales

---

# Unidad 7 — Feature engineering para texto y datos no estructurados

## 7.1 — Texto como variable

### Lección 7.1.1 — Bag of Words
### Lección 7.1.2 — TF-IDF
### Lección 7.1.3 — Limitaciones del enfoque clásico

## 7.2 — Features básicas para imágenes y otros datos

### Lección 7.2.1 — Estadísticas simples
### Lección 7.2.2 — Extracción manual
### Lección 7.2.3 — Cuándo usar deep learning

---

# Unidad 8 — Selección de variables

## 8.1 — Por qué seleccionar features

### Lección 8.1.1 — Reducir ruido
### Lección 8.1.2 — Mejorar generalización
### Lección 8.1.3 — Interpretabilidad

## 8.2 — Métodos de selección

### Lección 8.2.1 — Filtros estadísticos
### Lección 8.2.2 — Métodos wrapper
### Lección 8.2.3 — Métodos embedded

---

# Unidad 9 — Feature engineering y tipo de modelo

## 9.1 — Modelos lineales

### Lección 9.1.1 — Sensibilidad a escalado
### Lección 9.1.2 — Multicolinealidad
### Lección 9.1.3 — Interpretación de coeficientes

## 9.2 — Árboles y ensembles

### Lección 9.2.1 — Menor necesidad de escalado
### Lección 9.2.2 — Captura automática de interacciones
### Lección 9.2.3 — Límites reales

---

# Unidad 10 — Pipelines y reproducibilidad

## 10.1 — Automatizar el feature engineering

### Lección 10.1.1 — Pipelines de transformación
### Lección 10.1.2 — Evitar errores manuales
### Lección 10.1.3 — Reproducibilidad

## 10.2 — Feature engineering en producción

### Lección 10.2.1 — Consistencia train–test
### Lección 10.2.2 — Datos en tiempo real
### Lección 10.2.3 — Mantenimiento de features

---

# Unidad 11 — Riesgos y malas prácticas

## 11.1 — Over-engineering

### Lección 11.1.1 — Demasiadas variables
### Lección 11.1.2 — Features poco interpretables
### Lección 11.1.3 — Coste computacional

## 11.2 — Ética y responsabilidad

### Lección 11.2.1 — Variables sensibles
### Lección 11.2.2 — Discriminación indirecta
### Lección 11.2.3 — Uso responsable

---

# Unidad 12 — Mini-proyecto de feature engineering

## 12.1 — Proyecto guiado

### Lección 12.1.1 — Análisis del dataset
### Lección 12.1.2 — Creación de features
### Lección 12.1.3 — Selección de variables
### Lección 12.1.4 — Comparación de resultados
### Lección 12.1.5 — Conclusiones razonadas

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Optimización de modelos
### Lección 13.1.2 — Evaluación y validación avanzada
### Lección 13.1.3 — MLOps

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Evaluación y validación de modelos
### Lección 13.2.2 — Machine Learning clásico
### Lección 13.2.3 — Flujo completo de un proyecto de IA

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

