---
slug: evaluacion-y-validacion-de-modelos
title: Evaluación y validación de modelos (comoprogramar.es)
description: Aprende a evaluar y validar modelos de machine learning de forma rigurosa. Domina métricas, validación cruzada, detección de overfitting, data leakage y toma de decisiones informadas antes del despliegue.
keywords:
  - evaluación de modelos
  - validación de modelos
  - métricas de machine learning
  - validación cruzada
  - overfitting
  - data leakage
  - rendimiento de modelos
level: Intermedio
duration_estimate: "25-40 horas"
prerequisites:
  - "Machine Learning desde cero"
  - "Aprendizaje supervisado"
  - "Modelos de clasificación y regresión (recomendado)"
audience:
  - "Estudiantes de ciencia de datos"
  - "Científicos de datos en formación"
  - "Ingenieros de IA"
  - "Analistas de datos"
updated: 2025-12-26
---

# Evaluación y validación de modelos

## Objetivos del curso

## Cómo usar este curso

## Por qué evaluar bien es más importante que el modelo

---

# Unidad 1 — Qué significa evaluar un modelo

## 1.1 — Evaluar no es solo calcular una métrica

### Lección 1.1.1 — Rendimiento vs utilidad
### Lección 1.1.2 — Evaluación técnica vs evaluación de negocio
### Lección 1.1.3 — Riesgos de una mala evaluación

## 1.2 — Validación como proceso

### Lección 1.2.1 — Entrenar, validar y probar
### Lección 1.2.2 — Cuándo confiar en un resultado
### Lección 1.2.3 — Evaluación continua

---

# Unidad 2 — Separación correcta de los datos

## 2.1 — Datasets fundamentales

### Lección 2.1.1 — Training set
### Lección 2.1.2 — Validation set
### Lección 2.1.3 — Test set

## 2.2 — Errores críticos

### Lección 2.2.1 — Evaluar con datos vistos
### Lección 2.2.2 — Data leakage
### Lección 2.2.3 — Distribuciones irreales

---

# Unidad 3 — Métricas para problemas de regresión

## 3.1 — Métricas de error

### Lección 3.1.1 — MAE
### Lección 3.1.2 — MSE y RMSE
### Lección 3.1.3 — Cuándo usar cada una

## 3.2 — Métricas explicativas

### Lección 3.2.1 — R²
### Lección 3.2.2 — R² ajustado
### Lección 3.2.3 — Errores comunes de interpretación

---

# Unidad 4 — Métricas para problemas de clasificación

## 4.1 — Métricas básicas

### Lección 4.1.1 — Accuracy
### Lección 4.1.2 — Precision
### Lección 4.1.3 — Recall y F1

## 4.2 — Evaluación avanzada

### Lección 4.2.1 — Matriz de confusión
### Lección 4.2.2 — Curva ROC
### Lección 4.2.3 — AUC

---

# Unidad 5 — Desbalance de clases y evaluación

## 5.1 — Por qué accuracy falla

### Lección 5.1.1 — Clases minoritarias
### Lección 5.1.2 — Coste de errores
### Lección 5.1.3 — Casos reales

## 5.2 — Métricas adecuadas en desbalance

### Lección 5.2.1 — Precision–Recall
### Lección 5.2.2 — F1
### Lección 5.2.3 — Ajuste de umbrales

---

# Unidad 6 — Validación cruzada

## 6.1 — Qué es la validación cruzada

### Lección 6.1.1 — Idea general
### Lección 6.1.2 — k-fold cross-validation
### Lección 6.1.3 — Ventajas y costes

## 6.2 — Variantes de validación cruzada

### Lección 6.2.1 — Stratified k-fold
### Lección 6.2.2 — Time series split
### Lección 6.2.3 — Cuándo usar cada una

---

# Unidad 7 — Overfitting y underfitting

## 7.1 — Diagnóstico del modelo

### Lección 7.1.1 — Señales de overfitting
### Lección 7.1.2 — Señales de underfitting
### Lección 7.1.3 — Curvas de aprendizaje

## 7.2 — Cómo actuar

### Lección 7.2.1 — Más datos
### Lección 7.2.2 — Simplificar o complejizar modelos
### Lección 7.2.3 — Regularización

---

# Unidad 8 — Comparación y selección de modelos

## 8.1 — Comparar modelos correctamente

### Lección 8.1.1 — Mismas condiciones de evaluación
### Lección 8.1.2 — Significancia práctica
### Lección 8.1.3 — No sobreoptimizar

## 8.2 — Selección final

### Lección 8.2.1 — Trade-offs
### Lección 8.2.2 — Interpretabilidad vs rendimiento
### Lección 8.2.3 — Decisión informada

---

# Unidad 9 — Evaluación más allá de las métricas

## 9.1 — Evaluación desde negocio

### Lección 9.1.1 — Impacto económico
### Lección 9.1.2 — Riesgos operativos
### Lección 9.1.3 — Umbrales aceptables

## 9.2 — Evaluación ética y de riesgo

### Lección 9.2.1 — Sesgos
### Lección 9.2.2 — Impacto social
### Lección 9.2.3 — Uso responsable

---

# Unidad 10 — Validación antes del despliegue

## 10.1 — Pruebas finales

### Lección 10.1.1 — Casos límite
### Lección 10.1.2 — Pruebas de estrés
### Lección 10.1.3 — Simulación de uso real

## 10.2 — Decidir desplegar o no

### Lección 10.2.1 — Cuándo un modelo NO está listo
### Lección 10.2.2 — Riesgos de desplegar prematuramente
### Lección 10.2.3 — Criterios de aceptación

---

# Unidad 11 — Evaluación en producción

## 11.1 — El modelo cambia con el tiempo

### Lección 11.1.1 — Data drift
### Lección 11.1.2 — Concept drift
### Lección 11.1.3 — Degradación del rendimiento

## 11.2 — Monitorización continua

### Lección 11.2.1 — Métricas en producción
### Lección 11.2.2 — Alertas
### Lección 11.2.3 — Reentrenamiento

---

# Unidad 12 — Mini-proyecto de evaluación completa

## 12.1 — Proyecto guiado

### Lección 12.1.1 — Evaluación de un modelo entrenado
### Lección 12.1.2 — Selección de métricas adecuadas
### Lección 12.1.3 — Validación cruzada
### Lección 12.1.4 — Análisis de errores
### Lección 12.1.5 — Recomendación final

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Optimización de modelos
### Lección 13.1.2 — MLOps
### Lección 13.1.3 — Proyectos de IA en producción

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Flujo completo de un proyecto de IA
### Lección 13.2.2 — Machine Learning clásico
### Lección 13.2.3 — MLOps básico

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

