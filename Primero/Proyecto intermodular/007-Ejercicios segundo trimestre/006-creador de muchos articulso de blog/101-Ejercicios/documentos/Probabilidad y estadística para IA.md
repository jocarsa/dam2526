---
slug: curso-probabilidad-estadistica-para-ia
title: Probabilidad y estadística para IA (comoprogramar.es)
description: Aprende probabilidad y estadística aplicadas a inteligencia artificial de forma práctica. Comprende la incertidumbre, el comportamiento de los datos y la interpretación de resultados en modelos de machine learning y deep learning.
keywords:
  - probabilidad para IA
  - estadística para IA
  - estadística aplicada
  - probabilidad aplicada
  - machine learning estadística
  - análisis de datos probabilístico
level: Intermedio
duration_estimate: "30-45 horas"
prerequisites:
  - "Matemáticas para IA (prácticas, no académicas) o conocimientos equivalentes"
audience:
  - "Estudiantes de inteligencia artificial"
  - "Científicos de datos en formación"
  - "Desarrolladores que quieren entender la incertidumbre en IA"
updated: 2025-12-26
---

# Probabilidad y estadística para IA

## Objetivos del curso

## Cómo usar este curso

## Enfoque del curso (aplicado y orientado a datos)

---

# Unidad 1 — Por qué la probabilidad es clave en IA

## 1.1 — IA como gestión de incertidumbre

### Lección 1.1.1 — Predicción vs certeza
### Lección 1.1.2 — Modelos probabilísticos
### Lección 1.1.3 — Decisiones bajo incertidumbre

## 1.2 — Pensar en términos probabilísticos

### Lección 1.2.1 — Eventos y resultados
### Lección 1.2.2 — Frecuencia vs probabilidad
### Lección 1.2.3 — Errores comunes de intuición

## 1.3 — Probabilidad como lenguaje de la IA

### Lección 1.3.1 — Scores, probabilidades y confianza
### Lección 1.3.2 — Interpretación de salidas de modelos
### Lección 1.3.3 — Casos reales en IA

---

# Unidad 2 — Probabilidad básica aplicada

## 2.1 — Conceptos fundamentales

### Lección 2.1.1 — Espacio muestral
### Lección 2.1.2 — Eventos simples y compuestos
### Lección 2.1.3 — Reglas básicas de probabilidad

## 2.2 — Probabilidad conjunta y condicional

### Lección 2.2.1 — Dependencia entre variables
### Lección 2.2.2 — Probabilidad condicional
### Lección 2.2.3 — Interpretación práctica

## 2.3 — Teorema de Bayes (intuición)

### Lección 2.3.1 — Actualización de creencias
### Lección 2.3.2 — Priors y evidencia
### Lección 2.3.3 — Casos reales en clasificación

---

# Unidad 3 — Variables aleatorias y distribuciones

## 3.1 — Variables aleatorias

### Lección 3.1.1 — Discretas y continuas
### Lección 3.1.2 — Qué representa una variable aleatoria
### Lección 3.1.3 — Ejemplos en datos reales

## 3.2 — Distribuciones de probabilidad

### Lección 3.2.1 — Distribución uniforme
### Lección 3.2.2 — Distribución normal
### Lección 3.2.3 — Interpretación de forma y dispersión

## 3.3 — Otras distribuciones relevantes

### Lección 3.3.1 — Binomial
### Lección 3.3.2 — Poisson
### Lección 3.3.3 — Cuándo aparecen en IA

---

# Unidad 4 — Estadística descriptiva para IA

## 4.1 — Medidas de tendencia central

### Lección 4.1.1 — Media
### Lección 4.1.2 — Mediana
### Lección 4.1.3 — Moda

## 4.2 — Medidas de dispersión

### Lección 4.2.1 — Varianza
### Lección 4.2.2 — Desviación estándar
### Lección 4.2.3 — Qué indican realmente

## 4.3 — Forma de la distribución

### Lección 4.3.1 — Asimetría
### Lección 4.3.2 — Curtosis
### Lección 4.3.3 — Detección de outliers

---

# Unidad 5 — Muestreo y datos incompletos

## 5.1 — Población vs muestra

### Lección 5.1.1 — Por qué no usamos todos los datos
### Lección 5.1.2 — Sesgo de muestreo
### Lección 5.1.3 — Representatividad

## 5.2 — Técnicas de muestreo

### Lección 5.2.1 — Muestreo aleatorio
### Lección 5.2.2 — Muestreo estratificado
### Lección 5.2.3 — Impacto en modelos

## 5.3 — Datos faltantes

### Lección 5.3.1 — Tipos de ausencia
### Lección 5.3.2 — Estrategias de imputación
### Lección 5.3.3 — Riesgos en IA

---

# Unidad 6 — Inferencia estadística (sin formalismos)

## 6.1 — Estimar parámetros

### Lección 6.1.1 — Estimación puntual
### Lección 6.1.2 — Intervalos de confianza
### Lección 6.1.3 — Interpretación correcta

## 6.2 — Hipótesis y contrastes

### Lección 6.2.1 — Hipótesis nula y alternativa
### Lección 6.2.2 — p-value (intuición real)
### Lección 6.2.3 — Errores comunes

## 6.3 — Decisiones basadas en datos

### Lección 6.3.1 — Riesgo estadístico
### Lección 6.3.2 — Falsos positivos y negativos
### Lección 6.3.3 — Casos reales en IA

---

# Unidad 7 — Probabilidad en modelos de machine learning

## 7.1 — Modelos probabilísticos

### Lección 7.1.1 — Clasificadores probabilísticos
### Lección 7.1.2 — Scores vs probabilidades
### Lección 7.1.3 — Calibración de modelos

## 7.2 — Funciones de pérdida probabilísticas

### Lección 7.2.1 — Log-loss
### Lección 7.2.2 — Entropía cruzada
### Lección 7.2.3 — Interpretación intuitiva

## 7.3 — Predicción y confianza

### Lección 7.3.1 — Umbrales de decisión
### Lección 7.3.2 — Incertidumbre del modelo
### Lección 7.3.3 — Qué comunicar a usuarios

---

# Unidad 8 — Estadística en evaluación de modelos

## 8.1 — Métricas de evaluación

### Lección 8.1.1 — Accuracy
### Lección 8.1.2 — Precision y recall
### Lección 8.1.3 — F1-score

## 8.2 — Curvas y análisis

### Lección 8.2.1 — ROC
### Lección 8.2.2 — AUC
### Lección 8.2.3 — Interpretación correcta

## 8.3 — Validación de modelos

### Lección 8.3.1 — Train / test
### Lección 8.3.2 — Cross-validation (intuición)
### Lección 8.3.3 — Evitar sobreajuste

---

# Unidad 9 — Correlación, dependencia y causalidad

## 9.1 — Correlación estadística

### Lección 9.1.1 — Correlación positiva y negativa
### Lección 9.1.2 — Correlación espuria
### Lección 9.1.3 — Interpretación responsable

## 9.2 — Dependencia entre variables

### Lección 9.2.1 — Variables independientes
### Lección 9.2.2 — Dependencia condicional
### Lección 9.2.3 — Casos en datos reales

## 9.3 — Correlación no es causalidad

### Lección 9.3.1 — Errores comunes en IA
### Lección 9.3.2 — Riesgos en decisiones automáticas
### Lección 9.3.3 — Pensamiento crítico

---

# Unidad 10 — Probabilidad y estadística en deep learning

## 10.1 — Interpretación probabilística de redes

### Lección 10.1.1 — Salidas softmax
### Lección 10.1.2 — Distribuciones implícitas
### Lección 10.1.3 — Confianza del modelo

## 10.2 — Regularización y ruido

### Lección 10.2.1 — Overfitting como problema estadístico
### Lección 10.2.2 — Dropout (intuición probabilística)
### Lección 10.2.3 — Generalización

---

# Unidad 11 — Pensar estadísticamente como ingeniero de IA

## 11.1 — Diagnóstico de problemas

### Lección 11.1.1 — Datos insuficientes
### Lección 11.1.2 — Sesgos ocultos
### Lección 11.1.3 — Interpretaciones erróneas

## 11.2 — Equilibrio entre estadística y práctica

### Lección 11.2.1 — Cuándo profundizar
### Lección 11.2.2 — Cuándo confiar en librerías
### Lección 11.2.3 — Criterio profesional

---

# Unidad 12 — Mini-proyecto probabilístico aplicado

## 12.1 — Proyecto guiado

### Lección 12.1.1 — Dataset real con incertidumbre
### Lección 12.1.2 — Análisis estadístico
### Lección 12.1.3 — Modelado probabilístico básico
### Lección 12.1.4 — Interpretación de resultados
### Lección 12.1.5 — Preparación para ML

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Machine Learning desde cero
### Lección 13.1.2 — Deep Learning
### Lección 13.1.3 — NLP y modelos generativos

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Machine Learning desde cero
### Lección 13.2.2 — Deep Learning desde cero
### Lección 13.2.3 — IA generativa

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

