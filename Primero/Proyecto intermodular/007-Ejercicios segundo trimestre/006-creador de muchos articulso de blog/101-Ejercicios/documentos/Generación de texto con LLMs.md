---
slug: generacion-de-texto-con-llms
title: Generación de texto con LLMs (comoprogramar.es)
description: Aprende a generar texto con modelos de lenguaje grandes (LLMs). Controla la calidad, coherencia y seguridad de la generación mediante prompts, decodificación, contexto y evaluación práctica.
keywords:
  - generacion de texto llms
  - llm text generation
  - modelos de lenguaje grandes
  - prompting
  - decodificacion de texto
  - control de generacion
level: Intermedio–Avanzado
duration_estimate: "40-60 horas"
prerequisites:
  - "Modelos de lenguaje"
  - "Tokenización, embeddings y vectores"
  - "Introducción a la IA generativa"
audience:
  - "Desarrolladores"
  - "Ingenieros de IA"
  - "Product managers técnicos"
  - "Estudiantes avanzados de NLP"
updated: 2025-12-26
---

# Generación de texto con LLMs

## Objetivos del curso

## Cómo usar este curso

## Generar texto es elegir probabilidades, no frases

---

# Unidad 1 — Qué significa generar texto con LLMs

## 1.1 — Predicción del siguiente token

### Lección 1.1.1 — Probabilidades condicionadas
### Lección 1.1.2 — Tokens vs palabras
### Lección 1.1.3 — Secuencias largas

## 1.2 — Casos de uso reales

### Lección 1.2.1 — Redacción asistida
### Lección 1.2.2 — Resumen y reescritura
### Lección 1.2.3 — Generación de código y documentación

---

# Unidad 2 — Arquitectura de alto nivel de un LLM

## 2.1 — Transformer como motor

### Lección 2.1.1 — Atención al contexto
### Lección 2.1.2 — Embeddings y capas
### Lección 2.1.3 — Escalado del modelo

## 2.2 — Entrenamiento vs inferencia

### Lección 2.2.1 — Preentrenamiento
### Lección 2.2.2 — Ajuste fino (visión general)
### Lección 2.2.3 — Costes computacionales

---

# Unidad 3 — Prompts para generación de texto

## 3.1 — Componentes de un buen prompt

### Lección 3.1.1 — Instrucción clara
### Lección 3.1.2 — Contexto relevante
### Lección 3.1.3 — Formato esperado

## 3.2 — Técnicas de prompting

### Lección 3.2.1 — Zero-shot
### Lección 3.2.2 — Few-shot
### Lección 3.2.3 — Prompt estructurado

---

# Unidad 4 — Decodificación y control de la salida

## 4.1 — Métodos de decodificación

### Lección 4.1.1 — Greedy decoding
### Lección 4.1.2 — Beam search
### Lección 4.1.3 — Sampling estocástico

## 4.2 — Parámetros clave

### Lección 4.2.1 — Temperature
### Lección 4.2.2 — Top-k
### Lección 4.2.3 — Top-p (nucleus)

---

# Unidad 5 — Coherencia y consistencia

## 5.1 — Mantener el hilo del texto

### Lección 5.1.1 — Dependencias largas
### Lección 5.1.2 — Repetición y bucles
### Lección 5.1.3 — Señales de degradación

## 5.2 — Técnicas de mejora

### Lección 5.2.1 — Resumen intermedio
### Lección 5.2.2 — Segmentación de tareas
### Lección 5.2.3 — Reescritura controlada

---

# Unidad 6 — Control semántico de la generación

## 6.1 — Estilo, tono y registro

### Lección 6.1.1 — Formal vs informal
### Lección 6.1.2 — Voz y persona
### Lección 6.1.3 — Consistencia de estilo

## 6.2 — Restricciones de contenido

### Lección 6.2.1 — Longitud máxima
### Lección 6.2.2 — Palabras prohibidas
### Lección 6.2.3 — Plantillas de salida

---

# Unidad 7 — Contexto externo y grounding

## 7.1 — Por qué el LLM no “sabe” tus datos

### Lección 7.1.1 — Conocimiento estático
### Lección 7.1.2 — Alucinaciones
### Lección 7.1.3 — Necesidad de grounding

## 7.2 — Generación aumentada con recuperación (RAG)

### Lección 7.2.1 — Búsqueda semántica
### Lección 7.2.2 — Inyección de contexto
### Lección 7.2.3 — Control de fuentes

---

# Unidad 8 — Evaluación de texto generado

## 8.1 — Métricas automáticas

### Lección 8.1.1 — Perplejidad
### Lección 8.1.2 — BLEU / ROUGE (limitaciones)
### Lección 8.1.3 — Heurísticas prácticas

## 8.2 — Evaluación humana

### Lección 8.2.1 — Coherencia
### Lección 8.2.2 — Utilidad
### Lección 8.2.3 — Consistencia con el objetivo

---

# Unidad 9 — Riesgos y errores comunes

## 9.1 — Alucinaciones y sobreconfianza

### Lección 9.1.1 — Por qué ocurren
### Lección 9.1.2 — Señales de alerta
### Lección 9.1.3 — Mitigación básica

## 9.2 — Prompt injection y seguridad

### Lección 9.2.1 — Inyecciones directas
### Lección 9.2.2 — Contexto malicioso
### Lección 9.2.3 — Barreras defensivas

---

# Unidad 10 — Generación de texto en producción

## 10.1 — Integración técnica

### Lección 10.1.1 — APIs y SDKs
### Lección 10.1.2 — Latencia y coste
### Lección 10.1.3 — Escalabilidad

## 10.2 — Observabilidad y mejora continua

### Lección 10.2.1 — Logs de prompts y salidas
### Lección 10.2.2 — Detección de fallos
### Lección 10.2.3 — Iteración controlada

---

# Unidad 11 — Casos de uso avanzados

## 11.1 — Redacción especializada

### Lección 11.1.1 — Textos técnicos
### Lección 11.1.2 — Marketing y contenidos
### Lección 11.1.3 — Documentación automática

## 11.2 — Generación asistida por estructura

### Lección 11.2.1 — Esquemas y guías
### Lección 11.2.2 — Checklists generativos
### Lección 11.2.3 — Salidas verificables

---

# Unidad 12 — Mini-proyecto de generación de texto

## 12.1 — Proyecto guiado completo

### Lección 12.1.1 — Definición del objetivo
### Lección 12.1.2 — Diseño del prompt
### Lección 12.1.3 — Ajuste de decodificación
### Lección 12.1.4 — Evaluación de resultados
### Lección 12.1.5 — Despliegue básico

---

# Unidad 13 — Siguientes pasos

## 13.1 — Qué aprender después

### Lección 13.1.1 — Fine-tuning de LLMs
### Lección 13.1.2 — Agentes y herramientas
### Lección 13.1.3 — Sistemas RAG avanzados

## 13.2 — Ruta recomendada en comoprogramar.es

### Lección 13.2.1 — Chatbots con IA generativa
### Lección 13.2.2 — Arquitectura Transformer explicada
### Lección 13.2.3 — Evaluación y seguridad en LLMs

---

## Recursos recomendados

## Glosario (opcional)

## Créditos

> Última actualización: **2025-12-26**

