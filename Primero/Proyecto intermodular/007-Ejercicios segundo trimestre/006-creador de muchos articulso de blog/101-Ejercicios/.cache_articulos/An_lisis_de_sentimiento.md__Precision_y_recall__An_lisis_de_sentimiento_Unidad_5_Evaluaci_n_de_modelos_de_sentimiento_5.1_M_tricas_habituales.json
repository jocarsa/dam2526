{
  "source_file": "Análisis de sentimiento.md",
  "category": "Análisis de sentimiento, Unidad 5 — Evaluación de modelos de sentimiento, 5.1 — Métricas habituales",
  "title": "Precision y recall",
  "generated_at": "2026-01-12 15:48:36",
  "model": "qwen2.5:7b-instruct-q4_0",
  "content": "# Precision y recall: Evaluación crucial para modelos de análisis de sentimiento\n\n## Introducción\n\nEn el campo del análisis de sentimiento, la precisión (precision) y el recuerdo (recall) son dos métricas fundamentales que nos ayudan a entender cómo se comporta un modelo en términos de clasificación. A diferencia de otras métricas como accuracy, precision y recall proporcionan una visión más profunda del rendimiento del modelo, especialmente cuando los datos están desbalanceados. En este artículo, exploraremos por qué son importantes estas métricas y cómo interpretarlas.\n\n## Explicación principal\n\n### Definición e importancia de la precisión y el recuerdo\n\nLa **precision** mide cuántos de los documentos etiquetados como positivos (por ejemplo, reseñas favorables) realmente son positivos. Matemáticamente, se define como:\n\n\\[ \\text{Precision} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos + Falsos Positivos}} \\]\n\nLa **recall** mide cuántos de los documentos reales positivos son identificados correctamente. Se define como:\n\n\\[ \\text{Recall} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos + Falsos Negativos}} \\]\n\nEn el contexto del análisis de sentimiento, un alto nivel de precision significa que el modelo rara vez clasifica una reseña negativa como positiva. Un alto recall indica que el modelo no omite muchas reseñas negativas.\n\n### Ejemplo práctico\n\nSupongamos que estamos analizando reseñas de productos y queremos identificar las reseñas negativas. Consideremos los siguientes datos:\n\n- Verdaderos Positivos (VP): 25\n- Falsos Positivos (FP): 10\n- Falsos Negativos (FN): 5\n\nLa precisión sería:\n\\[ \\text{Precision} = \\frac{25}{25 + 10} = \\frac{25}{35} \\approx 71.4\\% \\]\n\nEl recall sería:\n\\[ \\text{Recall} = \\frac{25}{25 + 5} = \\frac{25}{30} \\approx 83.3\\% \\]\n\n### Visualización del desempeño\n\nPuedes visualizar la precisión y el recuerdo en un gráfico de curva ROC (Receiver Operating Characteristic). Cada punto en esta curva representa una combinación de recall e 1-precision para diferentes umbrales de clasificación.\n\n```python\nimport numpy as np\nfrom sklearn.metrics import precision_recall_curve, roc_curve\n\n# Ejemplo de datos\ny_true = [0, 0, 1, 1]\ny_scores = [0.1, 0.4, 0.35, 0.8]\n\n# Calcular precisión y recall\nprecision, recall, _ = precision_recall_curve(y_true, y_scores)\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\n\n# Visualizar la curva ROC\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Curva de Precision vs Recall')\nplt.show()\n```\n\n## Errores típicos / trampas\n\n### Error 1: Falsos Positivos en exceso\n\nUn alto número de falsos positivos puede llevar a errores costosos, especialmente si el modelo clasifica reseñas neutrales como negativas. Por ejemplo, en marketing, un error de este tipo podría resultar en una mala campaña basada en datos incorrectos.\n\n### Error 2: Bajas precisiones\n\nUna baja precisión indica que hay muchos documentos falsos positivos, lo cual puede llevar a decisiones inapropiadas basándose en predicciones erróneas. Por ejemplo, si un modelo clasifica reseñas como negativas cuando en realidad son neutrales o favorables.\n\n### Error 3: Falsos Negativos\n\nUn alto número de falsos negativos (baja recall) puede llevar a una mala detección de problemas importantes. En el contexto del análisis de sentimiento, esto podría significar que muchas reseñas negativas no son identificadas por el modelo.\n\n## Checklist accionable\n\n1. **Verifica la equilibración de datos**: Asegúrate de tener un conjunto de datos bien equilibrado para evitar sesgos en precision y recall.\n2. **Ajusta los umbrales de clasificación**: Experimenta con diferentes umbrales para optimizar tanto la precisión como el recuerdo.\n3. **Utiliza modelos balanceados**: Considera usar técnicas como oversampling, undersampling o SMOTE para equilibrar las clases en tu conjunto de datos.\n4. **Monitorea regularmente**: Realiza evaluaciones regulares del desempeño del modelo con precision y recall para detectar cambios en el rendimiento.\n5. **Analiza la curva ROC**: Visualiza la curva ROC para entender mejor cómo se comporta el modelo a diferentes umbrales de clasificación.\n\n## Cierre: Siguientes pasos\n\n1. **Evaluación continua**: Continúa monitoreando y ajustando tu modelo basado en precision y recall.\n2. **Mejora del conjunto de datos**: Asegúrate de que tu conjunto de datos esté bien representativo y equilibrado para evitar sesgos.\n3. **Implementación en producción**: Integra el uso de precision y recall en los pipelines de producción para garantizar un desempeño constante.\n\nLa precisión y el recuerdo son herramientas cruciales para evaluar modelos de análisis de sentimiento. Asegúrate de utilizar estas métricas adecuadamente para obtener resultados confiables y tomar decisiones informadas basadas en tus datos.",
  "hierarchy": {
    "h1": "Unidad 5 — Evaluación de modelos de sentimiento",
    "h2": "5.1 — Métricas habituales",
    "h3_raw": "Lección 5.1.2 — Precision y recall"
  }
}