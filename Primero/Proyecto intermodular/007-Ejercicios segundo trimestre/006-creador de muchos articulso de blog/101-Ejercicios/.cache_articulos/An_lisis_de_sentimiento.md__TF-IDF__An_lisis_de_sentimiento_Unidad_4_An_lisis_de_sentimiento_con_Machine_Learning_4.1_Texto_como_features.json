{
  "source_file": "Análisis de sentimiento.md",
  "category": "Análisis de sentimiento, Unidad 4 — Análisis de sentimiento con Machine Learning, 4.1 — Texto como features",
  "title": "TF-IDF",
  "generated_at": "2026-01-12 15:45:55",
  "model": "qwen2.5:7b-instruct-q4_0",
  "content": "# TF-IDF: Un Pilar Fundamental para la Extracción de Características en Análisis de Sentimiento\n\n## Introducción\n\nEl **TF-IDF** (Term Frequency-Inverse Document Frequency) es una técnica crucial para convertir texto en características numéricas que pueden ser utilizadas en modelos de aprendizaje automático, especialmente en tareas de análisis de sentimiento. Este método asigna un peso a cada palabra en un documento o corpus basado en su frecuencia y rareza relativa. A diferencia del **Bag of Words**, que solo cuenta la presencia de palabras, el TF-IDF también considera cuántas veces aparece una palabra en todos los documentos, lo que permite distinguir entre palabras relevantes y irrelevantes.\n\n## Explicación Principal\n\nEl **TF-IDF** combina dos componentes principales: la frecuencia del término (TF) y la rareza inversa de un documento (IDF). La fórmula para calcular el TF-IDF es:\n\n\\[\n\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n\\]\n\nDónde:\n- **TF(t, d)** es la frecuencia del término `t` en un documento `d`.\n- **IDF(t)** mide cuántas veces aparece el término `t` en todos los documentos.\n\n### Frecuencia de Terminos (TF)\n\nLa frecuencia del término se calcula como:\n\n\\[\n\\text{TF}(t, d) = \\frac{\\text{Nº de veces que } t \\text{ aparece en } d}{\\text{Número total de términos en } d}\n\\]\n\n### Rareza Inversa de Documento (IDF)\n\nLa rareza inversa del documento se calcula como:\n\n\\[\n\\text{IDF}(t) = \\log\\left(\\frac{\\text{Nº total de documentos}}{\\text{Nº de documentos con } t} + 1\\right)\n\\]\n\n### Ejemplo Práctico\n\nConsideremos un corpus con tres documentos y las palabras \"Python\", \"Machine Learning\" y \"Análisis\":\n\n| Documento | Python | Machine Learning | Análisis |\n|-----------|--------|------------------|----------|\n| D1        | 5      | 2                | 3        |\n| D2        | 4      | 6                | 0        |\n| D3        | 2      | 4                | 7        |\n\nPara la palabra \"Python\":\n\n- **TF(Python, D1)** = 5 / (5 + 2 + 3) ≈ 0.5\n- **IDF(Python)** = log(3/2) + 1 ≈ 0.4\n\n**TF-IDF(Python, D1)** = 0.5 * 0.4 = 0.2\n\nSimilarmente, se puede calcular para las otras palabras y documentos.\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ejemplo de datos\ndocuments = [\"Machine Learning es una subcampana del Análisis de Datos.\",\n             \"Python es un lenguaje de programación versátil.\",\n             \"Análisis de sentimiento implica la clasificación de textos positivos o negativos.\"]\n\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n\nprint(\"TF-IDF Matrix:\")\nprint(tfidf_matrix.toarray())\n```\n\n## Errores Típicos / Trampas\n\n### 1. No Considerar la Contextualización\nEl **TF-IDF** solo considera la frecuencia y rareza de términos, pero no el contexto en que aparecen. Por ejemplo, \"Python\" podría ser relevante para programación o reactividad.\n\n### 2. Ignorar el Lenguaje Coloquial o Sarcasmo\nEl **TF-IDF** no se adapta bien a lenguajes coloquiales o sarcasmos, lo que puede llevar a resultados erróneos en análisis de sentimiento.\n\n### 3. No Normalizar la Escala de TF-IDF\nLas escalas de frecuencia y rareza pueden variar significativamente entre documentos, lo cual afecta el rendimiento del modelo. Es crucial normalizar o escalar las características antes de usarlas en modelos de aprendizaje automático.\n\n## Checklist Accionable\n\n### 1. **Preprocesamiento**\n   - Limpieza y tokenización del texto.\n   - Eliminación de stop words (palabras comunes).\n\n### 2. **Selección de Características**\n   - Aplicar TfidfVectorizer o CountVectorizer para convertir el texto en características numéricas.\n\n### 3. **Normalización**\n   - Normalizar las escalas TF e IDF para evitar sesgos en la representación de características.\n\n### 4. **Ajuste de Parámetros**\n   - Experimentar con diferentes parámetros del vectorizador, como `ngram_range` y `max_df`.\n\n### 5. **Evaluación del Modelo**\n   - Evaluar el modelo con diversas métricas para asegurar su rendimiento.\n\n### 6. **Interpretación de Resultados**\n   - Analizar las características más relevantes para comprender mejor el análisis de sentimiento.\n\n## Cierre\n\nEl **TF-IDF** es una herramienta poderosa pero no sin sus desafíos. Al entender y aplicar correctamente esta técnica, puedes extraer características significativas del texto que son cruciales para la precisión en tareas de análisis de sentimiento. Recuerda siempre preprocesar adecuadamente el texto, evitar errores comunes y ajustar los parámetros según sea necesario.\n\n### Siguientes Pasos\n\n- **Profundizar en Modelos Avanzados**: Estudiar técnicas más avanzadas como BERT para mejorar la precisión.\n- **Aprendizaje Automático Profundo**: Explorar cómo la atención (attention) y las redes recurrentes (RNNs) pueden mejorar el análisis de sentimiento.\n- **Ética en Análisis de Sentimiento**: Reflexionar sobre los riesgos y beneficios éticos asociados con el uso de modelos de análisis de sentimientos.\n\n¡Espero que estos conocimientos te sean útiles para mejorar tus proyectos de análisis de sentimiento!\n\n---",
  "hierarchy": {
    "h1": "Unidad 4 — Análisis de sentimiento con Machine Learning",
    "h2": "4.1 — Texto como features",
    "h3_raw": "Lección 4.1.2 — TF-IDF"
  }
}