{
  "source_file": "Análisis de sentimiento.md",
  "category": "Análisis de sentimiento, Unidad 4 — Análisis de sentimiento con Machine Learning, 4.2 — Clasificadores clásicos",
  "title": "Naive Bayes",
  "generated_at": "2026-01-12 15:46:44",
  "model": "qwen2.5:7b-instruct-q4_0",
  "content": "# Naive Bayes: Clasificadores clásicos para análisis de sentimiento\n\n## Introducción\n\nEl análisis de sentimiento es una herramienta esencial en la toma de decisiones basadas en datos, especialmente cuando se trata con texto en español. Los clasificadores clásicos como el Naive Bayes son fundamentales debido a su simplicidad y eficiencia, lo que los hace ideales para proyectos donde no hay recursos ilimitados para la implementación de modelos más avanzados. En este artículo, exploraremos cómo utilizar el Naive Bayes en el análisis de sentimiento, sus ventajas y desventajas, así como algunos errores comunes a evitar.\n\n## Explicación principal\n\nEl Naive Bayes es un algoritmo de clasificación probabilística basado en la regla de Bayes, asumiendo que las características son independientes entre sí. Este supuesto \"naïve\" permite una implementación sencilla y eficiente del algoritmo.\n\n### Teoría básica\n\nLa probabilidad posterior \\( P(C|X) \\) puede calcularse usando la fórmula de Bayes:\n\n\\[\nP(C|X) = \\frac{P(X|C) \\cdot P(C)}{\\sum_{i} P(X|C_i) \\cdot P(C_i)}\n\\]\n\nDonde:\n- \\( C \\) es el clase (por ejemplo, positivo o negativo).\n- \\( X \\) son las características del texto a clasificar.\n- \\( P(X|C) \\) es la probabilidad de ver \\( X \\) dado que el texto es de la clase \\( C \\).\n\nEn el caso del Naive Bayes, asumimos \\( P(X_1, X_2, ..., X_n|C) = P(X_1|C) \\cdot P(X_2|C) ... \\cdot P(X_n|C) \\), lo que simplifica la computación.\n\n### Implementación\n\nPara aplicar Naive Bayes al análisis de sentimiento, primero necesitamos convertir los textos en características numéricas. Podemos usar el Bag of Words (BoW) para representar cada texto como un vector con frecuencias de palabras.\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Ejemplo de datos de entrada\ntextos = [\"Me encanta este producto\", \"Es una gran experiencia\", \"No me gustó nada\"]\n\nvectorizador = CountVectorizer()\nX = vectorizador.fit_transform(textos)\n\nprint(X.toarray())\n```\n\nEste ejemplo convierte cada texto en un vector con frecuencias de palabras, donde la posición 0 es la primera palabra del diccionario y así sucesivamente.\n\n### Ejemplo completo\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Datos de ejemplo: sentimientos (1 positivo, -1 negativo)\nsentimientos = [1, 1, -1]\n\n# Dividimos los datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, sentimientos, test_size=0.2, random_state=42)\n\n# Creamos el clasificador Naive Bayes\nclf = MultinomialNB()\n\n# Entrenamos el modelo\nclf.fit(X_train.toarray(), y_train)\n\n# Evaluamos el modelo\naccuracy = clf.score(X_test.toarray(), y_test)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\n# Predicción sobre un nuevo texto\nnuevo_texto = [\"Este producto es terrible\"]\nX_nuevo = vectorizador.transform(nuevo_texto).toarray()\nprediccion = clf.predict(X_nuevo)[0]\nprint(f\"Predicción: {'Positivo' if prediccion == 1 else 'Negativo'}\")\n```\n\n## Errores típicos / trampas\n\n### Supuesto de independencia\n\nUna de las suposiciones más problemáticas del Naive Bayes es que todas las características son independientes. En el caso del análisis de sentimiento, esto puede no ser realista debido a la dependencia entre palabras en frases.\n\n### Frecuencia de palabras comunes\n\nEl Naive Bayes tiende a dar demasiada importancia a palabras frecuentes como \"el\", \"y\" y \"a\". Aunque estas pueden ser útiles para el contexto, son menos relevantes para distinguir sentimientos.\n\n### Falta de normalización\n\nA menudo se olvida normalizar las características. Por ejemplo, no todas las palabras son igualmente importantes en diferentes contextos.\n\n## Checklist accionable\n\n1. **Preprocesamiento adecuado:** Limpieza y tokenización del texto.\n2. **Selección de características:** Usar técnicas como BoW o TF-IDF para representar el texto.\n3. **Verificación de supuestos:** Asegúrate de que las palabras sean verdaderamente independientes en tu dataset.\n4. **Evita la sobreajuste:** Usa validación cruzada y mantén un buen conjunto de datos de prueba.\n5. **Analiza los resultados:** Evalúa el modelo con métricas relevantes como accuracy, precision, recall y F1-score.\n\n## Cierre: Siguientes pasos\n\n- **Avance al siguiente nivel:** Explora modelos más avanzados que consideren la dependencia entre palabras, como las redes neuronales.\n- **Integración en sistemas reales:** Implementa el Naive Bayes en pipelines de análisis de sentimiento para monitorear y mejorar continuamente.\n- **Estudio de dominios específicos:** Ajusta los modelos para diferentes tipos de contenido, como reseñas o tweets.\n\nEl Naive Bayes es una herramienta valiosa para comenzar con el análisis de sentimiento, pero es importante ser consciente de sus limitaciones. Siguiendo estos pasos y aprendiendo a superar las trampas, puedes mejorar significativamente la precisión y aplicabilidad de tus modelos.",
  "hierarchy": {
    "h1": "Unidad 4 — Análisis de sentimiento con Machine Learning",
    "h2": "4.2 — Clasificadores clásicos",
    "h3_raw": "Lección 4.2.1 — Naive Bayes"
  }
}